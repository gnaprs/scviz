{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to scpviz","text":"<p>scpviz is a Python package for single-cell and spatial proteomics data analysis, built around a custom <code>pAnnData</code> object. It extends the AnnData ecosystem with proteomics-specific functionality, enabling seamless integration of proteins, peptides, and relational data.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>scpviz</code> with <code>pip</code> and get up and running in minutes</p> <p> Getting started</p> </li> <li> <p> Quickstart</p> <p>Check out the quickstart guide for a run through import, basic preprocessing and quick visualization</p> <p> Quickstart</p> </li> <li> <p> In-depth Tutorials</p> <p>Step-by-step guides for importing, filtering, plotting, and running enrichment.</p> <p> Tutorials</p> </li> <li> <p> API Reference</p> <p>Full function documentation for <code>pAnnData</code> and helper utilities.  </p> <p> API Reference</p> </li> </ul> <ul> <li>Single-cell proteomics support: Store protein and peptide quantifications in AnnData-compatible structures.  </li> <li>Relational mapping: Track protein\u2013peptide connectivity using a dedicated RS matrix.  </li> <li>Analysis tools: Filtering, normalization, imputation, and differential expression (DE) tailored for proteomics.  </li> <li>Functional enrichment: Integrated STRING queries for GSEA and PPI networks.  </li> <li>Custom plotting: Publication-ready plots (abundance, PCA/UMAP, clustermaps, rank-quant plots, etc.).  </li> <li>API utilities: Retrieve annotations from UniProt, cache mappings, and manage large datasets efficiently.  </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the quickstart guide. For more detailed examples, check out the Tutorials.</p> <ul> <li>Developer Notes \u2013 Guidelines for contributing, testing, and extending scpviz.  </li> </ul> <p>scpviz is developed for research in single-cell and spatial proteomics, supporting reproducible and scalable analysis. </p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#scpviz","title":"scpviz","text":""},{"location":"CHANGELOG/#0.5.2-alpha","title":"0.5.2-alpha November 16, 2025","text":""},{"location":"CHANGELOG/#added","title":"Added","text":""},{"location":"CHANGELOG/#io","title":"(Io)","text":"<ul> <li>Add delimiter support for parsing imports (b1a23eb\u2026)</li> </ul>"},{"location":"CHANGELOG/#build-system","title":"Build System","text":""},{"location":"CHANGELOG/#dependencies","title":"(Dependencies)","text":"<ul> <li>Add directlfq (c6dd2c3\u2026)</li> </ul>"},{"location":"CHANGELOG/#ci","title":"CI","text":""},{"location":"CHANGELOG/#changelog_1","title":"(Changelog)","text":"<ul> <li> <p>Add two minute sleep timer to prevent clash with joss workflow (478bacd\u2026)</p> </li> <li> <p>Add rebase after sleep (4500b28\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#joss","title":"(Joss)","text":"<ul> <li> <p>Update workflow to pull before pushing due to changelog upload (514d580\u2026)</p> </li> <li> <p>Remove dynamic ref logic to fix wrong head (ee792f4\u2026)</p> </li> <li> <p>Add local checkout to main for pull (959a605\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#chores","title":"Chores","text":"<ul> <li> <p>Update changelogs [skip ci] (20d06bc\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (9e5bac4\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (3e16c2e\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (752f655\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (3b72040\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (ddbbfd8\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (0033c88\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (5201ba9\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (366a05d\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (d35654a\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (1450a6d\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (14c452a\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (f920f5b\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (b73cbc1\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (2ee80b8\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (73f898a\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":""},{"location":"CHANGELOG/#ci_1","title":"(Ci)","text":"<ul> <li> <p>Parquets were updated to lfs, add lfs request to pytest checks (391bb8b\u2026)</p> </li> <li> <p>Add lfs update to python-package.yml (13f335f\u2026)</p> </li> <li> <p>Trigger pytest workflow (1a44b79\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#lfs","title":"(Lfs)","text":"<ul> <li>Fix LFS tracking for DIANN parquet fixture (9417883\u2026)</li> </ul>"},{"location":"CHANGELOG/#other","title":"Other","text":""},{"location":"CHANGELOG/#paper","title":"(Paper)","text":"<ul> <li> <p>Update with comments from coauthors (c916be0\u2026)</p> </li> <li> <p>Update Paper PDF Draft (a637997\u2026)</p> </li> <li> <p>Update references (0b80c03\u2026)</p> </li> <li> <p>Update Paper PDF Draft (74ab75a\u2026)</p> </li> <li> <p>Final updates to paper.md (20b500c\u2026)</p> </li> <li> <p>Update acknowledgements (0eb8e8b\u2026)</p> </li> <li> <p>Update references (6cb919d\u2026)</p> </li> <li> <p>Update Paper PDF Draft (4d41ca6\u2026)</p> </li> <li> <p>Update Paper PDF Draft (7bf727d\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#tests","title":"Tests","text":""},{"location":"CHANGELOG/#io_1","title":"(Io)","text":"<ul> <li>Add python check to mapping function to account for function deprecation in pandas (8b6b130\u2026)</li> </ul>"},{"location":"CHANGELOG/#0.5.1-alpha","title":"0.5.1-alpha November 10, 2025","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":""},{"location":"CHANGELOG/#filtering","title":"(Filtering)","text":"<ul> <li>Add exclude_file_list argument (2f592f1\u2026)</li> </ul>"},{"location":"CHANGELOG/#plot_cv","title":"(Plot_cv)","text":"<ul> <li>Add palette flag (f92100b\u2026)</li> </ul>"},{"location":"CHANGELOG/#build-system_1","title":"Build System","text":"<ul> <li>Bump scpviz to v0.5.0-alpha (1942e28\u2026)</li> </ul>"},{"location":"CHANGELOG/#ci_2","title":"CI","text":""},{"location":"CHANGELOG/#changelog_2","title":"(Changelog)","text":"<ul> <li>Update .git-cliff.toml (85f2304\u2026)</li> </ul>"},{"location":"CHANGELOG/#joss_1","title":"(Joss)","text":"<ul> <li> <p>Rename joss commit message to match conventional commit style (83c4935\u2026)</p> </li> <li> <p>Update test CI to run only on changes to /src or /tests (06b3dc6\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#chores_1","title":"Chores","text":""},{"location":"CHANGELOG/#assets","title":"(Assets)","text":"<ul> <li>Move assets out of src (917be4c\u2026)</li> </ul>"},{"location":"CHANGELOG/#git-lfs","title":"(Git lfs)","text":"<ul> <li>Update .gitattributes for lfs upload of parquet files (ccf0e04\u2026)</li> </ul>"},{"location":"CHANGELOG/#todo","title":"(Todo)","text":"<ul> <li> <p>Update todo list (388fdf3\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (8e09ce2\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (dfff921\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (6e3f746\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (8a99267\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (d6d3633\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (f0cbb17\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (00d4bf8\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (f849dd9\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (aa84d7c\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (11a3754\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (b7f7e72\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#documentation","title":"Documentation","text":""},{"location":"CHANGELOG/#assets_1","title":"(Assets)","text":"<ul> <li>Upload logo and test assets (e1ee49b\u2026)</li> </ul>"},{"location":"CHANGELOG/#filtering_1","title":"(Filtering)","text":"<ul> <li>Finished filtering tutorial, update docstrings (9a3f6a5\u2026)</li> </ul>"},{"location":"CHANGELOG/#package-rename","title":"(Package rename)","text":"<ul> <li> <p>Rename all links in docs (cfc8344\u2026)</p> </li> <li> <p>Update mkdocs.yml (838fa33\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#quickstart","title":"(Quickstart)","text":"<ul> <li> <p>Update tutorial files (422ae4f\u2026)</p> </li> <li> <p>Upload diann_report.parquet, using git lfs (32501ee\u2026)</p> </li> <li> <p>Update download links (22c60d9\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#quicstart","title":"(Quicstart)","text":"<ul> <li>Add colab link for user ease of use (8fbf44c\u2026)</li> </ul>"},{"location":"CHANGELOG/#readme","title":"(Readme)","text":"<ul> <li> <p>Update readme (b8637fb\u2026)</p> </li> <li> <p>Update broken links and logo (8d7b64a\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#setup","title":"(Setup)","text":"<ul> <li>Update mkdocs.yml, add dev and js for navigation (6427731\u2026)</li> </ul>"},{"location":"CHANGELOG/#tutorial","title":"(Tutorial)","text":"<ul> <li> <p>Update quickstart, some tutorials in works (91e10bf\u2026)</p> </li> <li> <p>Update tutorial home page (f1a0cb9\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#tutorials","title":"(Tutorials)","text":"<ul> <li> <p>Add pending notice (23b7d0c\u2026)</p> </li> <li> <p>Update readme to proper image (7b82bea\u2026)</p> </li> <li> <p>Remove conda installation from readme, update gitignore (0e5e5ac\u2026)</p> </li> <li> <p>Update joss paper.md (11151fc\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":""},{"location":"CHANGELOG/#io_2","title":"(Io)","text":"<ul> <li> <p>Implement handler for diann files when using suggest_obs_columns (93cfac5\u2026)</p> </li> <li> <p>Push python version fix for .map() (49be220\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#other_1","title":"Other","text":""},{"location":"CHANGELOG/#paper_1","title":"(Paper)","text":"<ul> <li> <p>Update Paper PDF Draft (2fe3401\u2026)</p> </li> <li> <p>Update paper.md and bib (f587cb5\u2026)</p> </li> <li> <p>Update Paper PDF Draft (8c63425\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#performance","title":"Performance","text":""},{"location":"CHANGELOG/#io_3","title":"(Io)","text":"<ul> <li>Initialize rs as sparse, memory improvement from 6+ GB usage to ~60MB (da721b6\u2026)</li> </ul>"},{"location":"CHANGELOG/#style","title":"Style","text":""},{"location":"CHANGELOG/#cv","title":"(Cv)","text":"<ul> <li>Default to false verbose on cv resolve_class_filter (ee346ca\u2026)</li> </ul>"},{"location":"CHANGELOG/#filtering_2","title":"(Filtering)","text":"<ul> <li> <p>Fix print statements to be more verbose (2fdeae3\u2026)</p> </li> <li> <p>Updated print statements to include exclude_file_list (3efd2c6\u2026)</p> </li> <li> <p>Add print for cleanup with no empty prots (b71aab6\u2026)</p> </li> <li> <p>Fix typo in print statement of annotate_significant_prot (9445f0a\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#readme_1","title":"(Readme)","text":"<ul> <li>Update version on docs badge (ad74fa6\u2026)</li> </ul>"},{"location":"CHANGELOG/#0.4.1-alpha","title":"0.4.1-alpha November 04, 2025","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":""},{"location":"CHANGELOG/#base","title":"(Base)","text":"<ul> <li>Add compare_current_to_raw and get_X_raw_aligned functionality (043251b\u2026)</li> </ul>"},{"location":"CHANGELOG/#filtering_3","title":"(Filtering)","text":"<ul> <li> <p>Add valid_genes, unique_profiles to filter_prot and cleanup (nans) to filter_sample (e251c8a\u2026)</p> </li> <li> <p>Add handling of duplicate gene name in filter_prot (01d750e\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#import","title":"(Import)","text":"<ul> <li> <p>Add cleanup after import (b290ec8\u2026)</p> </li> <li> <p>Add support for pd3.2 import (592a814\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#plot","title":"(Plot)","text":"<ul> <li>Add plot_abundnace wrapper from pdata (8e2a01c\u2026)</li> </ul>"},{"location":"CHANGELOG/#build-system_2","title":"Build System","text":"<ul> <li> <p>Update pyproject.toml (18af439\u2026)</p> </li> <li> <p>Update pyproject.toml and workflow bug (19f4c7a\u2026)</p> </li> <li> <p>Fix changelog yml tag fetch error (5b20d69\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#ci_3","title":"CI","text":""},{"location":"CHANGELOG/#changelog_3","title":"(Changelog)","text":"<ul> <li>Upload changelog.md (2be787e\u2026)</li> </ul>"},{"location":"CHANGELOG/#pytestini","title":"(Pytest.ini)","text":"<ul> <li>Add test \"slow\" marker (8280133\u2026)</li> </ul>"},{"location":"CHANGELOG/#chores_2","title":"Chores","text":""},{"location":"CHANGELOG/#changelog_4","title":"(Changelog)","text":"<ul> <li> <p>Changelog sync to docs (84e991d\u2026)</p> </li> <li> <p>Final updates (6b94c5b\u2026)</p> </li> <li> <p>Edit changelog workflow (0ec6ab5\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#docs","title":"(Docs)","text":"<ul> <li> <p>Update deploy workflow to use committed changelog (9fa1705\u2026)</p> </li> <li> <p>Fix github workflow bugs (1ef2b00\u2026)</p> </li> <li> <p>Fix workflow yml (cc2544f\u2026)</p> </li> <li> <p>Fix changelog yml workflow (cb956b1\u2026)</p> </li> <li> <p>Fix changelog toml format (7c5dcc8\u2026)</p> </li> <li> <p>Fix attempt for workflow (506109b\u2026)</p> </li> <li> <p>Update full changelog [skip ci] (b8b5634\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (c3a8704\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (2356fff\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (9910211\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (b5d04c7\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (7e5d8b6\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (a688e34\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (265982c\u2026)</p> </li> <li> <p>Update changelogs [skip ci] (ca558ad\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#workflow","title":"(Workflow)","text":"<ul> <li>Build and deploy runs after changelog is finished (97491c3\u2026)</li> </ul>"},{"location":"CHANGELOG/#documentation_1","title":"Documentation","text":""},{"location":"CHANGELOG/#coc","title":"(Coc)","text":"<ul> <li>Add contributor covenant code of conduct (72fd27e\u2026)</li> </ul>"},{"location":"CHANGELOG/#contributing","title":"(Contributing)","text":"<ul> <li> <p>Update contributing.md file (4002003\u2026)</p> </li> <li> <p>Update markdown files (d72e6fa\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#fixed_2","title":"Fixed","text":""},{"location":"CHANGELOG/#base_1","title":"(Base)","text":"<ul> <li>Anndata automatically aligns X_raw, removed function and tests (ce82a52\u2026)</li> </ul>"},{"location":"CHANGELOG/#export","title":"(Export)","text":"<ul> <li>Handle export of .X (a3028bd\u2026)</li> </ul>"},{"location":"CHANGELOG/#filtering_4","title":"(Filtering)","text":"<ul> <li>Fix bug on import with non-matching obs/summary after nan protein cleanup (f61c0b5\u2026)</li> </ul>"},{"location":"CHANGELOG/#import_1","title":"(Import)","text":"<ul> <li>Renaming scheme for pd prot var (9bcbf44\u2026)</li> </ul>"},{"location":"CHANGELOG/#plotting","title":"(Plotting)","text":"<ul> <li>Fix bug for volcano_df handling of \"not comparable\" (1d3d87b\u2026)</li> </ul>"},{"location":"CHANGELOG/#other_2","title":"Other","text":""},{"location":"CHANGELOG/#identifier","title":"(Identifier)","text":"<ul> <li>Fix indentation on update_identifiers tip (90b7bec\u2026)</li> </ul>"},{"location":"CHANGELOG/#package-name","title":"(Package name)","text":"<ul> <li>Rename scviz to scpviz (9bc6347\u2026)</li> </ul>"},{"location":"CHANGELOG/#style_1","title":"Style","text":""},{"location":"CHANGELOG/#changelog_5","title":"(Changelog)","text":"<ul> <li> <p>Edit markdown formatting for docs changelog (ff92df7\u2026)</p> </li> <li> <p>Update parsers to match conventional commit format (a870d32\u2026)</p> </li> </ul>"},{"location":"CHANGELOG/#de","title":"(De)","text":"<ul> <li>Add comment on metaboanalyst median normalization (42c0260\u2026)</li> </ul>"},{"location":"CHANGELOG/#summary","title":"(Summary)","text":"<ul> <li>Edit table of usage scenarios for update_summary to be clearer (ffd2a37\u2026)</li> </ul>"},{"location":"CHANGELOG/#tests_1","title":"Tests","text":""},{"location":"CHANGELOG/#filteringimport","title":"(Filtering,import)","text":"<ul> <li>Add tests for duplicate gene handling, import pd32 (4cf1b86\u2026)</li> </ul>"},{"location":"CHANGELOG/#test-files","title":"(Test files)","text":"<ul> <li>Add test for pd3.2 import with prot and pep, upload pd3.2 mock files (3c96c45\u2026)</li> </ul>"},{"location":"CHANGELOG/#0.4.0-alpha","title":"0.4.0-alpha October 28, 2025","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":""},{"location":"CHANGELOG/#chores_3","title":"Chores","text":"<ul> <li>Add git-cliff config and changelog workflows (0159d05\u2026)</li> </ul>"},{"location":"CHANGELOG/#0.3.0-alpha","title":"0.3.0-alpha October 08, 2025","text":""},{"location":"CHANGELOG/#added_3","title":"Added","text":""},{"location":"CHANGELOG/#changed_1","title":"Changed","text":""},{"location":"CHANGELOG/#documentation_2","title":"Documentation","text":"<ul> <li>Include changelog in docs (9d7dbc0\u2026)</li> </ul>"},{"location":"CHANGELOG/#fixed_3","title":"Fixed","text":""},{"location":"CHANGELOG/#other_3","title":"Other","text":""},{"location":"CHANGELOG/#tests_2","title":"Tests","text":""},{"location":"admonitions/","title":"Admonitions","text":"<p>Example of admonition/callout with title:</p> <p>Title of the callout</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>No title and inner note!</p> Note <p>Auto-expanded nested</p> <p>Collapsible callout:</p> Collapsible callout <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Admonitions that use the inline modifiers must be declared prior to the content block you want to place them beside</p> <p>note, abstract, info, tip, success, question, warning, failure, danger, bug, example, quote</p> <p>Link here: https://squidfunk.github.io/mkdocs-material/reference/admonitions/#inline-blocks-inline</p>"},{"location":"code-examples/","title":"code examples?","text":"<p>An example of a codeblock for Python: add_numbers.py<pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre></p> <p>Example codeblock with lines highlighted: code-examples.md<pre><code>// Function to concatenate two strings\nfunction concatenateStrings(str1, str2) {\n  return str1 + str2;\n}\n\n// Example usage\nconst result = concatenateStrings(\"Hello, \", \"World!\");\nconsole.log(\"The concatenated string is:\", result);\n</code></pre></p>"},{"location":"content-tabs/","title":"Content-Tabs","text":""},{"location":"content-tabs/#content-tabs","title":"Content Tabs","text":"<p>This is some examples of content tabs.</p>"},{"location":"content-tabs/#generic-content","title":"Generic Content","text":"Plain textUnordered listOrdered list <p>This is some plain text</p> <ul> <li>First item</li> <li>Second item</li> <li>Third item</li> </ul> <ol> <li>First item</li> <li>Second item</li> <li>Third item</li> </ol>"},{"location":"content-tabs/#code-blocks-in-content-tabs","title":"Code Blocks in Content Tabs","text":"PythonJavaScript <pre><code>def main():\n    print(\"Hello world!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>function main() {\n    console.log(\"Hello world!\");\n}\n\nmain();\n</code></pre>"},{"location":"diagram/","title":"Diagram Examples","text":""},{"location":"diagram/#flowcharts","title":"Flowcharts","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Failure?};\n  B --&gt;|Yes| C[Investigate...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Success!];</code></pre>"},{"location":"diagram/#sequence-diagrams","title":"Sequence Diagrams","text":"<pre><code>sequenceDiagram\n  autonumber\n  Server-&gt;&gt;Terminal: Send request\n  loop Health\n      Terminal-&gt;&gt;Terminal: Check for health\n  end\n  Note right of Terminal: System online\n  Terminal--&gt;&gt;Server: Everything is OK\n  Terminal-&gt;&gt;Database: Request customer data\n  Database--&gt;&gt;Terminal: Customer data</code></pre>"},{"location":"diagram/#definitely-look-into-class-diagram","title":"definitely look into class diagram","text":"<pre><code>classDiagram\n  Person &lt;|-- Student\n  Person &lt;|-- Professor\n  Person : +String name\n  Person : +String phoneNumber\n  Person : +String emailAddress\n  Person: +purchaseParkingPass()\n  Address \"1\" &lt;-- \"0..1\" Person:lives at\n  class Student{\n    +int studentNumber\n    +int averageMark\n    +isEligibleToEnrol()\n    +getSeminarsTaken()\n  }\n  class Professor{\n    +int salary\n  }\n  class Address{\n    +String street\n    +String city\n    +String state\n    +int postalCode\n    +String country\n    -validate()\n    +outputAsLabel()  \n  }</code></pre> <p>Another link: https://squidfunk.github.io/mkdocs-material/reference/diagrams/#using-state-diagrams</p>"},{"location":"dev/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"dev/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"dev/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"dev/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"dev/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"dev/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at sr_pang@hotmail.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"dev/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"dev/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"dev/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"dev/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"dev/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"dev/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"dev/contributing/","title":"<code>scpviz</code> Contribution Guidelines","text":"<p>Thank you for your interest in contributing to scpviz, an open-source Python package for visualizing and analyzing single-cell and bulk proteomics data. We welcome contributions from the community to help improve, expand, and document the functionality of scpviz.</p>"},{"location":"dev/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>By participating in this project, you agree to abide by the Contributor Covenant. Please be respectful and considerate in your interactions with others.</p>"},{"location":"dev/contributing/#how-to-contribute","title":"How to Contribute","text":"<p>To get an overview of the project, read the README file.</p> <p>There are several ways you can contribute to scpviz, including but not limited to:</p> <ul> <li>asking and answering questions in Discussions,</li> <li>reporting bugs and requesting features by submitting new issues,</li> <li>adding new features and fixing bugs by creating pull requests (PRs),</li> <li>improving and maintaining consistency in the documentation (including docstrings and tutorials), and</li> <li>providing reproducible examples and workflows in Jupyter notebooks.</li> </ul>"},{"location":"dev/contributing/#getting-started","title":"Getting Started","text":""},{"location":"dev/contributing/#issues","title":"Issues","text":""},{"location":"dev/contributing/#open-a-new-issue","title":"Open a New Issue","text":"<p>Before reporting a bug or requesting a feature, search to see if a related issue already exists. If not, you can submit a new issue \u2014 make sure to include:</p> <ul> <li>a clear and descriptive title,</li> <li>relevant environment or dataset information (if applicable), and</li> <li>a minimal, reproducible example (if possible).</li> </ul>"},{"location":"dev/contributing/#solve-an-issue","title":"Solve an Issue","text":"<p>Browse through the existing issues to find one that interests you. You can filter by labels (e.g., feature, bug, enhancement). If you find an issue you\u2019d like to work on, comment to let maintainers know and open a PR when ready.</p>"},{"location":"dev/contributing/#make-changes","title":"Make Changes","text":"<p>To contribute to scpviz, use the fork and pull request workflow described below.</p> <ol> <li>Fork the repository.</li> <li> <p>Clone your fork locally and navigate to it:</p> <p>git clone https://github.com/gnaprs/scpviz.git    cd scpviz</p> </li> <li> <p>Create a new branch for your feature or fix:</p> <p>git checkout -b  <li> <p>Install scpviz and its development dependencies:</p> <p>pip install -e .[dev]</p> </li> <p>You may also want to create and activate a virtual environment before installing dependencies:</p> <pre><code>   python3 -m venv .venv\n   source .venv/bin/activate\n</code></pre> <ol> <li> <p>Run tests to confirm everything works before editing:</p> <p>pytest tests/ -v</p> </li> </ol>"},{"location":"dev/contributing/#development-guidelines","title":"Development Guidelines","text":"<p>Please follow these best practices when contributing code:</p> <ul> <li>Follow the PEP 8 style guide.</li> <li>Write clear, consistent docstrings in the Google style.</li> <li>Add pytest unit tests for new functions and features.</li> <li>Use meaningful variable names and comments where appropriate.</li> <li>Keep imports organized and minimal.</li> <li>When modifying documentation, ensure that <code>mkdocs build --strict</code> completes successfully.</li> </ul>"},{"location":"dev/contributing/#commit-your-update","title":"Commit Your Update","text":"<p>When your changes are ready:</p> <ol> <li> <p>Ensure that all unit tests pass:</p> <p>pytest tests/ -v</p> </li> <li> <p>Stage and commit your changes:</p> <p>git add .    git commit -m \"\" <li> <p>Push your branch to your fork:</p> <p>git push origin"},{"location":"dev/contributing/#pull-request","title":"Pull Request","text":"<p>To contribute your changes to the main scpviz repository, create a pull request. The project maintainers will review your PR and provide feedback. If your changes align with the project goals and pass all tests, they will be merged into the main branch.</p>"},{"location":"dev/contributing/#documentation-contributions","title":"Documentation Contributions","text":"<p>The documentation for scpviz is built with MkDocs Material.</p> <p>To build and preview locally:</p> <pre><code>mkdocs serve\n</code></pre> <p>Your changes will automatically rebuild the site in your browser.</p> <p>When merged to <code>main</code>, the documentation is automatically deployed to https://gnaprs.github.io/scpviz/ via GitHub Actions (<code>ci.yml</code>).</p>"},{"location":"dev/contributing/#release-process-maintainers","title":"Release Process (Maintainers)","text":"<p>When ready to publish a new version:</p> <ol> <li>Update the version number in <code>pyproject.toml</code>.</li> <li>Commit, tag, and push:    <pre><code>git commit -am \"Bump version to v0.X.Y\"\ngit tag -a v0.X.Y -m \"Release v0.X.Y\"\ngit push origin main --tags\n</code></pre></li> <li>Build and upload to PyPI:    <pre><code>python -m build\ntwine upload dist/*\n</code></pre></li> <li>Verify:</li> <li>PyPI: https://pypi.org/project/scpviz/</li> <li>Docs: https://gnaprs.github.io/scpviz/</li> <li>Coverage: https://codecov.io/gh/gnaprs/scpviz</li> </ol>"},{"location":"dev/contributing/#additional-resources","title":"Additional Resources","text":"<ul> <li>PEP 621: <code>pyproject.toml</code> metadata format  </li> <li>MkDocs Material Documentation </li> <li>pytest Documentation </li> <li>Codecov Integration Guide</li> </ul> <p>Thank you for helping make scpviz a reliable, open, and community-driven platform for single-cell and spatial proteomics research.</p>"},{"location":"reference/","title":"<code>pAnnData</code> Overview","text":"<p>The <code>pAnnData</code> object is the central data container in scpviz, extending the AnnData structure for single-cell and bulk proteomics. It integrates matched protein-level and peptide-level matrices, along with metadata, summaries, and a protein\u2013peptide relational structure (RS matrix).</p> <p>This page introduces the core design of <code>pAnnData</code> and shows how to import data from supported formats.</p> <p>Key Features</p> <ul> <li>Matched <code>AnnData</code> objects for proteins (<code>.prot</code>) and peptides (<code>.pep</code>)</li> <li>Support for multiple layers (raw, normalized, imputed, etc.)</li> <li>Integrated metadata (<code>.metadata</code>) and summary tables (<code>.summary</code>)</li> <li>Tracking of filtering, normalization, and analysis history</li> <li>Compatible with all scpviz modules (plotting, enrichment, filtering, etc.)</li> </ul> <pre><code>flowchart LR\n    subgraph pAnnData[\"`pAnnData`\"]\n        P[\"`.prot  \n        (protein AnnData)`\"]\n        Q[\"`.pep  \n        (peptide AnnData)`\"]\n        S[\"`.summary  \n        (sample-level table)`\"]\n        M[\"`.metadata  \n        (dict of metadata)`\"]\n        R[\"`RS matrix  \n        (protein \u00d7 peptide mapping)`\"]\n    end\n\n    P -- \"proteins \u2194 peptides\" --&gt; R\n    Q -- \"peptides \u2194 proteins\" --&gt; R\n    P -. \"linked by sample IDs\" .-&gt; S\n    Q -. \"linked by sample IDs\" .-&gt; S\n    M --&gt; P\n    M --&gt; Q\n    M --&gt; S</code></pre>"},{"location":"reference/#importing-data","title":"Importing Data","text":"<p>Data can be imported into <code>pAnnData</code> directly from DIA-NN, Proteome Discoverer, or other supported formats:</p> <pre><code>from scpviz import pAnnData as pAnnData\nfrom scpviz import plotting as scplt\nfrom scpviz import utils as scutils\n\n# From DIA-NN report\npdata = pAnnData.import_data(source_type='diann', report_file =\"report.tsv\")\n\n# From Proteome Discoverer output\npdata = pAnnData.import_data(source_type='pd', prot_file =\"proteomediscoverer_prot.txt\", pept_file =\"proteomediscoverer_pep.txt\")\n</code></pre> <p>Once imported, the <code>pAnnData</code> object serves as the entry point for downstream workflows: filtering, normalization, imputation, visualization, and enrichment analysis.</p>"},{"location":"reference/#workflow-pipeline","title":"Workflow Pipeline","text":"<p>The <code>pAnnData</code> object enables a modular analysis pipeline for single-cell and bulk proteomics. Each step builds on the previous one, but you can skip or repeat steps depending on your dataset and analysis goals.</p> <pre><code>graph TB\n    A[\"`Import data  \n    (DIA-NN / PD)`\"] --&gt; B[\"`Parse metadata  \n    (.obs from filenames)`\"]\n    B --&gt; C[\"`Filter proteins/peptides  \n    (\u22652 unique peptides, sample queries)`\"]\n    C --&gt; D[\"`Normalize  \n    (global, reference feature, directLFQ)`\"]\n    D --&gt; E[\"`Impute missing values  \n    (KNN / group-wise)`\"]\n    E --&gt; F[\"`Visualize data  \n    (abundance, PCA/UMAP, clustermap, raincloud, volcano)`\"]\n    F --&gt; G[\"`Differential Expression  \n    (mean, pairwise, peptide-level)`\"]\n    G --&gt; H[\"`Enrichment (STRING)  \n    (GSEA, GO, PPI)`\"]\n    B --&gt; I[\"`Export results`\"]\n\n    %% Optional side paths\n    B -. \"QC summaries\" .-&gt; F\n    C -. \"RS matrix checks\" .-&gt; F\n    G -. \"ranked/unranked lists\" .-&gt; H\n    D .-&gt; I\n    F .-&gt; I\n    G .-&gt; I</code></pre> <p>Tutorials</p> <p>Each step of the pipeline is explained in detail in the tutorials:</p> <ul> <li>Importing and Exporting Data</li> <li>Filtering  </li> <li>Imputation + Normalization </li> <li>Plotting</li> <li>Differential Expression </li> <li>Enrichment + Networks </li> </ul> <p>For a guided introduction, start with the Quickstart.  </p>"},{"location":"reference/#modules","title":"Modules","text":"<p>In addition to the core <code>pAnnData</code> class, scpviz provides two standalone modules that support analysis and visualization:  </p> <ul> <li> <p><code>scpviz.utils</code>   A collection of helper functions for data processing, filtering, formatting, and interacting with external resources such as UniProt.   These functions are primarily used internally but can also be useful for advanced users who need finer control over their workflows.  </p> </li> <li> <p><code>scpviz.plotting</code>   A set of high-level visualization utilities designed to work seamlessly with <code>pAnnData</code>.   Functions include abundance plots, rank plots, raincloud plots, clustermaps, UMAP/PCA projections, and UpSet diagrams.   Each plotting function is designed to accept a <code>matplotlib.axes.Axes</code> object for flexible integration into custom figure layouts.  </p> </li> </ul>"},{"location":"reference/#developer-utilities","title":"Developer Utilities","text":"<p>scpviz also includes a small set of developer-focused tools that help maintain consistency and internal state:  </p> <ul> <li> <p><code>TrackedDataFrame</code>   A subclass of <code>pandas.DataFrame</code> that marks its parent <code>pAnnData</code> object as \u201cstale\u201d whenever it is modified directly.   This ensures that summary tables and metadata stay consistent with the main data layers.  </p> </li> <li> <p>Hidden functions   Internal helpers that are not part of the standard API.   These are documented for completeness but should generally not be used directly in analysis workflows.  </p> </li> </ul> <p>Developer utilities</p> <p>These tools are included for package maintainers and power users. Most end-users will not need to interact with them directly.  </p>"},{"location":"reference/TrackedDataFrame/","title":"Tracked DataFrame","text":""},{"location":"reference/TrackedDataFrame/#src.scpviz.TrackedDataFrame.TrackedDataFrame","title":"TrackedDataFrame","text":"<p>               Bases: <code>DataFrame</code></p> <p>A subclass of :class:<code>pandas.DataFrame</code> that integrates with a parent  :class:<code>pAnnData</code> object to track when derived tables (e.g., <code>.summary</code>)  have been modified outside the canonical workflow.</p> <p>Any in-place modifications automatically mark the parent object as \"stale\" using the provided callback (<code>mark_stale_fn</code>). This ensures downstream code can detect unsynchronized changes and prompt recomputation if needed.</p> <p>Attributes:</p> Name Type Description <code>_parent</code> <code>pAnnData</code> <p>The parent object associated with this DataFrame.</p> <code>_mark_stale_fn</code> <code>callable</code> <p>Function called when the DataFrame is modified.</p> <code>_raw_loc,</code> <code>_raw_iloc</code> <p>Direct accessors for untracked indexing (safe use).</p> <p>Internal Utility</p> <p><code>TrackedDataFrame</code> is primarily intended for internal use within <code>pAnnData</code>. Direct use in analysis code is not recommended, as stale-tracking may interfere with expected pandas behaviors.</p> <p>Tip</p> <p>Use <code>.raw_loc</code> and <code>.raw_iloc</code> to bypass stale-marking when read-only access is explicitly desired.</p> Source code in <code>src/scpviz/TrackedDataFrame.py</code> <pre><code>class TrackedDataFrame(pd.DataFrame):\n    \"\"\"\n    A subclass of :class:`pandas.DataFrame` that integrates with a parent \n    :class:`pAnnData` object to track when derived tables (e.g., `.summary`) \n    have been modified outside the canonical workflow.\n\n    Any in-place modifications automatically mark the parent object as \"stale\"\n    using the provided callback (`mark_stale_fn`). This ensures downstream\n    code can detect unsynchronized changes and prompt recomputation if needed.\n\n    Attributes:\n        _parent (pAnnData): The parent object associated with this DataFrame.\n        _mark_stale_fn (callable): Function called when the DataFrame is modified.\n        _raw_loc, _raw_iloc: Direct accessors for untracked indexing (safe use).\n\n    !!! warning \"Internal Utility\"\n        `TrackedDataFrame` is primarily intended for internal use within `pAnnData`.  \n        Direct use in analysis code is not recommended, as stale-tracking may\n        interfere with expected pandas behaviors.\n\n    !!! tip\n        Use `.raw_loc` and `.raw_iloc` to bypass stale-marking when read-only\n        access is explicitly desired.\n\n    \"\"\"\n    _metadata = [\"_parent\", \"_mark_stale_fn\"]\n\n    @property\n    def _constructor(self):\n        return TrackedDataFrame\n\n    def __new__(cls, *args, **kwargs):\n        return super().__new__(cls)\n\n    def __init__(self, *args, parent=None, mark_stale_fn=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._parent = parent\n        self._mark_stale_fn = mark_stale_fn\n        # Bind raw .loc/.iloc for safe use\n        self._raw_loc = super().loc\n        self._raw_iloc = super().iloc\n\n    def __repr__(self):\n        base = super().__repr__()\n        stale_msg = \"\"\n        if getattr(self, \"_mark_stale_fn\", None) and getattr(self._parent, \"_summary_is_stale\", False):\n            stale_msg = \"\\n\u26a0\ufe0f  [TrackedDataFrame] This summary has been modified and is not synced back to .obs.\"\n        return base + stale_msg\n\n    def _mark_stale(self):\n        \"\"\"\n        Mark the parent pAnnData object as stale.\n\n        Called internally before any modifying operations (e.g. `__setitem__`,\n        `drop`, `assign`). Triggers the parent\u2019s `_mark_stale_fn` if provided.\n        \"\"\"\n        if self._mark_stale_fn is not None:\n            self._mark_stale_fn()\n\n    def __setitem__(self, key, value):\n        self._mark_stale()\n        return super().__setitem__(key, value)\n\n    def drop(self, *args, **kwargs):\n        self._mark_stale()\n        return super().drop(*args, **kwargs)\n\n    def assign(self, *args, **kwargs):\n        self._mark_stale()\n        return super().assign(*args, **kwargs)\n\n    def pop(self, *args, **kwargs):\n        self._mark_stale()\n        return super().pop(*args, **kwargs)\n\n    @property\n    def loc(self):\n        self._mark_stale()\n        return super().loc\n\n    @property\n    def iloc(self):\n        self._mark_stale()\n        return super().iloc\n</code></pre>"},{"location":"reference/plotting/","title":"Plotting","text":"<p>This module provides a collection of plotting utilities for visualizing protein and peptide abundance data, quality control metrics, and results of statistical analyses. Functions are organized into categories based on their purpose, with paired \"plot\" and \"mark\" functions where applicable.</p> <p>Functions are written to work seamlessly with the <code>pAnnData</code> object structure and metadata conventions in scpviz.</p>"},{"location":"reference/plotting/#src.scpviz.plotting--distribution-and-abundance-plots","title":"Distribution and Abundance Plots","text":"<p>Functions:</p> Name Description <code>plot_abundance</code> <p>Violin/box/strip plots of protein or peptide abundance.</p> <code>plot_abundance_housekeeping</code> <p>Plot abundance of housekeeping proteins.</p> <code>plot_rankquant</code> <p>Rank abundance scatter distributions across groups.</p> <code>mark_rankquant</code> <p>Highlight specific features on a rank abundance plot.</p> <code>plot_raincloud</code> <p>Raincloud plot (violin + box + scatter) of distributions.</p> <code>mark_raincloud</code> <p>Highlight specific features on a raincloud plot.</p>"},{"location":"reference/plotting/#src.scpviz.plotting--multivariate-dimension-reduction","title":"Multivariate Dimension Reduction","text":"<p>Functions:</p> Name Description <code>plot_pca</code> <p>Principal Component Analysis (PCA) scatter plot.</p> <code>plot_pca_scree</code> <p>Scree plot of PCA variance explained.</p> <code>plot_umap</code> <p>UMAP projection for nonlinear dimensionality reduction.</p> <code>resolve_plot_colors</code> <p>Helper function for resolving PCA/UMAP colors.</p>"},{"location":"reference/plotting/#src.scpviz.plotting--clustering-and-heatmaps","title":"Clustering and Heatmaps","text":"<p>Functions:</p> Name Description <code>plot_clustermap</code> <p>Clustered heatmap of proteins/peptides \u00d7 samples.</p>"},{"location":"reference/plotting/#src.scpviz.plotting--differential-expression-and-volcano-plots","title":"Differential Expression and Volcano Plots","text":"<p>Functions:</p> Name Description <code>plot_volcano</code> <p>Volcano plot of differential expression results.</p> <code>mark_volcano</code> <p>Highlight specific features on a volcano plot.</p> <code>add_volcano_legend</code> <p>Add standard legend handles for volcano plots.</p>"},{"location":"reference/plotting/#src.scpviz.plotting--enrichment-plots","title":"Enrichment Plots","text":"<p>Functions:</p> Name Description <code>plot_enrichment_svg</code> <p>Plot STRING enrichment results (forwarded from <code>enrichment.py</code>).</p>"},{"location":"reference/plotting/#src.scpviz.plotting--set-operations","title":"Set Operations","text":"<p>Functions:</p> Name Description <code>plot_venn</code> <p>Venn diagrams for 2 to 3 sets.</p> <code>plot_upset</code> <p>UpSet diagrams for &gt;3 sets.</p>"},{"location":"reference/plotting/#src.scpviz.plotting--summaries-and-quality-control","title":"Summaries and Quality Control","text":"<p>Functions:</p> Name Description <code>plot_summary</code> <p>Bar plots summarizing sample-level metadata (e.g., protein counts).</p> <code>plot_significance</code> <p>Add significance bars to plots.</p> <code>plot_cv</code> <p>Boxplots of coefficient of variation (CV) across groups.</p>"},{"location":"reference/plotting/#src.scpviz.plotting--notes-and-tips","title":"Notes and Tips","text":"<p>Tip</p> <ul> <li>Most functions accept a <code>matplotlib.axes.Axes</code> as the first argument for flexible subplot integration. <code>ax</code> can be defined as such:</li> </ul> <pre><code>import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(6,4)) # configure size as needed\n</code></pre> <ul> <li>\"Mark\" functions are designed to be used immediately after their paired \"plot\" functions to highlight features of interest.</li> </ul>"},{"location":"reference/plotting/#src.scpviz.plotting.add_volcano_legend","title":"add_volcano_legend","text":"<pre><code>add_volcano_legend(ax, colors=None)\n</code></pre> <p>Add a standard legend for volcano plots.</p> <p>This function appends a legend to a volcano plot axis, showing handles for upregulated, downregulated, and non-significant features. Colors can be customized, but default to grey, red, and blue.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis object to which the legend will be added.</p> required <code>colors</code> <code>dict</code> <p>Custom colors for significance categories. Keys must include <code>\"upregulated\"</code>, <code>\"downregulated\"</code>, and <code>\"not significant\"</code>. Defaults to:</p> <pre><code>{\n    \"not significant\": \"grey\",\n    \"upregulated\": \"red\",\n    \"downregulated\": \"blue\"\n}\n</code></pre> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def add_volcano_legend(ax, colors=None):\n    \"\"\"\n    Add a standard legend for volcano plots.\n\n    This function appends a legend to a volcano plot axis, showing handles for\n    upregulated, downregulated, and non-significant features. Colors can be\n    customized, but default to grey, red, and blue.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis object to which the legend will be added.\n\n        colors (dict, optional): Custom colors for significance categories.\n            Keys must include `\"upregulated\"`, `\"downregulated\"`, and\n            `\"not significant\"`. Defaults to:\n\n            ```python\n            {\n                \"not significant\": \"grey\",\n                \"upregulated\": \"red\",\n                \"downregulated\": \"blue\"\n            }\n            ```\n\n    Returns:\n        None\n    \"\"\"\n    from matplotlib.lines import Line2D\n    import numpy as np\n\n    default_colors = {'not significant': 'grey', 'upregulated': 'red', 'downregulated': 'blue'}\n    if colors is None:\n        colors = default_colors.copy()\n    else:\n        default_colors.update(colors)\n        colors = default_colors\n\n    handles = [\n        Line2D([0], [0], marker='o', color='w', label='Up', markerfacecolor=colors['upregulated'], markersize=6),\n        Line2D([0], [0], marker='o', color='w', label='Down', markerfacecolor=colors['downregulated'], markersize=6),\n        Line2D([0], [0], marker='o', color='w', label='NS', markerfacecolor=colors['not significant'], markersize=6)\n    ]\n    ax.legend(handles=handles, loc='upper right', frameon=True, fontsize=7)\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.get_color","title":"get_color","text":"<pre><code>get_color(resource_type: str, n=None)\n</code></pre> <p>Generate a list of colors, a colormap, or a palette from package defaults.</p> <p>Parameters:</p> Name Type Description Default <code>resource_type</code> <code>str</code> <p>The type of resource to generate. Options are: - 'colors': Return a list of hex color codes. - 'cmap': Return a matplotlib colormap. - 'palette': Return a seaborn palette. - 'show': Display all 7 base colors.</p> required <code>n</code> <code>int</code> <p>The number of colors or colormaps to generate. Required for 'colors' and 'cmap'. Colors will repeat if n &gt; 7.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>colors</code> <code>list of str</code> <p>If <code>resource_type='colors'</code>, a list of hex color strings. Repeats colors if n &gt; 7.</p> <code>cmap</code> <code>LinearSegmentedColormap</code> <p>If <code>resource_type='cmap'</code>.</p> <code>palette</code> <code>color_palette</code> <p>If <code>resource_type='palette'</code>.</p> <code>None</code> <p>If <code>resource_type='show'</code>, displays the available colors.</p> <p>Default Colors</p> <p>The following base colors are used (hex codes):</p> <pre><code>['#FC9744', '#00AEE8', '#9D9D9D', '#6EDC00', '#F4D03F', '#FF0000', '#A454C7']\n</code></pre> <p> </p> Example <p>Get list of 5 colors:     <pre><code>colors = get_color('colors', 5)\n</code></pre></p> <p>Get default cmap:     <pre><code>cmap = get_color('cmap', 2)\n</code></pre></p> <p>Get default palette:     <pre><code>palette = get_color('palette')\n</code></pre></p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def get_color(resource_type: str, n=None):\n    \"\"\"\n    Generate a list of colors, a colormap, or a palette from package defaults.\n\n    Args:\n        resource_type (str): The type of resource to generate. Options are:\n            - 'colors': Return a list of hex color codes.\n            - 'cmap': Return a matplotlib colormap.\n            - 'palette': Return a seaborn palette.\n            - 'show': Display all 7 base colors.\n\n        n (int, optional): The number of colors or colormaps to generate.\n            Required for 'colors' and 'cmap'. Colors will repeat if n &gt; 7.\n\n    Returns:\n        colors (list of str): If ``resource_type='colors'``, a list of hex color strings. Repeats colors if n &gt; 7.\n        cmap (matplotlib.colors.LinearSegmentedColormap): If ``resource_type='cmap'``.\n        palette (seaborn.color_palette): If ``resource_type='palette'``.\n        None: If ``resource_type='show'``, displays the available colors.\n\n    !!! info \"Default Colors\"\n\n        The following base colors are used (hex codes):\n\n            ['#FC9744', '#00AEE8', '#9D9D9D', '#6EDC00', '#F4D03F', '#FF0000', '#A454C7']        \n\n        &lt;div style=\"display:flex;gap:0.5em;\"&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#FC9744;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#00AEE8;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#9D9D9D;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#6EDC00;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#F4D03F;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#FF0000;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#A454C7;border:1px solid #000\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n\n    Example:\n        Get list of 5 colors:\n            ```python\n            colors = get_color('colors', 5)\n            ```\n\n        &lt;div style=\"display:flex;gap:0.5em;\"&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#FC9744;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#00AEE8;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#9D9D9D;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#6EDC00;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.5em;height:1.5em;background:#F4D03F;border:1px solid #000\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n\n        Get default cmap:\n            ```python\n            cmap = get_color('cmap', 2)\n            ```\n        &lt;div style=\"width:150px;height:20px;background:linear-gradient(to right, white, #FC9744);border:1px solid #000\"&gt;&lt;/div&gt;\n        &lt;div style=\"width:150px;height:20px;background:linear-gradient(to right, white, #00AEE8);border:1px solid #000\"&gt;&lt;/div&gt;\n\n        Get default palette:\n            ```python\n            palette = get_color('palette')\n            ```\n\n        &lt;div style=\"display:flex;gap:0.3em;\"&gt;\n            &lt;div style=\"width:1.2em;height:1.2em;background:#FC9744;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.2em;height:1.2em;background:#00AEE8;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.2em;height:1.2em;background:#9D9D9D;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.2em;height:1.2em;background:#6EDC00;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.2em;height:1.2em;background:#F4D03F;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.2em;height:1.2em;background:#FF0000;border:1px solid #000\"&gt;&lt;/div&gt;\n            &lt;div style=\"width:1.2em;height:1.2em;background:#A454C7;border:1px solid #000\"&gt;&lt;/div&gt;\n        &lt;/div&gt;\n    \"\"\"\n\n    # --- \n    # Create a list of colors\n    base_colors = ['#FC9744', '#00AEE8', '#9D9D9D', '#6EDC00', '#F4D03F', '#FF0000', '#A454C7']\n    # ---\n\n    if resource_type == 'colors':\n        if n is None:\n            raise ValueError(\"Parameter 'n' must be specified when resource_type is 'colors'\")\n        if n &gt; len(base_colors):\n            warnings.warn(f\"Requested {n} colors, but only {len(base_colors)} available. Reusing from the start.\")\n        return [base_colors[i % len(base_colors)] for i in range(n)]\n\n    elif resource_type == 'cmap':\n        if n is None:\n            n = 1  # Default to generating one colormap from the first base color\n        if n &gt; len(base_colors):\n            warnings.warn(f\"Requested {n} colormaps, but only {len(base_colors)} base colors. Reusing from the start.\")\n        cmaps = []\n        for i in range(n):\n            color = base_colors[i % len(base_colors)]\n            cmap = mcolors.LinearSegmentedColormap.from_list(f'cmap_{i}', ['white', color])\n            cmaps.append(cmap)\n        return cmaps if n &gt; 1 else cmaps[0]\n\n    elif resource_type == 'palette':\n        return sns.color_palette(base_colors)\n\n    elif resource_type == 'show':\n        # Show palette and colormaps\n        fig, axs = plt.subplots(2, 1, figsize=(10, 5), gridspec_kw={'height_ratios': [1, 1]})\n\n        # Format labels as \"n: #HEX\"\n        hex_labels = [f\"{i}: {mcolors.to_hex(color)}\" for i, color in enumerate(base_colors)]\n\n        # --- Palette ---\n        for i, color in enumerate(base_colors):\n            axs[0].bar(i, 1, color=color)\n        axs[0].set_title(\"Base Colors (Colors and Palette)\")\n        axs[0].set_xticks(range(len(base_colors)))\n        axs[0].set_xticklabels(hex_labels, rotation=45, ha='right')\n        axs[0].set_yticks([])\n\n        # --- Colormaps ---\n        gradient = np.linspace(0, 1, 256).reshape(1, -1)\n        n_colors = len(base_colors)\n\n        for i, color in enumerate(base_colors):\n            cmap = mcolors.LinearSegmentedColormap.from_list(f'cmap_{i}', ['white', color])\n            axs[1].imshow(\n                gradient,\n                aspect='auto',\n                cmap=cmap,\n                extent=(i, i + 1, 0, 1)\n            )\n\n        axs[1].set_title(\"Colormaps\")\n        axs[1].set_xlim(0, n_colors)\n        axs[1].set_xticks(np.arange(n_colors) + 0.5)\n        axs[1].set_xticklabels(hex_labels, rotation=45, ha='right')\n        axs[1].set_yticks([])\n\n        plt.tight_layout()\n        plt.show()\n        return None\n\n    else:\n        raise ValueError(\"Invalid resource_type. Options are 'colors', 'cmap', and 'palette'\")\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.mark_raincloud","title":"mark_raincloud","text":"<pre><code>mark_raincloud(plot, pdata, mark_df, class_values, layer='X', on='protein', lowest_index=0, color='red', s=10, alpha=1)\n</code></pre> <p>Highlight specific features on a raincloud plot.</p> <p>This function marks selected proteins or peptides on an existing raincloud plot, using summary statistics written to <code>.var</code> during <code>plot_raincloud()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <code>Axes</code> <p>Axis containing the raincloud plot.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>mark_df</code> <code>DataFrame</code> <p>DataFrame containing entries to highlight. Must include an <code>\"Entry\"</code> column.</p> required <code>class_values</code> <code>list of str</code> <p>Class values to highlight (must match those used in <code>plot_raincloud</code>).</p> required <code>layer</code> <code>str</code> <p>Data layer to use. Default is <code>\"X\"</code>.</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Data level, either <code>\"protein\"</code> or <code>\"peptide\"</code>. Default is <code>\"protein\"</code>.</p> <code>'protein'</code> <code>lowest_index</code> <code>int</code> <p>Offset for horizontal positioning. Default is 0.</p> <code>0</code> <code>color</code> <code>str</code> <p>Marker color. Default is <code>\"red\"</code>.</p> <code>'red'</code> <code>s</code> <code>float</code> <p>Marker size. Default is 10.</p> <code>10</code> <code>alpha</code> <code>float</code> <p>Marker transparency. Default is 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis with highlighted features.</p> <p>Tip</p> <p>Works best when paired with <code>plot_raincloud()</code>, which computes and stores the required statistics in <code>.var</code>.</p> Example <p>Highlight specific proteins on a raincloud plot:     <pre><code>ax = scplt.plot_raincloud(\n    ax, pdata_filter, classes=\"size\", order=order,\n    color=colors, linewidth=0.5\n)\nscplt.mark_raincloud(\n    ax, pdata_filter, mark_df=prot_sc_df,\n    class_values=[\"sc\"], color=\"black\"\n)\n</code></pre></p> See Also <p>plot_raincloud: Generate raincloud plots with distributions per group. plot_rankquant: Alternative distribution visualization using rank abundance.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def mark_raincloud(plot,pdata,mark_df,class_values,layer = \"X\", on = 'protein',lowest_index=0,color='red',s=10,alpha=1):\n    \"\"\"\n    Highlight specific features on a raincloud plot.\n\n    This function marks selected proteins or peptides on an existing\n    raincloud plot, using summary statistics written to `.var` during\n    `plot_raincloud()`.\n\n    Args:\n        plot (matplotlib.axes.Axes): Axis containing the raincloud plot.\n        pdata (pAnnData): Input pAnnData object.\n        mark_df (pandas.DataFrame): DataFrame containing entries to highlight.\n            Must include an `\"Entry\"` column.\n        class_values (list of str): Class values to highlight (must match those\n            used in `plot_raincloud`).\n        layer (str): Data layer to use. Default is `\"X\"`.\n        on (str): Data level, either `\"protein\"` or `\"peptide\"`. Default is `\"protein\"`.\n        lowest_index (int): Offset for horizontal positioning. Default is 0.\n        color (str): Marker color. Default is `\"red\"`.\n        s (float): Marker size. Default is 10.\n        alpha (float): Marker transparency. Default is 1.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axis with highlighted features.\n\n    !!! tip \n\n        Works best when paired with `plot_raincloud()`, which computes and\n        stores the required statistics in `.var`.\n\n    Example:\n        Highlight specific proteins on a raincloud plot:\n            ```python\n            ax = scplt.plot_raincloud(\n                ax, pdata_filter, classes=\"size\", order=order,\n                color=colors, linewidth=0.5\n            )\n            scplt.mark_raincloud(\n                ax, pdata_filter, mark_df=prot_sc_df,\n                class_values=[\"sc\"], color=\"black\"\n            )\n            ```\n\n    See Also:\n        plot_raincloud: Generate raincloud plots with distributions per group.  \n        plot_rankquant: Alternative distribution visualization using rank abundance.\n    \"\"\"\n    adata = utils.get_adata(pdata, on)\n    names = mark_df['Entry'].tolist()\n    # TEST: check if names are in the data\n    pdata._check_rankcol(on, class_values)\n\n    for j, class_value in enumerate(class_values):\n        print('Class: ', class_value)\n\n        for i, txt in enumerate(names):\n            try:\n                y = np.log10(adata.var['Average: '+class_value].loc[txt])\n                x = lowest_index + j + .14 + 0.8\n            except Exception as e:\n                print(f\"Name {txt} not found in {on}.var. Check {on} name for spelling errors and whether it is in data.\")\n                continue\n            plot.scatter(x,y,marker='o',color=color,s=s, alpha=alpha)\n    return plot\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.mark_rankquant","title":"mark_rankquant","text":"<pre><code>mark_rankquant(plot, pdata, mark_df, class_values, layer='X', on='protein', color='red', s=10, alpha=1, show_label=True, label_type='accession')\n</code></pre> <p>Highlight specific features on a rank abundance plot.</p> <p>This function marks selected proteins or peptides on an existing rank abundance plot, optionally adding labels. It uses statistics stored in <code>.var</code> during <code>plot_rankquant()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>plot</code> <code>Axes</code> <p>Axis containing the rank abundance plot.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>mark_df</code> <code>DataFrame</code> <p>Features to highlight.</p> <ul> <li>DataFrame: Must include an <code>\"Entry\"</code> and <code>\"accession\"</code> column, and optionally   <code>\"Gene Names\"</code> if <code>label_type=\"gene\"</code>.   A typical way to generate this is using   <code>scutils.get_upset_query()</code>, e.g.:</li> </ul> <pre><code>size_upset = scutils.get_upset_contents(pdata_filter, classes=\"size\")\nprot_sc_df = scutils.get_upset_query(size_upset, present=[\"sc\"], absent=[\"5k\", \"10k\", \"20k\"])\n</code></pre> required <code>class_values</code> <code>list of str</code> <p>Class values to highlight (must match those used in <code>plot_rankquant</code>).</p> required <code>layer</code> <code>str</code> <p>Data layer to use. Default is <code>\"X\"</code>.</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Data level, either <code>\"protein\"</code> or <code>\"peptide\"</code>. Default is <code>\"protein\"</code>.</p> <code>'protein'</code> <code>color</code> <code>str</code> <p>Marker color. Default is <code>\"red\"</code>.</p> <code>'red'</code> <code>s</code> <code>float</code> <p>Marker size. Default is 10.</p> <code>10</code> <code>alpha</code> <code>float</code> <p>Marker transparency. Default is 1.</p> <code>1</code> <code>show_label</code> <code>bool</code> <p>Whether to display labels for highlighted features. Default is True.</p> <code>True</code> <code>label_type</code> <code>str</code> <p>Label type. Options:</p> <ul> <li><code>\"accession\"</code>: show accession IDs.</li> <li><code>\"gene\"</code>: map to gene names using <code>\"Gene Names\"</code> in <code>mark_df</code>.</li> </ul> <code>'accession'</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis with highlighted features.</p> <p>Tip</p> <p>Works best when paired with <code>plot_rankquant()</code>, which stores <code>Average</code>, <code>Stdev</code>, and <code>Rank</code> statistics in <code>.var</code>. Call <code>plot_rankquant()</code> first to generate these values, then use <code>mark_rankquant()</code> to overlay highlights.</p> Example <p>Plot rank abundance and highlight specific proteins:     <code>python     fig, ax = plt.subplots()     ax = scplt.plot_rankquant(         ax, pdata_filter, classes=\"size\", order=order,         cmap=cmaps, color=colors, s=10, calpha=1, alpha=0.005     )     size_upset = scutils.get_upset_contents(pdata_filter, classes=\"size\")     prot_sc_df = scutils.get_upset_query(         size_upset, present=[\"sc\"], absent=[\"5k\", \"10k\", \"20k\"]     )     scplt.mark_rankquant(         ax, pdata_filter, mark_df=prot_sc_df,         class_values=[\"sc\"], show_label=True,         color=\"darkorange\", label_type=\"gene\"     )</code>python</p> See Also <p>plot_rankquant: Generate rank abundance plots with statistics stored in <code>.var</code>. get_upset_query: Create a DataFrame of proteins based on set intersections (obs membership).</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def mark_rankquant(plot, pdata, mark_df, class_values, layer = \"X\", on = 'protein', color='red', s=10,alpha=1, show_label=True, label_type='accession'):\n    \"\"\"\n    Highlight specific features on a rank abundance plot.\n\n    This function marks selected proteins or peptides on an existing rank\n    abundance plot, optionally adding labels. It uses statistics stored in\n    `.var` during `plot_rankquant()`.\n\n    Args:\n        plot (matplotlib.axes.Axes): Axis containing the rank abundance plot.\n        pdata (pAnnData): Input pAnnData object.\n        mark_df (pandas.DataFrame): Features to highlight.\n\n            - DataFrame: Must include an `\"Entry\"` and `\"accession\"` column, and optionally\n              `\"Gene Names\"` if `label_type=\"gene\"`.  \n              A typical way to generate this is using\n              `scutils.get_upset_query()`, e.g.:\n\n              ```python\n              size_upset = scutils.get_upset_contents(pdata_filter, classes=\"size\")\n              prot_sc_df = scutils.get_upset_query(size_upset, present=[\"sc\"], absent=[\"5k\", \"10k\", \"20k\"])\n              ```\n\n        class_values (list of str): Class values to highlight (must match those\n            used in `plot_rankquant`).\n        layer (str): Data layer to use. Default is `\"X\"`.\n        on (str): Data level, either `\"protein\"` or `\"peptide\"`. Default is `\"protein\"`.\n        color (str): Marker color. Default is `\"red\"`.\n        s (float): Marker size. Default is 10.\n        alpha (float): Marker transparency. Default is 1.\n        show_label (bool): Whether to display labels for highlighted features.\n            Default is True.\n        label_type (str): Label type. Options:\n\n            - `\"accession\"`: show accession IDs.\n            - `\"gene\"`: map to gene names using `\"Gene Names\"` in `mark_df`.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axis with highlighted features.\n\n    !!! tip \n\n        Works best when paired with `plot_rankquant()`, which stores `Average`,\n        `Stdev`, and `Rank` statistics in `.var`. Call `plot_rankquant()` first\n        to generate these values, then use `mark_rankquant()` to overlay\n        highlights.\n\n    Example:\n        Plot rank abundance and highlight specific proteins:\n            ```python\n            fig, ax = plt.subplots()\n            ax = scplt.plot_rankquant(\n                ax, pdata_filter, classes=\"size\", order=order,\n                cmap=cmaps, color=colors, s=10, calpha=1, alpha=0.005\n            )\n            size_upset = scutils.get_upset_contents(pdata_filter, classes=\"size\")\n            prot_sc_df = scutils.get_upset_query(\n                size_upset, present=[\"sc\"], absent=[\"5k\", \"10k\", \"20k\"]\n            )\n            scplt.mark_rankquant(\n                ax, pdata_filter, mark_df=prot_sc_df,\n                class_values=[\"sc\"], show_label=True,\n                color=\"darkorange\", label_type=\"gene\"\n            )\n            ```python\n\n    See Also:\n        plot_rankquant: Generate rank abundance plots with statistics stored in `.var`.\n        get_upset_query: Create a DataFrame of proteins based on set intersections (obs membership).\n    \"\"\"\n    adata = utils.get_adata(pdata, on)\n    names = mark_df['Entry'].tolist()\n\n    # TEST: check if names are in the data\n    pdata._check_rankcol(on, class_values)\n\n    for j, class_value in enumerate(class_values):\n        print('Class: ', class_value)\n\n        for i, txt in enumerate(names):\n            try:\n                x = adata.var['Average: '+class_value].loc[txt]\n                y = adata.var['Rank: '+class_value].loc[txt]\n            except Exception as e:\n                print(f\"Name {txt} not found in {on}.var. Check {on} name for spelling errors and whether it is in data.\")\n                continue\n            if show_label:\n                if label_type == 'accession':\n                    pass\n                elif label_type == 'gene':\n                    txt = mark_df.loc[mark_df['Entry'] == txt, 'Gene Names'].values[0]\n                # elif name_type == 'name':\n\n                plot.annotate(txt, (y,x), xytext=(y+10,x*1.1), fontsize=8)\n            plot.scatter(y,x,marker='o',color=color,s=s, alpha=alpha)\n    return plot\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.mark_volcano","title":"mark_volcano","text":"<pre><code>mark_volcano(ax, volcano_df, label, label_color='black', label_type='Gene', s=10, alpha=1, show_names=True, fontsize=8)\n</code></pre> <p>Mark a volcano plot with specific proteins or genes.</p> <p>This function highlights selected features on an existing volcano plot, optionally labeling them with names.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot.</p> required <code>volcano_df</code> <code>DataFrame</code> <p>DataFrame returned by <code>plot_volcano()</code> or <code>pdata.de()</code>, containing differential expression results.</p> required <code>label</code> <code>list</code> <p>Features to highlight. Can also be a nested list, with separate lists of features for different cases.</p> required <code>label_color</code> <code>str or list</code> <p>Marker color(s). Defaults to <code>\"black\"</code>. If a list is provided, each case receives a different color.</p> <code>'black'</code> <code>label_type</code> <code>str</code> <p>Type of label to display. Default is <code>\"Gene\"</code>.</p> <code>'Gene'</code> <code>s</code> <code>float</code> <p>Marker size. Default is 10.</p> <code>10</code> <code>alpha</code> <code>float</code> <p>Marker transparency. Default is 1.</p> <code>1</code> <code>show_names</code> <code>bool</code> <p>Whether to show labels for the selected features. Default is True.</p> <code>True</code> <code>fontsize</code> <code>int</code> <p>Font size for labels. Default is 8.</p> <code>8</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis with the highlighted volcano plot.</p> Example <p>Highlight specific features on a volcano plot:     <pre><code>fig, ax = plt.subplots()\nax, df = scplt.plot_volcano(ax, pdata, classes=\"treatment\", values=[\"ctrl\", \"drug\"])\nax = scplt.mark_volcano(\n    ax, df, label=[\"P11247\", \"O35639\", \"F6ZDS4\"],\n    label_color=\"red\", s=10, alpha=1, show_names=True\n)\n</code></pre></p> Note <p>This function works especially well in combination with <code>plot_volcano(..., no_marks=True)</code> to render all points in grey, followed by <code>mark_volcano()</code> to selectively highlight features of interest.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def mark_volcano(ax, volcano_df, label, label_color=\"black\", label_type='Gene', s=10, alpha=1, show_names=True, fontsize=8):\n    \"\"\"\n    Mark a volcano plot with specific proteins or genes.\n\n    This function highlights selected features on an existing volcano plot,\n    optionally labeling them with names.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot.\n        volcano_df (pandas.DataFrame): DataFrame returned by `plot_volcano()` or\n            `pdata.de()`, containing differential expression results.\n        label (list): Features to highlight. Can also be a nested list, with\n            separate lists of features for different cases.\n        label_color (str or list, optional): Marker color(s). Defaults to `\"black\"`.\n            If a list is provided, each case receives a different color.\n        label_type (str): Type of label to display. Default is `\"Gene\"`.\n        s (float): Marker size. Default is 10.\n        alpha (float): Marker transparency. Default is 1.\n        show_names (bool): Whether to show labels for the selected features.\n            Default is True.\n        fontsize (int): Font size for labels. Default is 8.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axis with the highlighted volcano plot.\n\n    Example:\n        Highlight specific features on a volcano plot:\n            ```python\n            fig, ax = plt.subplots()\n            ax, df = scplt.plot_volcano(ax, pdata, classes=\"treatment\", values=[\"ctrl\", \"drug\"])\n            ax = scplt.mark_volcano(\n                ax, df, label=[\"P11247\", \"O35639\", \"F6ZDS4\"],\n                label_color=\"red\", s=10, alpha=1, show_names=True\n            )\n            ```\n\n    Note:\n        This function works especially well in combination with\n        `plot_volcano(..., no_marks=True)` to render all points in grey,\n        followed by `mark_volcano()` to selectively highlight features of interest.\n    \"\"\"\n\n    if not isinstance(label[0], list):\n        label = [label]\n        label_color = [label_color] if isinstance(label_color, str) else label_color\n\n    for i, label_group in enumerate(label):\n        color = label_color[i % len(label_color)] if isinstance(label_color, list) else label_color\n\n        # Match by index or 'Genes' column\n        match_df = volcano_df[\n            volcano_df.index.isin(label_group) |\n            volcano_df['Genes'].isin(label_group)\n        ]\n\n        ax.scatter(match_df['log2fc'], match_df['-log10(p_value)'],\n                   c=color, s=s, alpha=alpha, edgecolors='none')\n\n        if show_names:\n            texts = []\n            for _, row in match_df.iterrows():\n                text = row['Genes'] if label_type == 'Gene' and pd.notna(row.get('Genes')) else row.name\n                txt = ax.text(row['log2fc'], row['-log10(p_value)'],\n                              s=text,\n                              fontsize=fontsize,\n                              color=color,\n                              bbox=dict(facecolor='white', edgecolor=color, boxstyle='round'))\n                txt.set_path_effects([PathEffects.withStroke(linewidth=3, foreground='w')])\n                texts.append(txt)\n            adjust_text(texts, expand=(2, 2),\n                        arrowprops=dict(arrowstyle='-&gt;', color=color, zorder=5))\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_abundance","title":"plot_abundance","text":"<pre><code>plot_abundance(ax, pdata, namelist=None, layer='X', on='protein', classes=None, return_df=False, order=None, palette=None, log=True, facet=None, height=4, aspect=0.5, plot_points=True, x_label='gene', kind='auto', **kwargs)\n</code></pre> <p>Plot abundance of proteins or peptides across samples.</p> <p>This function visualizes expression values for selected proteins or peptides using violin + box + strip plots, or bar plots when the number of replicates per group is small. Supports grouping, faceting, and custom ordering.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis to plot on. Ignored if <code>facet</code> is used.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>namelist</code> <code>list of str</code> <p>List of accessions or gene names to plot. If None, all available features are considered.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to use for abundance values. Default is <code>'X'</code>.</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Data level to plot, either <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <code>classes</code> <code>str or list of str</code> <p><code>.obs</code> column(s) to use for grouping samples. Determines coloring and grouping structure.</p> <code>None</code> <code>return_df</code> <code>bool</code> <p>If True, returns the DataFrame of replicate and summary values.</p> <code>False</code> <code>order</code> <code>dict or list</code> <p>Custom order of classes. For dictionary input, keys are class names and values are the ordered categories. Example: <code>order = {\"condition\": [\"sc\", \"kd\"]}</code>.</p> <code>None</code> <code>palette</code> <code>list or dict</code> <p>Color palette mapping groups to colors.</p> <code>None</code> <code>log</code> <code>bool</code> <p>If True, apply log2 transformation to abundance values. Default is True.</p> <code>True</code> <code>facet</code> <code>str</code> <p><code>.obs</code> column to facet by, creating multiple subplots.</p> <code>None</code> <code>height</code> <code>float</code> <p>Height of each facet plot. Default is 4.</p> <code>4</code> <code>aspect</code> <code>float</code> <p>Aspect ratio of each facet plot. Default is 0.5.</p> <code>0.5</code> <code>plot_points</code> <code>bool</code> <p>Whether to overlay stripplot of individual samples.</p> <code>True</code> <code>x_label</code> <code>str</code> <p>Label for the x-axis, either <code>'gene'</code> or <code>'accession'</code>.</p> <code>'gene'</code> <code>kind</code> <code>str</code> <p>Type of plot. Options:</p> <ul> <li><code>'auto'</code>: Default; uses barplot if groups have \u2264 3 samples, otherwise violin.</li> <li><code>'violin'</code>: Always use violin + box + strip.</li> <li><code>'bar'</code>: Always use barplot.</li> </ul> <code>'auto'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to seaborn plotting functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes or FacetGrid</code> <p>The axis or facet grid containing the plot.</p> <code>df</code> <code>(DataFrame, optional)</code> <p>Returned if <code>return_df=True</code>.</p> <p>Example</p> <p>Plot abundance of two selected proteins:     <pre><code>from scpviz import plotting as scplt\nscplt.plot_abundance(ax, pdata, namelist=['Slc12a2','Septin6'])\n</code></pre></p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_abundance(ax, pdata, namelist=None, layer='X', on='protein',\n                   classes=None, return_df=False, order=None, palette=None,\n                   log=True, facet=None, height=4, aspect=0.5,\n                   plot_points=True, x_label='gene', kind='auto', **kwargs):\n    \"\"\"\n    Plot abundance of proteins or peptides across samples.\n\n    This function visualizes expression values for selected proteins or peptides\n    using violin + box + strip plots, or bar plots when the number of replicates\n    per group is small. Supports grouping, faceting, and custom ordering.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis to plot on. Ignored if `facet` is used.\n        pdata (pAnnData): Input pAnnData object.\n        namelist (list of str, optional): List of accessions or gene names to plot.\n            If None, all available features are considered.\n        layer (str): Data layer to use for abundance values. Default is `'X'`.\n        on (str): Data level to plot, either `'protein'` or `'peptide'`.\n        classes (str or list of str, optional): `.obs` column(s) to use for grouping\n            samples. Determines coloring and grouping structure.\n        return_df (bool): If True, returns the DataFrame of replicate and summary values.\n        order (dict or list, optional): Custom order of classes. For dictionary input,\n            keys are class names and values are the ordered categories.  \n            Example: `order = {\"condition\": [\"sc\", \"kd\"]}`.\n        palette (list or dict, optional): Color palette mapping groups to colors.\n        log (bool): If True, apply log2 transformation to abundance values. Default is True.\n        facet (str, optional): `.obs` column to facet by, creating multiple subplots.\n        height (float): Height of each facet plot. Default is 4.\n        aspect (float): Aspect ratio of each facet plot. Default is 0.5.\n        plot_points (bool): Whether to overlay stripplot of individual samples.\n        x_label (str): Label for the x-axis, either `'gene'` or `'accession'`.\n        kind (str): Type of plot. Options:\n\n            - `'auto'`: Default; uses barplot if groups have \u2264 3 samples, otherwise violin.\n            - `'violin'`: Always use violin + box + strip.\n            - `'bar'`: Always use barplot.\n\n        **kwargs: Additional keyword arguments passed to seaborn plotting functions.\n\n    Returns:\n        ax (matplotlib.axes.Axes or seaborn.FacetGrid):\n            The axis or facet grid containing the plot.\n        df (pandas.DataFrame, optional): Returned if `return_df=True`.\n\n    !!! example\n        Plot abundance of two selected proteins:\n            ```python\n            from scpviz import plotting as scplt\n            scplt.plot_abundance(ax, pdata, namelist=['Slc12a2','Septin6'])\n            ```\n\n    \"\"\"\n\n    # Get abundance DataFrame\n    df = utils.get_abundance(\n        pdata, namelist=namelist, layer=layer, on=on,\n        classes=classes, log=log, x_label=x_label\n    )\n\n    # --- Handle custom class ordering ---\n    if classes is not None and order is not None:\n        unused = set(order) - (set([classes]) if isinstance(classes, str) else set(classes))\n        if unused:\n            print(f\"\u26a0\ufe0f Unused keys in `order`: {unused} (not in `classes`)\")\n\n        if isinstance(classes, str):\n            if classes in order:\n                cat_type = pd.api.types.CategoricalDtype(order[classes], ordered=True)\n                df['class'] = df['class'].astype(cat_type)\n        else:\n            for cls in classes:\n                if cls in order and cls in df.columns:\n                    cat_type = pd.api.types.CategoricalDtype(order[cls], ordered=True)\n                    df[cls] = df[cls].astype(cat_type)\n\n    # --- Sort the dataframe so group order is preserved in plotting ---\n    if classes is not None:\n        sort_cols = ['x_label_name']\n        if isinstance(classes, str):\n            sort_cols.append('class')\n        else:\n            sort_cols.extend(classes)\n        df = df.sort_values(by=sort_cols)\n\n\n    # Add facet column (plotting only)\n    df['facet'] = df[facet] if facet else 'all'\n\n    if facet and classes and facet == classes:\n        raise ValueError(\"`facet` and `classes` must be different.\")\n\n    if return_df:\n        return df\n\n    if palette is None:\n        palette = get_color('palette')\n\n    x_col = 'x_label_name'\n    y_col = 'log2_abundance' if log else 'abundance'\n\n    if kind == 'auto':\n        sample_counts = df.groupby([x_col, 'class', 'facet']).size()\n        kind = 'bar' if sample_counts.min() &lt;= 3 else 'violin'\n\n    def _plot_bar(df):\n        bar_kwargs = dict(\n            ci='sd',\n            capsize=0.2,\n            errwidth=1.5,\n            palette=palette\n        )\n        bar_kwargs.update(kwargs)\n        if facet and df['facet'].nunique() &gt; 1:\n            g = sns.FacetGrid(df, col='facet', height=height, aspect=aspect, sharey=True)\n            g.map_dataframe(sns.barplot, x=x_col, y=y_col, hue='class', **bar_kwargs)\n            g.set_axis_labels(\"Gene\" if x_label == 'gene' else \"Accession\", \"log2(Abundance)\" if log else \"Abundance\")\n            g.set_titles(\"{col_name}\")\n            g.add_legend(title='Class', frameon=True)\n            return g\n        else:\n            if ax is None:\n                fig, _ax = plt.subplots(figsize=(6, 4))\n            else:\n                _ax = ax\n            sns.barplot(data=df, x=x_col, y=y_col, hue='class', ax=_ax, **bar_kwargs)\n            handles, labels = _ax.get_legend_handles_labels()\n            by_label = dict(zip(labels, handles))\n            _ax.legend(by_label.values(), by_label.keys(), title='Class', frameon=True)\n            _ax.set_ylabel(\"log2(Abundance)\" if log else \"Abundance\")\n            _ax.set_xlabel(\"Gene\" if x_label == 'gene' else \"Accession\")\n            return _ax\n\n    def _plot_violin(df):\n        violin_kwargs = dict(inner=\"box\", linewidth=1, cut=0, alpha=0.5, density_norm=\"width\")\n        violin_kwargs.update(kwargs)\n        if facet and df['facet'].nunique() &gt; 1:\n            g = sns.FacetGrid(df, col='facet', height=height, aspect=aspect, sharey=True)\n            g.map_dataframe(sns.violinplot, x=x_col, y=y_col, hue='class', palette=palette, **violin_kwargs)\n            if plot_points:\n                def _strip(data, color, **kwargs_inner):\n                    sns.stripplot(data=data, x=x_col, y=y_col, hue='class', dodge=True, jitter=True,\n                                  color='black', size=3, alpha=0.5, legend=False, **kwargs_inner)\n                g.map_dataframe(_strip)\n            g.set_axis_labels(\"Gene\" if x_label == 'gene' else \"Accession\", \"log2(Abundance)\" if log else \"Abundance\")\n            g.set_titles(\"{col_name}\")\n            g.add_legend(title='Class', frameon=True)\n            return g\n        else:\n            if ax is None:\n                fig, _ax = plt.subplots(figsize=(6, 4))\n            else:\n                _ax = ax\n            sns.violinplot(data=df, x=x_col, y=y_col, hue='class', palette=palette, ax=_ax, **violin_kwargs)\n            if plot_points:\n                sns.stripplot(data=df, x=x_col, y=y_col, hue='class', dodge=True, jitter=True,\n                              color='black', size=3, alpha=0.5, legend=False, ax=_ax)\n            handles, labels = _ax.get_legend_handles_labels()\n            by_label = dict(zip(labels, handles))\n            _ax.legend(by_label.values(), by_label.keys(), title='Class', frameon=True)\n            _ax.set_ylabel(\"log2(Abundance)\" if log else \"Abundance\")\n            _ax.set_xlabel(\"Gene\" if x_label == 'gene' else \"Accession\")\n            return _ax\n\n    return _plot_bar(df) if kind == 'bar' else _plot_violin(df)\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_abundance_housekeeping","title":"plot_abundance_housekeeping","text":"<pre><code>plot_abundance_housekeeping(ax, pdata, classes=None, loading_control='all', **kwargs)\n</code></pre> <p>Plot abundance of housekeeping proteins.</p> <p>This function visualizes the abundance of canonical housekeeping proteins as loading controls, grouped by sample-level metadata if specified. Different sets of proteins are supported depending on the chosen loading control type.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>matplotlib.axes.Axes or list of matplotlib.axes.Axes</code> <p>Axis or list of axes to plot on. If <code>loading_control='all'</code>, must provide a list of 3 axes.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>classes</code> <code>str or list of str</code> <p>One or more <code>.obs</code> columns to use for grouping samples.</p> <code>None</code> <code>loading_control</code> <code>str</code> <p>Type of housekeeping controls to plot. Options:</p> <ul> <li> <p><code>'whole cell'</code>: GAPDH, TBCD (\u03b2-tubulin), ACTB (\u03b2-actin), VCL (vinculin), TBP (TATA-binding protein)</p> </li> <li> <p><code>'nuclear'</code>: COX (cytochrome c oxidase), LMNB1 (lamin B1), PCNA (proliferating cell nuclear antigen), HDAC1 (histone deacetylase 1)</p> </li> <li> <p><code>'mitochondrial'</code>: VDAC1 (voltage-dependent anion channel 1)</p> </li> <li> <p><code>'all'</code>: plots all three categories across separate subplots.</p> </li> </ul> <code>'all'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to seaborn plotting functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>matplotlib.axes.Axes or list of matplotlib.axes.Axes</code> <p>Axis or list of axes with the plotted protein abundances.</p> <p>Note:     This function assumes that the specified housekeeping proteins are annotated in <code>.prot.var['Genes']</code>. Missing proteins will be skipped during plotting and may result in empty or partially filled plots.</p> <p>Example</p> <p>Plot housekeeping protein abundance for whole cell controls:     <pre><code>from scpviz import plotting as scplt\nfig, ax = plt.subplots(figsize=(6,4))\nscplt.plot_abundance_housekeeping(ax, pdata, loading_control='whole cell', classes='condition')\n</code></pre></p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_abundance_housekeeping(ax, pdata, classes=None, loading_control='all', **kwargs):\n    \"\"\"\n    Plot abundance of housekeeping proteins.\n\n    This function visualizes the abundance of canonical housekeeping proteins\n    as loading controls, grouped by sample-level metadata if specified.\n    Different sets of proteins are supported depending on the chosen loading\n    control type.\n\n    Args:\n        ax (matplotlib.axes.Axes or list of matplotlib.axes.Axes): Axis or list of axes to plot on.\n            If `loading_control='all'`, must provide a list of 3 axes.\n        pdata (pAnnData): Input pAnnData object.\n        classes (str or list of str, optional): One or more `.obs` columns to use for grouping samples.\n        loading_control (str): Type of housekeeping controls to plot. Options:\n\n            - `'whole cell'`: GAPDH, TBCD (\u03b2-tubulin), ACTB (\u03b2-actin), VCL (vinculin), TBP (TATA-binding protein)\n\n            - `'nuclear'`: COX (cytochrome c oxidase), LMNB1 (lamin B1), PCNA (proliferating cell nuclear antigen), HDAC1 (histone deacetylase 1)\n\n            - `'mitochondrial'`: VDAC1 (voltage-dependent anion channel 1)\n\n            - `'all'`: plots all three categories across separate subplots.\n\n        **kwargs: Additional keyword arguments passed to seaborn plotting functions.\n\n    Returns:\n        ax (matplotlib.axes.Axes or list of matplotlib.axes.Axes):\n            Axis or list of axes with the plotted protein abundances.\n    Note:\n        This function assumes that the specified housekeeping proteins are annotated in `.prot.var['Genes']`. Missing proteins will be skipped during plotting and may result in empty or partially filled plots.\n\n    !!! example\n        Plot housekeeping protein abundance for whole cell controls:\n            ```python\n            from scpviz import plotting as scplt\n            fig, ax = plt.subplots(figsize=(6,4))\n            scplt.plot_abundance_housekeeping(ax, pdata, loading_control='whole cell', classes='condition')\n            ```\n    \"\"\"\n\n\n    loading_controls = {\n        'whole cell': ['GAPDH', 'TBCD', 'ACTB', 'VCL', 'TBP'],\n        'nuclear': ['COX', 'LMNB1', 'PCNA', 'HDAC1'],\n        'mitochondrial': ['VDAC1'],\n        'all': ['GAPDH', 'TBCD', 'ACTB', 'VCL', 'TBP', 'COX', 'LMNB1', 'PCNA', 'HDAC1', 'VDAC1']\n    }\n\n    # Check validity\n    if loading_control not in loading_controls:\n        raise ValueError(f\"\u274c Invalid loading control type: {loading_control}\")\n\n    # Plot all categories as subplots\n    if loading_control == 'all':\n        # Create 1x3 subplots\n        fig, axes = plt.subplots(1, 3, figsize=(16, 4), constrained_layout=True)\n        groups = ['whole cell', 'nuclear', 'mitochondrial']\n        for ax_sub, group in zip(axes, groups):\n            palette = get_color('colors', n=len(loading_controls[group]))\n            plot_abundance(ax_sub, pdata, namelist=loading_controls[group], classes=classes, layer='X', palette=palette, **kwargs)\n            ax_sub.set_title(group.title())\n        fig.suptitle(\"Housekeeping Protein Abundance\", fontsize=14)\n        return fig, axes\n    else:\n        palette = get_color('colors', n=len(loading_controls[loading_control]))\n        plot_abundance(ax, pdata, namelist=loading_controls[loading_control], classes=classes, layer='X', palette=palette, **kwargs)\n        ax.set_title(loading_control.title())\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_clustermap","title":"plot_clustermap","text":"<pre><code>plot_clustermap(ax, pdata, on='prot', classes=None, layer='X', x_label='accession', namelist=None, lut=None, log2=True, cmap='coolwarm', figsize=(6, 10), force=False, impute=None, order=None, **kwargs)\n</code></pre> <p>Plot a clustered heatmap of proteins or peptides by samples.</p> <p>This function creates a hierarchical clustered heatmap (features \u00d7 samples) with optional column annotations from sample-level metadata. Supports custom annotation colors, log2 transformation, and missing value imputation.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Unused; included for API compatibility.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>on</code> <code>str</code> <p>Data level to plot, either <code>\"prot\"</code> or <code>\"pep\"</code>. Default is <code>\"prot\"</code>.</p> <code>'prot'</code> <code>classes</code> <code>str or list of str</code> <p>One or more <code>.obs</code> columns to annotate samples in the heatmap.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to use. Defaults to <code>\"X\"</code>.</p> <code>'X'</code> <code>x_label</code> <code>str</code> <p>Row label mode, either <code>\"accession\"</code> or <code>\"gene\"</code>. Used for mapping <code>namelist</code>.</p> <code>'accession'</code> <code>namelist</code> <code>list of str</code> <p>Subset of accessions or gene names to plot. If None, all rows are included.</p> <code>None</code> <code>lut</code> <code>dict</code> <p>Nested dictionary of <code>{class_name: {label: color}}</code> controlling annotation bar colors. Missing entries fall back to default palettes. See the note 'lut example' below.</p> <code>None</code> <code>log2</code> <code>bool</code> <p>Whether to log2-transform the abundance matrix. Default is True.</p> <code>True</code> <code>cmap</code> <code>str</code> <p>Colormap for heatmap. Default is <code>\"coolwarm\"</code>.</p> <code>'coolwarm'</code> <code>figsize</code> <code>tuple</code> <p>Figure size in inches. Default is <code>(6, 10)</code>.</p> <code>(6, 10)</code> <code>force</code> <code>bool</code> <p>If True, imputes missing values instead of dropping rows with NaNs.</p> <code>False</code> <code>impute</code> <code>str</code> <p>Imputation strategy used when <code>force=True</code>.</p> <ul> <li><code>\"row_min\"</code>: fill NaNs with minimum value of that protein row.</li> <li><code>\"global_min\"</code>: fill NaNs with global minimum value of the matrix.</li> </ul> <code>None</code> <code>order</code> <code>dict</code> <p>Custom order for categorical annotations. Example: <code>{\"condition\": [\"kd\", \"sc\"], \"cellline\": [\"AS\", \"BE\"]}</code>.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>seaborn.clustermap</code>.</p> <p>Common options include:</p> <ul> <li><code>z_score (int)</code>: Normalize rows (0, features) or columns (1, samples).</li> <li><code>standard_scale (int)</code>: Scale rows or columns to unit variance.</li> <li><code>center (float)</code>: Value to center colormap on (e.g. 0 with <code>z_score</code>).</li> <li><code>col_cluster (bool)</code>: Cluster columns (samples). Default is False.</li> <li><code>row_cluster (bool)</code>: Cluster rows (features). Default is True.</li> <li><code>linewidth (float)</code>: Grid line width between cells.</li> <li><code>xticklabels</code> / <code>yticklabels</code> (bool): Show axis tick labels.</li> <li><code>colors_ratio (tuple)</code>: Proportion of space allocated to annotation bars.</li> </ul> <code>{}</code> <p>Returns:</p> Name Type Description <code>g</code> <code>ClusterGrid</code> <p>The seaborn clustermap object.</p> <p>lut example</p> <p>Example of a custom lookup table for annotation colors:     <pre><code>lut = {\n    \"cellline\": {\n        \"AS\": \"#e41a1c\",\n        \"BE\": \"#377eb8\"\n    },\n    \"condition\": {\n        \"kd\": \"#4daf4a\",\n        \"sc\": \"#984ea3\"\n   }\n}\n</code></pre></p> <p>Examples Pending</p> <p>Add usage examples here.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_clustermap(ax, pdata, on='prot', classes=None, layer=\"X\", x_label='accession', namelist=None, lut=None, log2=True,\n                    cmap=\"coolwarm\", figsize=(6, 10), force=False, impute=None, order=None, **kwargs):\n    \"\"\"\n    Plot a clustered heatmap of proteins or peptides by samples.\n\n    This function creates a hierarchical clustered heatmap (features \u00d7 samples)\n    with optional column annotations from sample-level metadata. Supports\n    custom annotation colors, log2 transformation, and missing value imputation.\n\n    Args:\n        ax (matplotlib.axes.Axes): Unused; included for API compatibility.\n        pdata (pAnnData): Input pAnnData object.\n        on (str): Data level to plot, either `\"prot\"` or `\"pep\"`. Default is `\"prot\"`.\n        classes (str or list of str, optional): One or more `.obs` columns to\n            annotate samples in the heatmap.\n        layer (str): Data layer to use. Defaults to `\"X\"`.\n        x_label (str): Row label mode, either `\"accession\"` or `\"gene\"`. Used\n            for mapping `namelist`.\n        namelist (list of str, optional): Subset of accessions or gene names to plot.\n            If None, all rows are included.\n        lut (dict, optional): Nested dictionary of `{class_name: {label: color}}`\n            controlling annotation bar colors. Missing entries fall back to\n            default palettes. See the note 'lut example' below.\n        log2 (bool): Whether to log2-transform the abundance matrix. Default is True.\n        cmap (str): Colormap for heatmap. Default is `\"coolwarm\"`.\n        figsize (tuple): Figure size in inches. Default is `(6, 10)`.\n        force (bool): If True, imputes missing values instead of dropping rows\n            with NaNs.\n        impute (str, optional): Imputation strategy used when `force=True`.\n\n            - `\"row_min\"`: fill NaNs with minimum value of that protein row.\n            - `\"global_min\"`: fill NaNs with global minimum value of the matrix.\n\n        order (dict, optional): Custom order for categorical annotations.\n            Example: `{\"condition\": [\"kd\", \"sc\"], \"cellline\": [\"AS\", \"BE\"]}`.\n        **kwargs: Additional keyword arguments passed to `seaborn.clustermap`.\n\n            Common options include:\n\n            - `z_score (int)`: Normalize rows (0, features) or columns (1, samples).\n            - `standard_scale (int)`: Scale rows or columns to unit variance.\n            - `center (float)`: Value to center colormap on (e.g. 0 with `z_score`).\n            - `col_cluster (bool)`: Cluster columns (samples). Default is False.\n            - `row_cluster (bool)`: Cluster rows (features). Default is True.\n            - `linewidth (float)`: Grid line width between cells.\n            - `xticklabels` / `yticklabels` (bool): Show axis tick labels.\n            - `colors_ratio (tuple)`: Proportion of space allocated to annotation bars.\n\n    Returns:\n        g (seaborn.matrix.ClusterGrid): The seaborn clustermap object.\n\n    !!! note \"lut example\"\n        Example of a custom lookup table for annotation colors:\n            ```python\n            lut = {\n                \"cellline\": {\n                    \"AS\": \"#e41a1c\",\n                    \"BE\": \"#377eb8\"\n                },\n                \"condition\": {\n                    \"kd\": \"#4daf4a\",\n                    \"sc\": \"#984ea3\"\n               }\n            }\n            ```\n\n    !!! todo \"Examples Pending\"\n        Add usage examples here.\n    \"\"\"\n    # --- Step 1: Extract data ---\n    if on not in (\"prot\", \"pep\"):\n        raise ValueError(f\"`on` must be 'prot' or 'pep', got '{on}'\")\n\n    if namelist is not None:\n        df_abund = utils.get_abundance(\n        pdata, namelist=namelist, layer=layer, on=on,\n        classes=classes, log=log2, x_label=x_label)\n\n        pivot_col = \"log2_abundance\" if log2 else \"abundance\"\n        row_index = \"gene\" if x_label == \"gene\" else \"accession\"\n        df = df_abund.pivot(index=row_index, columns=\"cell\", values=pivot_col)\n\n    else:\n        adata = pdata.prot if on == 'prot' else pdata.pep\n        X = adata.layers[layer] if layer in adata.layers else adata.X\n        data = X.toarray() if hasattr(X, \"toarray\") else np.asarray(X)\n        df = pd.DataFrame(data.T, index=adata.var_names, columns=adata.obs_names)\n        if log2:\n            with np.errstate(divide='ignore', invalid='ignore'):\n                df = np.log2(df)\n                df[df == -np.inf] = np.nan\n\n    # --- Handle missing values ---\n    nan_rows = df.index[df.isna().any(axis=1)].tolist()\n    if nan_rows:\n        if not force:\n            print(f\"Warning: {len(nan_rows)} proteins contain missing values and will be excluded: {nan_rows}\")\n            print(\"To include them, rerun with force=True and impute='row_min' or 'global_min'.\")\n            df = df.drop(index=nan_rows)\n        else:\n            print(f\"{len(nan_rows)} proteins contain missing values: {nan_rows}.\\nImputing using strategy: '{impute}'\")\n            if impute == \"row_min\":\n                global_min = df.min().min()\n                df = df.apply(lambda row: row.fillna(row.min() if not np.isnan(row.min()) else global_min), axis=1)\n            elif impute == \"global_min\":\n                df = df.fillna(df.min().min())\n            else:\n                raise ValueError(\"`impute` must be either 'row_min' or 'global_min' when force=True.\")\n\n    # --- Step 2: Column annotations ---\n    col_colors = None\n    legend_handles, legend_labels = [], []\n\n    if classes is not None:\n        if isinstance(classes, str):\n            sample_labels = utils.get_samplenames(adata, classes)\n            annotations = pd.DataFrame({classes: sample_labels}, index=adata.obs_names)\n        else:\n            sample_labels = utils.get_samplenames(adata, classes)\n            split_labels = [[part.strip() for part in s.split(\",\")] for s in sample_labels]\n            annotations = pd.DataFrame(split_labels, index=adata.obs_names, columns=classes)\n\n        # Optional: apply custom category order from `order` dict\n        if order is not None and isinstance(order, dict):\n            for col in classes:\n                if col in annotations.columns and col in order:\n                    cat_type = pd.api.types.CategoricalDtype(order[col], ordered=True)\n                    annotations[col] = annotations[col].astype(cat_type)\n            unused_keys = set(order) - set(classes)\n            if unused_keys:\n                print(f\"\u26a0\ufe0f Unused keys in `order`: {unused_keys} (not present in `classes`)\")\n\n        # Sort columns (samples) by class hierarchy\n        sort_order = annotations.sort_values(by=classes).index\n        df = df[sort_order]\n        annotations = annotations.loc[sort_order]\n\n        if lut is None:\n            lut = {}\n\n        full_lut = {}\n        for col in annotations.columns:\n            unique_vals = sorted(annotations[col].dropna().unique())\n            user_colors = lut.get(col, {})\n            missing_vals = [v for v in unique_vals if v not in user_colors]\n            fallback_palette = sns.color_palette(n_colors=len(missing_vals))\n            fallback_colors = dict(zip(missing_vals, fallback_palette))\n            full_lut[col] = {**user_colors, **fallback_colors}\n\n            unmatched = set(user_colors) - set(unique_vals)\n            if unmatched:\n                print(f\"Warning: The following labels in `lut['{col}']` are not found in the data: {sorted(unmatched)}\")\n\n        col_colors = annotations.apply(lambda col: col.map(full_lut[col.name]))\n\n        # Legend handles\n        for col in annotations.columns:\n            legend_handles.append(mpatches.Patch(facecolor=\"none\", edgecolor=\"none\", label=col))  # header\n            for label, color in full_lut[col].items():\n                legend_handles.append(mpatches.Patch(facecolor=color, edgecolor=\"black\", label=label))\n            legend_labels.extend([col] + list(full_lut[col].keys()))\n\n    # --- Step 3: Clustermap defaults (user-overridable) ---\n    col_cluster = kwargs.pop(\"col_cluster\", False)\n    row_cluster = kwargs.pop(\"row_cluster\", True)\n    linewidth = kwargs.pop(\"linewidth\", 0)\n    yticklabels = kwargs.pop(\"yticklabels\", False)\n    xticklabels = kwargs.pop(\"xticklabels\", False)\n    colors_ratio = kwargs.pop(\"colors_ratio\", (0.03, 0.02))\n    if kwargs.get(\"z_score\", None) == 0:\n        zero_var_rows = df.var(axis=1) == 0\n        if zero_var_rows.any():\n            dropped = df.index[zero_var_rows].tolist()\n            print(f\"\u26a0\ufe0f {len(dropped)} proteins have zero variance and will be dropped due to z_score=0: {dropped}\")\n            df = df.drop(index=dropped)\n\n    # --- Step 4: Plot clustermap ---\n    try:\n        g = sns.clustermap(df,\n                        cmap=cmap,\n                        col_cluster=col_cluster,\n                        row_cluster=row_cluster,\n                        col_colors=col_colors,\n                        figsize=figsize,\n                        xticklabels=xticklabels,\n                        yticklabels=yticklabels,\n                        linewidth=linewidth,\n                        colors_ratio=colors_ratio,\n                        **kwargs)\n    except Exception as e:\n        print(f\"Error occurred while creating clustermap: {e}\")\n        return df\n\n    # --- Step 5: Column annotation legend ---\n    if classes is not None:\n        g.ax_col_dendrogram.legend(legend_handles, legend_labels,\n                                   title=None,\n                                   bbox_to_anchor=(0.5, 1.15),\n                                   loc=\"upper center\",\n                                   ncol=len(classes),\n                                   handletextpad=0.5,\n                                   columnspacing=1.5,\n                                   frameon=False)\n\n    # --- Step 6: Row label remapping ---\n    if x_label == \"gene\" and xticklabels:\n        _ , prot_map = pdata.get_gene_maps(on='protein' if on == 'prot' else 'peptide')\n        row_labels = [prot_map.get(row, row) for row in g.data2d.index]\n        g.ax_heatmap.set_yticklabels(row_labels, rotation=0)\n\n    # --- Step 8: Store clustering results ---\n    cluster_key  = f\"{on}_{layer}_clustermap\"\n    row_order = list(g.data2d.index)\n    row_indices = g.dendrogram_row.reordered_ind\n\n    pdata.stats[cluster_key]  = {\n        \"row_order\": row_order,\n        \"row_indices\": row_indices,\n        \"row_labels\": x_label,   # 'accession' or 'gene'\n        \"namelist_used\": namelist if namelist is not None else \"all_proteins\",\n        \"col_order\": list(g.data2d.columns),\n        \"col_indices\": g.dendrogram_col.reordered_ind if g.dendrogram_col else None,\n        \"row_linkage\": g.dendrogram_row.linkage,  # &lt;--- NEW\n        \"col_linkage\": g.dendrogram_col.linkage if g.dendrogram_col else None,\n    }\n\n    return g\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_cv","title":"plot_cv","text":"<pre><code>plot_cv(ax, pdata, classes=None, layer='X', on='protein', order=None, palette=None, return_df=False, **kwargs)\n</code></pre> <p>Generate a box-and-whisker plot for the coefficient of variation (CV).</p> <p>This function computes CV values across proteins or peptides, grouped by sample-level classes, and visualizes their distribution as a box plot.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object containing protein or peptide data.</p> required <code>classes</code> <code>str or list of str</code> <p>One or more <code>.obs</code> columns to use for grouping samples in the plot. If None, no grouping is applied.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to use for CV calculation. Default is <code>'X'</code>.</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Data level to compute CV on, either <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <code>order</code> <code>list</code> <p>Custom order of classes for plotting. If None, defaults to alphabetical order.</p> <code>None</code> <code>palette</code> <code>dict or list</code> <p>Custom color palette for class groups. If None, defaults to <code>scviz</code> package color palette.</p> <code>None</code> <code>return_df</code> <code>bool</code> <p>If True, returns the underlying DataFrame used for plotting.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to seaborn plotting functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the plotted CV distribution.</p> <code>cv_df</code> <code>DataFrame</code> <p>Optional, returned if <code>return_df=True</code>.</p> <p>Examples Pending</p> <p>Add usage examples here.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_cv(ax, pdata, classes=None, layer='X', on='protein', order=None, palette=None, return_df=False, **kwargs):\n    \"\"\"\n    Generate a box-and-whisker plot for the coefficient of variation (CV).\n\n    This function computes CV values across proteins or peptides, grouped by\n    sample-level classes, and visualizes their distribution as a box plot.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot.\n        pdata (pAnnData): Input pAnnData object containing protein or peptide data.\n        classes (str or list of str, optional): One or more `.obs` columns to use\n            for grouping samples in the plot. If None, no grouping is applied.\n        layer (str): Data layer to use for CV calculation. Default is `'X'`.\n        on (str): Data level to compute CV on, either `'protein'` or `'peptide'`.\n        order (list, optional): Custom order of classes for plotting.\n            If None, defaults to alphabetical order.\n        palette (dict or list, optional): Custom color palette for class groups.\n            If None, defaults to `scviz` package color palette.\n        return_df (bool): If True, returns the underlying DataFrame used for plotting.\n        **kwargs: Additional keyword arguments passed to seaborn plotting functions.\n\n    Returns:\n        ax (matplotlib.axes.Axes): The axis with the plotted CV distribution.\n        cv_df (pandas.DataFrame): Optional, returned if `return_df=True`.\n\n    !!! todo \"Examples Pending\"\n        Add usage examples here.\n    \"\"\"\n    # Compute CVs for the selected layer\n    pdata.cv(classes = classes, on = on, layer = layer)\n    adata = utils.get_adata(pdata, on)    \n    classes_list = utils.get_classlist(adata, classes = classes, order = order)\n\n    cv_data = []\n    for class_value in classes_list:\n        cv_col = f'CV: {class_value}'\n        if cv_col in adata.var.columns:\n            cv_values = adata.var[cv_col].values\n            cv_data.append(pd.DataFrame({'Class': class_value, 'CV': cv_values}))\n\n    if not cv_data:\n        print(f\"{utils.format_log_prefix('warn')} No valid CV subsets found \u2014 skipping plot.\")\n        return ax if ax is not None else None\n\n    cv_df = pd.concat(cv_data, ignore_index=True)\n\n    # return cv_df for user to plot themselves\n    if return_df:\n        return cv_df\n\n    if palette is None:\n        palette = get_color('palette')\n\n    # Ensure consistent class ordering\n    if order is not None:\n        cat_type = pd.api.types.CategoricalDtype(order, ordered=True)\n        cv_df['Class'] = cv_df['Class'].astype(cat_type)\n    else:\n        cv_df['Class'] = pd.Categorical(cv_df['Class'],\n                                        categories=sorted(cv_df['Class'].unique()),\n                                        ordered=True)    \n\n    violin_kwargs = dict(inner=\"box\", linewidth=1, cut=0, alpha=0.6, density_norm=\"width\")\n    violin_kwargs.update(kwargs)\n\n    sns.violinplot(x='Class', y='CV', data=cv_df, ax=ax, palette=palette, **violin_kwargs)\n\n    plt.title('Coefficient of Variation (CV) by Class')\n    plt.xlabel('Class')\n    plt.ylabel('CV')\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_enrichment_svg","title":"plot_enrichment_svg","text":"<pre><code>plot_enrichment_svg(*args, **kwargs)\n</code></pre> <p>Plot STRING enrichment results as an SVG figure.</p> <p>This is a wrapper that redirects to the implementation in <code>enrichment.py</code> for convenience and discoverability.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to <code>scpviz.enrichment.plot_enrichment_svg</code>.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to <code>scpviz.enrichment.plot_enrichment_svg</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>svg</code> <code>SVG</code> <p>SVG figure object.</p> See Also <p>scpviz.enrichment.plot_enrichment_svg</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_enrichment_svg(*args, **kwargs):\n    \"\"\"\n    Plot STRING enrichment results as an SVG figure.\n\n    This is a wrapper that redirects to the implementation in `enrichment.py`\n    for convenience and discoverability.\n\n    Args:\n        *args: Positional arguments passed to `scpviz.enrichment.plot_enrichment_svg`.\n        **kwargs: Keyword arguments passed to `scpviz.enrichment.plot_enrichment_svg`.\n\n    Returns:\n        svg (SVG): SVG figure object.\n\n    See Also:\n        scpviz.enrichment.plot_enrichment_svg\n    \"\"\"\n    from .enrichment import plot_enrichment_svg as actual_plot\n    return actual_plot(*args, **kwargs)\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_pca","title":"plot_pca","text":"<pre><code>plot_pca(ax, pdata, classes=None, layer='X', on='protein', cmap='default', s=20, alpha=0.8, plot_pc=[1, 2], pca_params=None, force=False, basis='X_pca', show_labels=False, label_column=None, add_ellipses=False, ellipse_kwargs=None, return_fit=False)\n</code></pre> <p>Plot principal component analysis (PCA) of protein or peptide abundance.</p> <p>This function computes or reuses PCA of abundance values and visualizes samples in a scatter plot colored by metadata or feature expression. Supports both 2D and 3D plotting, with optional labels and confidence ellipses.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis to plot on. Must be 3D if plotting 3 PCs.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object with <code>.prot</code>, <code>.pep</code>, and <code>.summary</code>.</p> required <code>classes</code> <code>str, list of str, or None</code> <p>Coloring scheme.</p> <ul> <li> <p>None: plot all samples in grey.</p> </li> <li> <p>str: an <code>.obs</code> column (e.g. <code>\"treatment\"</code>) or a gene/protein (e.g. <code>\"UBE4B\"</code>).</p> </li> <li> <p>list of str: combine multiple <code>.obs</code> columns (e.g. <code>[\"cellline\", \"treatment\"]</code>).</p> </li> </ul> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to use. Default is <code>\"X\"</code>.</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Data level to plot, either <code>\"protein\"</code> or <code>\"peptide\"</code>. Default is <code>\"protein\"</code>.</p> <code>'protein'</code> <code>cmap</code> <code>str, list, or matplotlib colormap</code> <p>Colormap for point coloring.</p> <ul> <li> <p><code>\"default\"</code>: uses <code>get_color()</code> scheme.</p> </li> <li> <p>list of colors: categorical mapping for <code>classes</code>.</p> </li> <li> <p>colormap name or object: continuous coloring for expression values.</p> </li> </ul> <code>'default'</code> <code>s</code> <code>float</code> <p>Scatter dot size. Default is 20.</p> <code>20</code> <code>alpha</code> <code>float</code> <p>Point opacity. Default is 0.8.</p> <code>0.8</code> <code>plot_pc</code> <code>list of int</code> <p>Principal components to plot, e.g. <code>[1, 2]</code> or <code>[1, 2, 3]</code>.</p> <code>[1, 2]</code> <code>pca_params</code> <code>dict</code> <p>Additional parameters for <code>sklearn.decomposition.PCA</code>.</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, recompute PCA even if it is already cached.</p> <code>False</code> <code>basis</code> <code>str</code> <p>PCA basis to use. Defaults to <code>X_pca</code>, alternatives include <code>X_pca_harmony</code> after running pdata.harmony(batch=\"\"). <code>'X_pca'</code> <code>show_labels</code> <code>bool or list</code> <p>Whether to label points.</p> <ul> <li> <p>False: no labels.</p> </li> <li> <p>True: label all samples.</p> </li> <li> <p>list: label only specified samples.</p> </li> </ul> <code>False</code> <code>label_column</code> <code>str</code> <p>Column in <code>.summary</code> to use as label source. Overrides sample names if provided.</p> <code>None</code> <code>add_ellipses</code> <code>bool</code> <p>If True, overlay confidence ellipses per class (2D only). Ellipses represent a 95% confidence region under a bivariate Gaussian assumption.</p> <code>False</code> <code>ellipse_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the ellipse patch.</p> <code>None</code> <code>return_fit</code> <code>bool</code> <p>If True, also return the fitted PCA object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis containing the PCA scatter plot.</p> <code>pca</code> <code>PCA</code> <p>The fitted PCA object.</p> Note <p>PCA results are cached in <code>pdata.uns[\"pca\"]</code> and reused across plotting calls. To force recalculation (e.g., after filtering or normalization), set <code>force=True</code>.</p> Example <p>Basic usage in grey:     <pre><code>plot_pca(ax, pdata)\n</code></pre></p> <p>Color by categorical class:     <pre><code>plot_pca(ax, pdata, classes=\"treatment\")\n</code></pre></p> <p>Combine multiple classes:     <pre><code>plot_pca(ax, pdata, classes=[\"cellline\", \"treatment\"])\n</code></pre></p> <p>Color by protein expression:     <pre><code>plot_pca(ax, pdata, classes=\"UBE4B\")\n</code></pre></p> <p>Label all samples:     <pre><code>plot_pca(ax, pdata, show_labels=True)\n</code></pre></p> <p>Label with custom column:     <pre><code>plot_pca(ax, pdata, show_labels=True, label_column=\"short_name\")\n</code></pre></p> <p>Add confidence ellipses:     <pre><code>plot_pca(ax, pdata, classes=\"treatment\", add_ellipses=True)\n</code></pre></p> <p>Customize ellipse appearance:     <pre><code>plot_pca(\n    ax, pdata, classes=\"treatment\", add_ellipses=True,\n    ellipse_kwargs={\"alpha\": 0.1, \"lw\": 2}\n)\n</code></pre></p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_pca(ax, pdata, classes=None, layer=\"X\", on='protein',\n             cmap='default', s=20, alpha=.8, plot_pc=[1, 2],\n             pca_params=None, force=False, basis='X_pca',\n             show_labels=False, label_column=None,\n             add_ellipses=False, ellipse_kwargs=None, return_fit=False):\n    \"\"\"\n    Plot principal component analysis (PCA) of protein or peptide abundance.\n\n    This function computes or reuses PCA of abundance values and visualizes\n    samples in a scatter plot colored by metadata or feature expression.\n    Supports both 2D and 3D plotting, with optional labels and confidence ellipses.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis to plot on. Must be 3D if plotting 3 PCs.\n        pdata (pAnnData): Input pAnnData object with `.prot`, `.pep`, and `.summary`.\n        classes (str, list of str, or None): Coloring scheme.\n\n            - None: plot all samples in grey.\n\n            - str: an `.obs` column (e.g. `\"treatment\"`) or a gene/protein (e.g. `\"UBE4B\"`).\n\n            - list of str: combine multiple `.obs` columns (e.g. `[\"cellline\", \"treatment\"]`).\n\n        layer (str): Data layer to use. Default is `\"X\"`.\n        on (str): Data level to plot, either `\"protein\"` or `\"peptide\"`. Default is `\"protein\"`.\n        cmap (str, list, or matplotlib colormap): Colormap for point coloring.\n\n            - `\"default\"`: uses `get_color()` scheme.\n\n            - list of colors: categorical mapping for `classes`.\n\n            - colormap name or object: continuous coloring for expression values.\n\n        s (float): Scatter dot size. Default is 20.\n        alpha (float): Point opacity. Default is 0.8.\n        plot_pc (list of int): Principal components to plot, e.g. `[1, 2]` or `[1, 2, 3]`.\n        pca_params (dict, optional): Additional parameters for `sklearn.decomposition.PCA`.\n        force (bool): If True, recompute PCA even if it is already cached.\n        basis (str): PCA basis to use. Defaults to `X_pca`, alternatives include `X_pca_harmony` after running pdata.harmony(batch=\"&lt;key&gt;\").\n        show_labels (bool or list): Whether to label points.\n\n            - False: no labels.\n\n            - True: label all samples.\n\n            - list: label only specified samples.\n\n        label_column (str, optional): Column in `.summary` to use as label source.\n            Overrides sample names if provided.\n        add_ellipses (bool): If True, overlay confidence ellipses per class (2D only).\n            Ellipses represent a 95% confidence region under a bivariate Gaussian assumption.\n        ellipse_kwargs (dict, optional): Additional keyword arguments for the ellipse patch.\n        return_fit (bool): If True, also return the fitted PCA object.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axis containing the PCA scatter plot.\n\n        pca (sklearn.decomposition.PCA): The fitted PCA object.\n\n    Note:\n        PCA results are cached in `pdata.uns[\"pca\"]` and reused across plotting calls.\n        To force recalculation (e.g., after filtering or normalization), set `force=True`.\n\n    Example:\n        Basic usage in grey:\n            ```python\n            plot_pca(ax, pdata)\n            ```\n\n        Color by categorical class:\n            ```python\n            plot_pca(ax, pdata, classes=\"treatment\")\n            ```\n\n        Combine multiple classes:\n            ```python\n            plot_pca(ax, pdata, classes=[\"cellline\", \"treatment\"])\n            ```\n\n        Color by protein expression:\n            ```python\n            plot_pca(ax, pdata, classes=\"UBE4B\")\n            ```\n\n        Label all samples:\n            ```python\n            plot_pca(ax, pdata, show_labels=True)\n            ```\n\n        Label with custom column:\n            ```python\n            plot_pca(ax, pdata, show_labels=True, label_column=\"short_name\")\n            ```\n\n        Add confidence ellipses:\n            ```python\n            plot_pca(ax, pdata, classes=\"treatment\", add_ellipses=True)\n            ```\n\n        Customize ellipse appearance:\n            ```python\n            plot_pca(\n                ax, pdata, classes=\"treatment\", add_ellipses=True,\n                ellipse_kwargs={\"alpha\": 0.1, \"lw\": 2}\n            )\n            ```\n    \"\"\"\n    from matplotlib.patches import Ellipse\n\n    def plot_confidence_ellipse(x, y, ax, n_std=2.4477, facecolor='none', edgecolor='black', alpha=0.2, **kwargs):\n        if x.size &lt;= 2:\n            return\n        cov = np.cov(x, y)\n        if np.linalg.matrix_rank(cov) &lt; 2:\n            return\n        mean_x, mean_y = np.mean(x), np.mean(y)\n        vals, vecs = np.linalg.eigh(cov)\n        order = vals.argsort()[::-1]\n        vals, vecs = vals[order], vecs[:, order]\n        width, height = 2 * n_std * np.sqrt(vals)\n        angle = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n        ellipse = Ellipse((mean_x, mean_y), width, height, angle=angle,\n                          facecolor=facecolor, edgecolor=edgecolor, alpha=alpha, lw=1.5, **kwargs)\n        ax.add_patch(ellipse)\n\n    # Validate PCA dimensions\n    assert isinstance(plot_pc, list) and len(plot_pc) in [2, 3], \"plot_pc must be a list of 2 or 3 PCs.\"\n    if len(plot_pc) == 3:\n        assert ax.name == '3d', \"3 PCs requested \u2014 ax must be a 3D projection\"\n\n    pc_x, pc_y = plot_pc[0] - 1, plot_pc[1] - 1\n    pc_z = plot_pc[2] - 1 if len(plot_pc) == 3 else None\n\n    adata = utils.get_adata(pdata, on)\n\n    default_pca_params = {'n_comps': min(len(adata.obs_names), len(adata.var_names)) - 1, 'random_state': 42}\n    pca_param = {**default_pca_params, **(pca_params or {})}\n\n    if basis != \"X_pca\":\n        # User-specified alternative basis (e.g. Harmony, ICA)\n        if basis not in adata.obsm:\n            raise KeyError(f\"{utils.format_log_prefix('error',2)} Custom PCA basis '{basis}' not found in adata.obsm.\")\n    else:\n        # Standard PCA case\n        if \"X_pca\" not in adata.obsm or force:\n            print(f\"{utils.format_log_prefix('info')} Computing PCA (force={force})...\")\n            pdata.pca(on=on, layer=layer, **pca_param)\n        else:\n            print(f\"{utils.format_log_prefix('info')} Using existing PCA embedding.\")\n\n    # --- Select PCA basis for plotting ---\n    X_pca = adata.obsm[basis] if basis in adata.obsm else adata.obsm[\"X_pca\"]\n    pca = adata.uns[\"pca\"]\n\n    # Get colors\n    color_mapped, cmap_resolved, legend_elements = resolve_plot_colors(adata, classes, cmap, layer=layer)\n\n    # Plot\n    if len(plot_pc) == 2:\n        ax.scatter(X_pca[:, pc_x], X_pca[:, pc_y], c=color_mapped, cmap=cmap_resolved, s=s, alpha=alpha)\n        ax.set_xlabel(f'PC{pc_x+1} ({pca[\"variance_ratio\"][pc_x]*100:.2f}%)')\n        ax.set_ylabel(f'PC{pc_y+1} ({pca[\"variance_ratio\"][pc_y]*100:.2f}%)')\n\n        # Add colorbar if using continuous color (i.e., abundance coloring)\n        if isinstance(color_mapped, np.ndarray) and cmap_resolved is not None:\n            norm = mcolors.Normalize(vmin=np.min(color_mapped), vmax=np.max(color_mapped))\n            sm = cm.ScalarMappable(cmap=cmap_resolved, norm=norm)\n            sm.set_array([])\n            cb = ax.figure.colorbar(sm, ax=ax, pad=0.01)\n            cb.set_label(classes if isinstance(classes, str) else \"Abundance\", fontsize=9)\n\n        if add_ellipses and isinstance(classes, (str, list)) and all(c in adata.obs.columns for c in (classes if isinstance(classes, list) else [classes])):\n            ellipse_kwargs = ellipse_kwargs.copy() if ellipse_kwargs else {}\n            y = utils.get_samplenames(adata, classes)\n            df_coords = pd.DataFrame(X_pca[:, [pc_x, pc_y]], columns=[\"PC1\", \"PC2\"], index=adata.obs_names)\n            df_coords['class'] = y\n            for cls in df_coords['class'].unique():\n                sub = df_coords[df_coords['class'] == cls]\n                color_series = pd.Series(color_mapped, index=adata.obs_names)\n                color = color_series[df_coords['class'] == cls].iloc[0]\n\n                kwargs = ellipse_kwargs.copy() if ellipse_kwargs else {}\n                kwargs[\"facecolor\"] = color\n                kwargs[\"edgecolor\"] = color\n\n                plot_confidence_ellipse(sub['PC1'].values, sub['PC2'].values, ax=ax, **kwargs)\n\n    elif len(plot_pc) == 3:\n        ax.scatter(X_pca[:, pc_x], X_pca[:, pc_y], X_pca[:, pc_z], c=color_mapped, cmap=cmap_resolved, s=s, alpha=alpha)\n        ax.set_xlabel(f'PC{pc_x+1} ({pca[\"variance_ratio\"][pc_x]*100:.2f}%)')\n        ax.set_ylabel(f'PC{pc_y+1} ({pca[\"variance_ratio\"][pc_y]*100:.2f}%)')\n        ax.set_zlabel(f'PC{pc_z+1} ({pca[\"variance_ratio\"][pc_z]*100:.2f}%)')\n\n        # Add colorbar if using continuous color (i.e., abundance coloring)\n        if isinstance(color_mapped, np.ndarray) and cmap_resolved is not None:\n            norm = mcolors.Normalize(vmin=np.min(color_mapped), vmax=np.max(color_mapped))\n            sm = cm.ScalarMappable(cmap=cmap_resolved, norm=norm)\n            sm.set_array([])\n            cb = ax.figure.colorbar(sm, ax=ax, pad=0.01)\n            cb.set_label(classes if isinstance(classes, str) else \"Abundance\", fontsize=9)\n\n    # Labels\n    if show_labels:\n        show_set = set(show_labels) if isinstance(show_labels, list) else set(adata.obs_names)\n        label_series = pdata.summary[label_column] if label_column and label_column in pdata.summary.columns else adata.obs_names\n        for i, sample in enumerate(adata.obs_names):\n            if sample in show_set:\n                label = label_series[i] if i &lt; len(label_series) else sample\n                pos = X_pca[i, [pc_x, pc_y, pc_z][:len(plot_pc)]]\n                if len(pos) == 2:\n                    ax.text(pos[0], pos[1], str(label), fontsize=8, ha='right', va='bottom')\n                elif len(pos) == 3:\n                    ax.text(pos[0], pos[1], pos[2], str(label), fontsize=8)\n        if not label_column and max(len(str(n)) for n in label_series) &gt; 20:\n            print(\"[plot_pca] Warning: Labels are long. Consider using label_column='your_column'.\")\n\n    if legend_elements:\n        if classes is None:\n            legend_title = None  # no title if no classes\n        elif isinstance(classes, list):\n            legend_title = \"/\".join(c.capitalize() for c in classes)\n        else:\n            legend_title = str(classes).capitalize()\n\n        ax.legend(handles=legend_elements,\n                title=legend_title,\n                loc='best',\n                frameon=False)\n\n    if return_fit:\n        return ax, pca\n    else:\n        return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_pca_scree","title":"plot_pca_scree","text":"<pre><code>plot_pca_scree(ax, pca)\n</code></pre> <p>Plot a scree plot of explained variance from PCA.</p> <p>This function visualizes the proportion of variance explained by each principal component as a bar chart, helping to assess how many PCs are meaningful.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot the scree plot.</p> required <code>pca</code> <code>PCA or dict</code> <p>The fitted PCA object, or a dictionary from <code>.uns</code> with key <code>\"variance_ratio\"</code>.</p> required <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis containing the scree plot.</p> Example <p>Basic usage with fitted PCA, first run PCA:     <pre><code>import matplotlib.pyplot as plt\nfrom scpviz import plotting as scplt\nfig, ax = plt.subplots()\nax, pca = scplt.plot_pca(ax, pdata, classes=[\"cellline\", \"treatment\"], plot_pc=[1, 2])  # run PCA and plot\nax = scplt.plot_pca_scree(ax, pca)  # scree plot\n</code></pre></p> <p>If PCA has already been run, use cached PCA results from <code>.uns</code>:     <pre><code>scplt.plot_pca_scree(ax, pdata.prot.uns[\"pca\"])\n</code></pre></p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_pca_scree(ax, pca):\n    \"\"\"\n    Plot a scree plot of explained variance from PCA.\n\n    This function visualizes the proportion of variance explained by each\n    principal component as a bar chart, helping to assess how many PCs are\n    meaningful.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot the scree plot.\n\n        pca (sklearn.decomposition.PCA or dict): The fitted PCA object, or a\n            dictionary from `.uns` with key `\"variance_ratio\"`.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axis containing the scree plot.\n\n    Example:\n        Basic usage with fitted PCA, first run PCA:\n            ```python\n            import matplotlib.pyplot as plt\n            from scpviz import plotting as scplt\n            fig, ax = plt.subplots()\n            ax, pca = scplt.plot_pca(ax, pdata, classes=[\"cellline\", \"treatment\"], plot_pc=[1, 2])  # run PCA and plot\n            ax = scplt.plot_pca_scree(ax, pca)  # scree plot\n            ```\n\n        If PCA has already been run, use cached PCA results from `.uns`:\n            ```python\n            scplt.plot_pca_scree(ax, pdata.prot.uns[\"pca\"])\n            ```\n    \"\"\"\n    if isinstance(pca, dict):\n        variance_ratio = np.array(pca[\"variance_ratio\"])\n        n_components = len(variance_ratio)\n    else:\n        variance_ratio = pca.explained_variance_ratio_\n        n_components = pca.n_components_\n\n    PC_values = np.arange(1, n_components + 1)\n    cumulative = np.cumsum(variance_ratio)\n\n    ax.plot(PC_values, variance_ratio, 'o-', linewidth=2, label='Explained Variance', color='blue')\n    ax.plot(PC_values, cumulative, 'o--', linewidth=2, label='Cumulative Variance', color='gray')\n    ax.set_title('Scree Plot')\n    ax.set_xlabel('Principal Component')\n    ax.set_ylabel('Variance Explained')\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_raincloud","title":"plot_raincloud","text":"<pre><code>plot_raincloud(ax, pdata, classes=None, layer='X', on='protein', order=None, color=['blue'], boxcolor='black', linewidth=0.5, debug=False)\n</code></pre> <p>Plot raincloud distributions of protein or peptide abundances.</p> <p>This function generates a raincloud plot (violin + boxplot + scatter) to visualize abundance distributions across groups. Summary statistics (average, standard deviation, rank) are written into <code>.var</code> for downstream use with <code>mark_raincloud()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>classes</code> <code>str or list of str</code> <p>One or more <code>.obs</code> columns to group samples. If None, all samples are combined.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to use. Default is <code>\"X\"</code>.</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Data level, either <code>\"protein\"</code> or <code>\"peptide\"</code>. Default is <code>\"protein\"</code>.</p> <code>'protein'</code> <code>order</code> <code>list of str</code> <p>Custom order of class categories. If None, categories appear in data order.</p> <code>None</code> <code>color</code> <code>list of str</code> <p>Colors for each class distribution. Default is <code>[\"blue\"]</code>.</p> <code>['blue']</code> <code>boxcolor</code> <code>str</code> <p>Color for boxplot outlines. Default is <code>\"black\"</code>.</p> <code>'black'</code> <code>linewidth</code> <code>float</code> <p>Line width for box/whisker elements. Default is 0.5.</p> <code>0.5</code> <code>debug</code> <code>bool</code> <p>If True, return both axis and computed data arrays.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>If <code>debug=False</code>: axis with raincloud plot.</p> <code>tuple</code> <code>matplotlib.axes.Axes, list of np.ndarray</code> <p>If <code>debug=True</code>: <code>(axis, data_X)</code> where <code>data_X</code> are the transformed abundance distributions per group.</p> Note <p>Statistics (<code>Average</code>, <code>Stdev</code>, <code>Rank</code>) are stored in <code>.var</code> and can be used with <code>mark_raincloud()</code> to highlight specific features.</p> Example <p>Plot raincloud distributions grouped by sample size:     <pre><code>ax = scplt.plot_raincloud(\n    ax, pdata_filter, classes=\"size\",\n    order=order, color=colors, linewidth=0.5, debug=False\n)\nscplt.mark_raincloud(\n    ax, pdata_filter, mark_df=prot_sc_df,\n    class_values=[\"sc\"], color=\"black\"\n)\n</code></pre></p> See Also <p>mark_raincloud: Highlight specific features on a raincloud plot. plot_rankquant: Alternative distribution visualization using rank abundance.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_raincloud(ax,pdata,classes = None, layer = 'X', on = 'protein', order = None, color=['blue'],boxcolor='black',linewidth=0.5, debug = False):\n    \"\"\"\n    Plot raincloud distributions of protein or peptide abundances.\n\n    This function generates a raincloud plot (violin + boxplot + scatter)\n    to visualize abundance distributions across groups. Summary statistics\n    (average, standard deviation, rank) are written into `.var` for downstream\n    use with `mark_raincloud()`.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot.\n        pdata (pAnnData): Input pAnnData object.\n        classes (str or list of str, optional): One or more `.obs` columns to\n            group samples. If None, all samples are combined.\n        layer (str): Data layer to use. Default is `\"X\"`.\n        on (str): Data level, either `\"protein\"` or `\"peptide\"`. Default is `\"protein\"`.\n        order (list of str, optional): Custom order of class categories. If None,\n            categories appear in data order.\n        color (list of str): Colors for each class distribution. Default is `[\"blue\"]`.\n        boxcolor (str): Color for boxplot outlines. Default is `\"black\"`.\n        linewidth (float): Line width for box/whisker elements. Default is 0.5.\n        debug (bool): If True, return both axis and computed data arrays.\n\n    Returns:\n        ax (matplotlib.axes.Axes): If `debug=False`: axis with raincloud plot.\n\n        tuple (matplotlib.axes.Axes, list of np.ndarray): If `debug=True`: `(axis, data_X)` where `data_X` are the transformed abundance distributions per group.\n\n    Note:\n        Statistics (`Average`, `Stdev`, `Rank`) are stored in `.var` and can be\n        used with `mark_raincloud()` to highlight specific features.\n\n    Example:\n        Plot raincloud distributions grouped by sample size:\n            ```python\n            ax = scplt.plot_raincloud(\n                ax, pdata_filter, classes=\"size\",\n                order=order, color=colors, linewidth=0.5, debug=False\n            )\n            scplt.mark_raincloud(\n                ax, pdata_filter, mark_df=prot_sc_df,\n                class_values=[\"sc\"], color=\"black\"\n            )\n            ```\n\n    See Also:\n        mark_raincloud: Highlight specific features on a raincloud plot.  \n        plot_rankquant: Alternative distribution visualization using rank abundance.\n    \"\"\"\n    adata = utils.get_adata(pdata, on)\n\n    classes_list = utils.get_classlist(adata, classes = classes, order = order)\n    data_X = []\n\n    for j, class_value in enumerate(classes_list):\n        rank_data = utils.resolve_class_filter(adata, classes, class_value, debug=True)\n\n        plot_df = rank_data.to_df().transpose()\n        plot_df['Average: '+class_value] = np.nanmean(rank_data.X.toarray(), axis=0)\n        plot_df['Stdev: '+class_value] = np.nanstd(rank_data.X.toarray(), axis=0)\n        plot_df.sort_values(by=['Average: '+class_value], ascending=False, inplace=True)\n        plot_df['Rank: '+class_value] = np.where(plot_df['Average: '+class_value].isna(), np.nan, np.arange(1, len(plot_df) + 1))\n\n        sorted_indices = plot_df.index\n\n        plot_df = plot_df.loc[adata.var.index]\n        adata.var['Average: ' + class_value] = plot_df['Average: ' + class_value]\n        adata.var['Stdev: ' + class_value] = plot_df['Stdev: ' + class_value]\n        adata.var['Rank: ' + class_value] = plot_df['Rank: ' + class_value]\n        plot_df = plot_df.reindex(sorted_indices)\n\n        stats_df = plot_df.filter(regex = 'Average: |Stdev: |Rank: ', axis=1)\n        plot_df = plot_df.drop(stats_df.columns, axis=1)\n\n        nsample = plot_df.shape[1]\n        nprot = plot_df.shape[0]\n\n        # merge all abundance columns into one column\n        X = np.zeros((nsample*nprot))\n        for i in range(nsample):\n            X[i*nprot:(i+1)*nprot] = plot_df.iloc[:, i].values\n\n        X = X[~np.isnan(X)] # remove NaN values\n        X = X[X != 0] # remove 0 values\n        X = np.log10(X)\n\n        data_X.append(X)\n\n    print('data_X shape: ', len(data_X)) if debug else None\n\n    # boxplot\n    bp = ax.boxplot(data_X, positions=np.arange(1,len(classes_list)+1)-0.06, widths=0.1, patch_artist = True,\n                    flierprops=dict(marker='o', alpha=0.2, markersize=2, markerfacecolor=boxcolor, markeredgecolor=boxcolor),\n                    whiskerprops=dict(color=boxcolor, linestyle='-', linewidth=linewidth),\n                    medianprops=dict(color=boxcolor, linewidth=linewidth),\n                    boxprops=dict(facecolor='none', color=boxcolor, linewidth=linewidth),\n                    capprops=dict(color=boxcolor, linewidth=linewidth))\n\n    # Violinplot\n    vp = ax.violinplot(data_X, points=500, vert=True, positions=np.arange(1,len(classes_list)+1)+0.06,\n                showmeans=False, showextrema=False, showmedians=False)\n\n    for idx, b in enumerate(vp['bodies']):\n        # Get the center of the plot\n        m = np.mean(b.get_paths()[0].vertices[:, 1])\n        # Modify it so we only see the upper half of the violin plot\n        b.get_paths()[0].vertices[:, 0] = np.clip(b.get_paths()[0].vertices[:, 0], idx+1.06, idx+2.06)\n        # Change to the desired color\n        b.set_color(color[idx])\n    # Scatterplot data\n    for idx in range(len(data_X)):\n        features = data_X[idx]\n        # Add jitter effect so the features do not overlap on the y-axis\n        y = np.full(len(features), idx + .8)\n        idxs = np.arange(len(y))\n        out = y.astype(float)\n        out.flat[idxs] += np.random.uniform(low=.1, high=.18, size=len(idxs))\n        y = out\n        ax.scatter(y, features, s=2., c=color[idx], alpha=0.5)\n\n    if debug:\n        return ax, data_X\n    else:\n        return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_rankquant","title":"plot_rankquant","text":"<pre><code>plot_rankquant(ax, pdata, classes=None, layer='X', on='protein', cmap=['Blues'], color=['blue'], order=None, s=20, alpha=0.2, calpha=1, exp_alpha=70, debug=False)\n</code></pre> <p>Plot rank abundance distributions across samples or groups.</p> <p>This function visualizes rank abundance of proteins or peptides, optionally grouped by sample-level classes. Distributions are drawn as scatter plots with adjustable opacity and color schemes. Mean, standard deviation, and rank statistics are written to <code>.var</code> for downstream annotation.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>classes</code> <code>str or list of str</code> <p>One or more <code>.obs</code> columns to group samples. If None, samples are combined into identifier classes.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to use. Default is <code>\"X\"</code>.</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Data level to plot, either <code>\"protein\"</code> or <code>\"peptide\"</code>. Default is <code>\"protein\"</code>.</p> <code>'protein'</code> <code>cmap</code> <code>str or list of str</code> <p>Colormap(s) used for scatter distributions. Default is <code>[\"Blues\"]</code>.</p> <code>['Blues']</code> <code>color</code> <code>list of str</code> <p>List of colors used for scatter distributions. Defaults to <code>[\"blue\"]</code>.</p> <code>['blue']</code> <code>order</code> <code>list of str</code> <p>Custom order of class categories. If None, categories appear in data order.</p> <code>None</code> <code>s</code> <code>float</code> <p>Marker size. Default is 20.</p> <code>20</code> <code>alpha</code> <code>float</code> <p>Marker transparency for distributions. Default is 0.2.</p> <code>0.2</code> <code>calpha</code> <code>float</code> <p>Marker transparency for class means. Default is 1.</p> <code>1</code> <code>exp_alpha</code> <code>float</code> <p>Exponent for scaling probability density values by average abundance. Default is 70.</p> <code>70</code> <code>debug</code> <code>bool</code> <p>If True, print debug information during computation.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis containing the rank abundance plot.</p> Example <p>Plot rank abundance grouped by sample size:     <pre><code>import seaborn as sns\ncolors = sns.color_palette(\"Blues\", 4)\ncmaps = [\"Blues\", \"Reds\", \"Greens\", \"Oranges\"]\norder = [\"sc\", \"5k\", \"10k\", \"20k\"]\nfig, ax = plt.subplots(figsize=(4, 3))\nax = scplt.plot_rankquant(\n    ax, pdata_filter, classes=\"size\",\n    order=order,\n    cmap=cmaps, color=colors, calpha=1, alpha=0.005\n)\n</code></pre></p> <p>Format the plot better:     <pre><code>plt.ylabel(\"Abundance\")\nax.set_ylim(10**ylims[0], 10**ylims[1])\nlegend_patches = [\n    mpatches.Patch(color=color, label=label)\n    for color, label in zip(colors, order)\n]\nplt.legend(\n    handles=legend_patches, bbox_to_anchor=(0.75, 1),\n    loc=2, borderaxespad=0., frameon=False\n)\n</code></pre></p> <p>Highlight specific points on the rank-quant plot:     <pre><code>scplt.mark_rankquant(\n    ax, pdata_filter, mark_df=prot_sc_df,\n    class_values=[\"sc\"], show_label=True,\n    color=\"darkorange\", label_type=\"gene\"\n)\n</code></pre></p> See Also <p>mark_rankquant: Highlight specific proteins or genes on a rank abundance plot.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_rankquant(ax, pdata, classes = None, layer = \"X\", on = 'protein', cmap=['Blues'], color=['blue'], order = None, s=20, alpha=0.2, calpha=1, exp_alpha = 70, debug = False):\n    \"\"\"\n    Plot rank abundance distributions across samples or groups.\n\n    This function visualizes rank abundance of proteins or peptides, optionally\n    grouped by sample-level classes. Distributions are drawn as scatter plots\n    with adjustable opacity and color schemes. Mean, standard deviation, and\n    rank statistics are written to `.var` for downstream annotation.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot.\n        pdata (pAnnData): Input pAnnData object.\n        classes (str or list of str, optional): One or more `.obs` columns to\n            group samples. If None, samples are combined into identifier classes.\n        layer (str): Data layer to use. Default is `\"X\"`.\n        on (str): Data level to plot, either `\"protein\"` or `\"peptide\"`. Default is `\"protein\"`.\n        cmap (str or list of str): Colormap(s) used for scatter distributions.\n            Default is `[\"Blues\"]`.\n        color (list of str): List of colors used for scatter distributions.\n            Defaults to `[\"blue\"]`.\n        order (list of str, optional): Custom order of class categories. If None,\n            categories appear in data order.\n        s (float): Marker size. Default is 20.\n        alpha (float): Marker transparency for distributions. Default is 0.2.\n        calpha (float): Marker transparency for class means. Default is 1.\n        exp_alpha (float): Exponent for scaling probability density values by\n            average abundance. Default is 70.\n        debug (bool): If True, print debug information during computation.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axis containing the rank abundance plot.\n\n    Example:\n        Plot rank abundance grouped by sample size:\n            ```python\n            import seaborn as sns\n            colors = sns.color_palette(\"Blues\", 4)\n            cmaps = [\"Blues\", \"Reds\", \"Greens\", \"Oranges\"]\n            order = [\"sc\", \"5k\", \"10k\", \"20k\"]\n            fig, ax = plt.subplots(figsize=(4, 3))\n            ax = scplt.plot_rankquant(\n                ax, pdata_filter, classes=\"size\",\n                order=order,\n                cmap=cmaps, color=colors, calpha=1, alpha=0.005\n            )\n            ```\n\n        Format the plot better:\n            ```python\n            plt.ylabel(\"Abundance\")\n            ax.set_ylim(10**ylims[0], 10**ylims[1])\n            legend_patches = [\n                mpatches.Patch(color=color, label=label)\n                for color, label in zip(colors, order)\n            ]\n            plt.legend(\n                handles=legend_patches, bbox_to_anchor=(0.75, 1),\n                loc=2, borderaxespad=0., frameon=False\n            )\n            ```\n\n        Highlight specific points on the rank-quant plot:\n            ```python\n            scplt.mark_rankquant(\n                ax, pdata_filter, mark_df=prot_sc_df,\n                class_values=[\"sc\"], show_label=True,\n                color=\"darkorange\", label_type=\"gene\"\n            )\n            ```\n\n    See Also:\n        mark_rankquant: Highlight specific proteins or genes on a rank abundance plot.            \n\n    \"\"\"\n    # all the plot_dfs should now be stored in pdata.var\n    pdata.rank(classes, on, layer)\n\n    adata = utils.get_adata(pdata, on)\n    classes_list = utils.get_classlist(adata, classes = classes, order = order)\n\n    # Ensure colormap and color list match number of classes\n    cmap = cmap if cmap and len(cmap) == len(classes_list) else get_color('cmap', n=len(classes_list))\n    color = color if color and len(color) == len(classes_list) else get_color('colors', n=len(classes_list))\n\n    for j, class_value in enumerate(classes_list):\n        if classes is None or isinstance(classes, (str, list)):\n            values = class_value.split('_') if classes is not str else class_value\n            rank_data = utils.filter(adata, classes, values, debug=False)\n\n        plot_df = rank_data.to_df().transpose()\n        plot_df['Average: '+class_value] = np.nanmean(rank_data.X.toarray(), axis=0)\n        plot_df['Stdev: '+class_value] = np.nanstd(rank_data.X.toarray(), axis=0)\n        plot_df.sort_values(by=['Average: '+class_value], ascending=False, inplace=True)\n        plot_df['Rank: '+class_value] = np.where(plot_df['Average: '+class_value].isna(), np.nan, np.arange(1, len(plot_df) + 1))\n\n        sorted_indices = plot_df.index\n        plot_df = plot_df.loc[adata.var.index]\n        adata.var['Average: ' + class_value] = plot_df['Average: ' + class_value]\n        adata.var['Stdev: ' + class_value] = plot_df['Stdev: ' + class_value]\n        adata.var['Rank: ' + class_value] = plot_df['Rank: ' + class_value]\n        plot_df = plot_df.reindex(sorted_indices)\n\n        # if taking from pdata.var, can continue from here\n        # problem is that we need rank_data, the data consisting of samples from this class to make\n        # stats df should have 3 column, average stdev and rank\n        # plot_df should only have the abundance \n        stats_df = plot_df.filter(regex = 'Average: |Stdev: |Rank: ', axis=1)\n        plot_df = plot_df.drop(stats_df.columns, axis=1)\n        print(stats_df.shape) if debug else None\n        print(plot_df.shape) if debug else None\n\n        nsample = plot_df.shape[1]\n        nprot = plot_df.shape[0]\n\n        # Abundance matrix: shape (nprot, nsample)\n        X_matrix = plot_df.values  # shape: (nprot, nsample)\n        ranks = stats_df['Rank: ' + class_value].values  # shape: (nprot,)\n        mu = np.log10(np.clip(stats_df['Average: ' + class_value].values, 1e-6, None))\n        std = np.log10(np.clip(stats_df['Stdev: ' + class_value].values, 1e-6, None))\n        # Flatten abundance data (X) and repeat ranks (Y)\n        X = X_matrix.flatten(order='F')  # Fortran order stacks column-wise, matching your loop\n        Y = np.tile(ranks, nsample)\n        # Compute Z-values\n        logX = np.log10(np.clip(X, 1e-6, None))\n        z = ((logX - np.tile(mu, nsample)) / np.tile(std, nsample)) ** 2\n        Z = np.exp(-z * exp_alpha)\n        # Remove NaNs\n        mask = ~np.isnan(X)\n        X = X[mask]\n        Y = Y[mask]\n        Z = Z[mask]\n\n        print(f'nsample: {nsample}, nprot: {np.max(Y)}') if debug else None\n\n        ax.scatter(Y, X, c=Z, marker='.',cmap=cmap[j], s=s,alpha=alpha)\n        ax.scatter(stats_df['Rank: '+class_value], \n                   stats_df['Average: '+class_value], \n                   marker='.', \n                   color=color[j], \n                   alpha=calpha,\n                   label=class_value)\n        ax.set_yscale('log')\n        ax.set_xlabel('Rank')\n        ax.set_ylabel('Abundance')\n\n    # format the argument string classes to be first letter capitalized\n    legend_title = (\n        \"/\".join(cls.capitalize() for cls in classes)\n        if isinstance(classes, list)\n        else classes.capitalize() if isinstance(classes, str)\n        else None)\n\n    ax.legend(title=legend_title, loc='best', frameon=True, fontsize='small')\n    return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_significance","title":"plot_significance","text":"<pre><code>plot_significance(ax, y, h, x1=0, x2=1, col='k', pval='n.s.', fontsize=12)\n</code></pre> <p>Plot significance bars on a matplotlib axis.</p> <p>This function draws horizontal significance bars (e.g., for statistical annotations) between two x-positions with a label indicating the p-value or significance level.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot the significance bars.</p> required <code>y</code> <code>float</code> <p>Vertical coordinate of the top of the bars.</p> required <code>h</code> <code>float</code> <p>Height of the vertical ticks extending downward from <code>y</code>.</p> required <code>x1</code> <code>float</code> <p>X-coordinate of the first bar endpoint.</p> <code>0</code> <code>x2</code> <code>float</code> <p>X-coordinate of the second bar endpoint.</p> <code>1</code> <code>col</code> <code>str</code> <p>Color of the bars.</p> <code>'k'</code> <code>pval</code> <code>float or str</code> <p>P-value or significance label.</p> <ul> <li> <p>If a float, it is compared against thresholds (e.g., 0.05, 0.01) to assign   significance markers (<code>*</code>, <code>**</code>, <code>***</code>).</p> </li> <li> <p>If a string, it is directly rendered as the label.</p> </li> </ul> <code>'n.s.'</code> <code>fontsize</code> <code>int</code> <p>Font size of the significance text.</p> <code>12</code> <p>Returns:</p> Type Description <p>None</p> <p>Examples Pending</p> <p>Add usage examples here.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_significance(ax, y, h, x1=0, x2=1, col='k', pval='n.s.', fontsize=12):\n    \"\"\"\n    Plot significance bars on a matplotlib axis.\n\n    This function draws horizontal significance bars (e.g., for statistical annotations)\n    between two x-positions with a label indicating the p-value or significance level.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot the significance bars.\n        y (float): Vertical coordinate of the top of the bars.\n        h (float): Height of the vertical ticks extending downward from `y`.\n        x1 (float): X-coordinate of the first bar endpoint.\n        x2 (float): X-coordinate of the second bar endpoint.\n        col (str): Color of the bars.\n        pval (float or str): P-value or significance label.\n\n            - If a float, it is compared against thresholds (e.g., 0.05, 0.01) to assign\n              significance markers (`*`, `**`, `***`).\n\n            - If a string, it is directly rendered as the label.\n\n        fontsize (int): Font size of the significance text.\n\n    Returns:\n        None\n\n    !!! todo \"Examples Pending\"\n        Add usage examples here.\n    \"\"\"\n\n    # check variable type of pval\n    sig = 'n.s.'\n    if isinstance(pval, float):\n        if pval &gt; 0.05:\n            sig = 'n.s.'\n        else:\n            sig = '*' * int(np.floor(-np.log10(pval)))\n    else:\n        sig = pval\n\n    ax.plot([x1, x1, x2, x2], [y, y+h, y+h, y], lw=1, c=col)\n    ax.text((x1+x2)*.5, y+h, sig, ha='center', va='bottom', color=col, fontsize=fontsize)\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_summary","title":"plot_summary","text":"<pre><code>plot_summary(ax, pdata, value='protein_count', classes=None, plot_mean=True, **kwargs)\n</code></pre> <p>Plot summary statistics of sample metadata.</p> <p>This function visualizes values from <code>pdata.summary</code> (e.g., protein count, peptide count, abundance) as bar plots, optionally grouped by sample-level classes. It supports both per-sample visualization and mean values across groups.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object with <code>.summary</code> metadata table.</p> required <code>value</code> <code>str</code> <p>Column in <code>pdata.summary</code> to plot. Default is <code>'protein_count'</code>.</p> <code>'protein_count'</code> <code>classes</code> <code>str or list of str</code> <p>Sample-level classes to group by. - If None: plot per-sample values directly.</p> <ul> <li> <p>If str: group by the specified column, aggregating with mean if <code>plot_mean=True</code>.</p> </li> <li> <p>If list: when multiple classes are provided, combinations of class values   are used for grouping and subplots are created per unique value of <code>classes[0]</code>.</p> </li> </ul> <code>None</code> <code>plot_mean</code> <code>bool</code> <p>Whether to plot mean \u00b1 standard deviation by class. If True, <code>classes</code> must be provided. Default is True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to seaborn plotting functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>matplotlib.axes.Axes or list of matplotlib.axes.Axes</code> <p>The axis (or </p> <p>list of axes if subplots are created) with the plotted summary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>plot_mean=True</code> but <code>classes</code> is not specified.</p> <code>ValueError</code> <p>If <code>classes</code> is invalid (not None, str, or non-empty list).</p> <p>Examples Pending</p> <p>Add usage examples here.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_summary(ax, pdata, value='protein_count', classes=None, plot_mean=True, **kwargs):\n    \"\"\"\n    Plot summary statistics of sample metadata.\n\n    This function visualizes values from `pdata.summary` (e.g., protein count,\n    peptide count, abundance) as bar plots, optionally grouped by sample-level classes.\n    It supports both per-sample visualization and mean values across groups.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot.\n        pdata (pAnnData): Input pAnnData object with `.summary` metadata table.\n        value (str): Column in `pdata.summary` to plot. Default is `'protein_count'`.\n        classes (str or list of str, optional): Sample-level classes to group by.\n            - If None: plot per-sample values directly.\n\n            - If str: group by the specified column, aggregating with mean if `plot_mean=True`.\n\n            - If list: when multiple classes are provided, combinations of class values\n              are used for grouping and subplots are created per unique value of `classes[0]`.\n\n        plot_mean (bool): Whether to plot mean \u00b1 standard deviation by class.\n            If True, `classes` must be provided. Default is True.\n        **kwargs: Additional keyword arguments passed to seaborn plotting functions.\n\n    Returns:\n        ax (matplotlib.axes.Axes or list of matplotlib.axes.Axes): The axis (or \n        list of axes if subplots are created) with the plotted summary.\n\n    Raises:\n        ValueError: If `plot_mean=True` but `classes` is not specified.\n        ValueError: If `classes` is invalid (not None, str, or non-empty list).\n\n    !!! todo \"Examples Pending\"\n        Add usage examples here.\n    \"\"\"\n\n    if pdata.summary is None:\n        pdata._update_summary()\n\n    summary_data = pdata.summary.copy()\n\n    if plot_mean:\n        if classes is None:\n            raise ValueError(\"Classes must be specified when plot_mean is True.\")\n        elif isinstance(classes, str):\n            sns.barplot(x=classes, y=value, hue=classes, data=summary_data, errorbar='sd', ax=ax, **kwargs)\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n        elif isinstance(classes, list) and len(classes) &gt; 0:\n            if len(classes) == 1:\n                sns.catplot(x=classes[0], y=value, data=summary_data, hue=classes[0], kind='bar', ax=ax, **kwargs)\n                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n            elif len(classes) &gt;= 2:\n                summary_data['combined_classes'] = summary_data[classes[1:]].astype(str).agg('-'.join, axis=1)\n\n                unique_values = summary_data[classes[0]].unique()\n                num_unique_values = len(unique_values)\n\n                fig, ax = plt.subplots(nrows=num_unique_values, figsize=(10, 5 * num_unique_values))\n\n                if num_unique_values == 1:\n                    ax = [ax]\n\n                for ax_sub, unique_value in zip(ax, unique_values):\n                    subset_data = summary_data[summary_data[classes[0]] == unique_value]\n                    sns.barplot(x='combined_classes', y=value, data=subset_data, hue='combined_classes', ax=ax_sub, **kwargs)\n                    ax_sub.set_title(f\"{classes[0]}: {unique_value}\")\n                    ax_sub.set_xticklabels(ax_sub.get_xticklabels(), rotation=45, ha='right')\n    else:\n        if classes is None:\n            sns.barplot(x=summary_data.index, y=value, data=summary_data, ax=ax, **kwargs)\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n        elif isinstance(classes, str):\n            sns.barplot(x=summary_data.index, y=value, hue=classes, data=summary_data, ax=ax, **kwargs)\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n        elif isinstance(classes, list) and len(classes) &gt; 0:\n            if len(classes) == 1:\n                sns.barplot(x=summary_data.index, y=value, hue=classes[0], data=summary_data, ax=ax, **kwargs)\n                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n            elif len(classes) &gt;= 2:\n                summary_data['combined_classes'] = summary_data[classes[1:]].astype(str).agg('-'.join, axis=1)\n                # Create a subplot for each unique value in classes[0]\n                unique_values = summary_data[classes[0]].unique()\n                num_unique_values = len(unique_values)\n\n                fig, ax = plt.subplots(nrows=num_unique_values, figsize=(10, 5 * num_unique_values))\n\n                if num_unique_values == 1:\n                    ax = [ax]  # Ensure axes is iterable\n\n                for ax_sub, unique_value in zip(ax, unique_values):\n                    subset_data = summary_data[summary_data[classes[0]] == unique_value]\n                    sns.barplot(x=subset_data.index, y=value, hue='combined_classes', data=subset_data, ax=ax_sub, **kwargs)\n                    ax_sub.set_title(f\"{classes[0]}: {unique_value}\")\n                    ax_sub.set_xticklabels(ax_sub.get_xticklabels(), rotation=45, ha='right')\n\n                plt.tight_layout()            \n        else:\n            raise ValueError(\"Invalid 'classes' parameter. It should be None, a string, or a non-empty list.\")\n\n    plt.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_umap","title":"plot_umap","text":"<pre><code>plot_umap(ax, pdata, classes=None, layer='X', on='protein', cmap='default', s=20, alpha=0.8, umap_params={}, text_size=10, force=False, return_fit=False)\n</code></pre> <p>Plot UMAP projection of protein or peptide abundance data.</p> <p>This function projects the data using UMAP and colors samples based on metadata or abundance of a specific feature. Supports categorical or continuous coloring, with automatic handling of legends and colormaps.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axis to plot on (must be 3D if n_components=3).</p> required <code>pdata</code> <code>pAnnData</code> <p>The pAnnData object containing .prot, .pep, and .summary.</p> required <code>classes</code> <code>str or list of str or None</code> <ul> <li>None: all samples plotted in grey</li> <li>str: column in <code>.obs</code> or a gene/protein to color by</li> <li>list of str: combine multiple <code>.obs</code> columns (e.g., ['cellline', 'day'])</li> </ul> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to use for UMAP input (default: \"X\").</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Whether to use 'protein' or 'peptide' data (default: 'protein').</p> <code>'protein'</code> <code>cmap</code> <code>str, list, or dict</code> <ul> <li>'default': use internal color scheme</li> <li>list: list of colors, assigned to class labels in sorted order</li> <li>dict: {label: color} mapping</li> <li>str: continuous colormap name (e.g., 'viridis') for abundance coloring</li> </ul> <code>'default'</code> <code>s</code> <code>float</code> <p>Marker size (default: 20).</p> <code>20</code> <code>alpha</code> <code>float</code> <p>Marker opacity (default: 0.8).</p> <code>0.8</code> <code>umap_params</code> <code>dict</code> <p>Parameters to pass to UMAP (e.g., 'min_dist', 'metric').</p> <code>{}</code> <code>text_size</code> <code>int</code> <p>Font size for axis labels and legend (default: 10).</p> <code>10</code> <code>force</code> <code>bool</code> <p>If True, re-compute UMAP even if results already exist.</p> <code>False</code> <code>return_fit</code> <code>bool</code> <p>If True, return the fitted UMAP object along with the axis.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The axis with the UMAP plot.</p> <code>fit_umap</code> <code>UMAP</code> <p>The fitted UMAP object.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If 'n_components' is 3 and the axis is not 3D.</p> Example <p>Plot by treatment group with default palette:     <pre><code>plot_umap(ax, pdata, classes='treatment')\n</code></pre></p> <p>Plot by protein abundance (continuous coloring):     <pre><code>plot_umap(ax, pdata, classes='P12345', cmap='plasma')\n</code></pre></p> <p>Plot with custom palette:     <pre><code>custom_palette = {'ctrl': '#CCCCCC', 'treated': '#E41A1C'}\nplot_umap(ax, pdata, classes='group', cmap=custom_palette)\n</code></pre></p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_umap(ax, pdata, classes = None, layer = \"X\", on = 'protein', cmap='default', s=20, alpha=.8, umap_params={}, text_size = 10, force = False, return_fit=False):\n    \"\"\"\n    Plot UMAP projection of protein or peptide abundance data.\n\n    This function projects the data using UMAP and colors samples based on\n    metadata or abundance of a specific feature. Supports categorical or\n    continuous coloring, with automatic handling of legends and colormaps.\n\n    Args:\n        ax (matplotlib.axes.Axes): The axis to plot on (must be 3D if n_components=3).\n        pdata (scpviz.pAnnData): The pAnnData object containing .prot, .pep, and .summary.\n        classes (str or list of str or None): \n            - None: all samples plotted in grey\n            - str: column in `.obs` or a gene/protein to color by\n            - list of str: combine multiple `.obs` columns (e.g., ['cellline', 'day'])\n        layer (str): Data layer to use for UMAP input (default: \"X\").\n        on (str): Whether to use 'protein' or 'peptide' data (default: 'protein').\n        cmap (str, list, or dict): \n            - 'default': use internal color scheme\n            - list: list of colors, assigned to class labels in sorted order\n            - dict: {label: color} mapping\n            - str: continuous colormap name (e.g., 'viridis') for abundance coloring\n        s (float): Marker size (default: 20).\n        alpha (float): Marker opacity (default: 0.8).\n        umap_params (dict): Parameters to pass to UMAP (e.g., 'min_dist', 'metric').\n        text_size (int): Font size for axis labels and legend (default: 10).\n        force (bool): If True, re-compute UMAP even if results already exist.\n        return_fit (bool): If True, return the fitted UMAP object along with the axis.\n\n    Returns:\n        ax (matplotlib.axes.Axes): The axis with the UMAP plot.\n        fit_umap (umap.UMAP): The fitted UMAP object.\n\n    Raises:\n        AssertionError: If 'n_components' is 3 and the axis is not 3D.\n\n    Example:\n        Plot by treatment group with default palette:\n            ```python\n            plot_umap(ax, pdata, classes='treatment')\n            ```\n\n        Plot by protein abundance (continuous coloring):\n            ```python\n            plot_umap(ax, pdata, classes='P12345', cmap='plasma')\n            ```\n\n        Plot with custom palette:\n            ```python\n            custom_palette = {'ctrl': '#CCCCCC', 'treated': '#E41A1C'}\n            plot_umap(ax, pdata, classes='group', cmap=custom_palette)\n            ```\n    \"\"\"\n    default_umap_params = {'n_components': 2, 'random_state': 42}\n    umap_param = {**default_umap_params, **(umap_params if umap_params else {})}\n\n    if umap_param['n_components'] == 3:\n        assert ax.name == '3d', \"The ax must be a 3D projection, please define projection='3d'\"\n\n    if on == 'protein':\n        adata = pdata.prot\n    elif on == 'peptide':\n        adata = pdata.pep\n    else:\n        raise ValueError(\"Invalid value for 'on'. Options are 'protein' or 'peptide'.\")\n\n    if force == False:\n        if 'X_umap' in adata.obsm.keys():\n            print(f'{utils.format_log_prefix(\"warn\")} UMAP already exists in {on} data, using existing UMAP. Run with `force=True` to recompute.')\n        else:\n            pdata.umap(on=on, layer=layer, **umap_param)\n    else:\n        print(f'UMAP calculation forced, re-calculating UMAP')\n        pdata.umap(on=on, layer=layer, **umap_param)\n\n    Xt = adata.obsm['X_umap']\n    umap = adata.uns['umap']\n\n    color_mapped, cmap_resolved, legend_elements = resolve_plot_colors(adata, classes, cmap, layer=layer)\n\n    if umap_param['n_components'] == 1:\n        ax.scatter(Xt[:,0], range(len(Xt)), c=color_mapped, cmap=cmap_resolved, s=s, alpha=alpha)\n        ax.set_xlabel('UMAP 1', fontsize=text_size)\n    elif umap_param['n_components'] == 2:\n        ax.scatter(Xt[:,0], Xt[:,1], c=color_mapped, cmap=cmap_resolved, s=s, alpha=alpha)\n        ax.set_xlabel('UMAP 1', fontsize=text_size)\n        ax.set_ylabel('UMAP 2', fontsize=text_size)\n    elif umap_param['n_components'] == 3:\n        ax.scatter(Xt[:,0], Xt[:,1], Xt[:,2], c=color_mapped, cmap=cmap_resolved, s=s, alpha=alpha)\n        ax.set_xlabel('UMAP 1', fontsize=text_size)\n        ax.set_ylabel('UMAP 2', fontsize=text_size)\n        ax.set_zlabel('UMAP 3', fontsize=text_size)\n\n    if legend_elements:\n        if classes is None:\n            legend_title = None\n        elif isinstance(classes, list):\n            legend_title = \"/\".join(c.capitalize() for c in classes)\n        else:\n            legend_title = str(classes).capitalize()\n        ax.legend(handles=legend_elements, title=legend_title, loc='upper right', bbox_to_anchor=(1.3, 1), fontsize=text_size)\n\n    if return_fit:\n        return ax, umap\n    else:\n        return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_upset","title":"plot_upset","text":"<pre><code>plot_upset(pdata, classes, return_contents=False, **kwargs)\n</code></pre> <p>Plot an UpSet diagram of shared proteins or peptides across groups.</p> <p>This function generates an UpSet plot for &gt;2 sets based on presence/absence data across specified sample-level classes. Uses the <code>upsetplot</code> package for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>classes</code> <code>str or list of str</code> <p>Sample-level classes to partition proteins or peptides into sets.</p> required <code>return_contents</code> <code>bool</code> <p>If True, return both the UpSet object and the underlying set contents used for plotting.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>upsetplot.UpSet</code>. See the upsetplot documentation for more details. Common arguments include:</p> <ul> <li><code>sort_categories_by</code> (str): How to sort categories. Options are   <code>\"cardinality\"</code>, <code>\"input\"</code>, <code>\"-cardinality\"</code>, or <code>\"-input\"</code>.</li> <li><code>min_subset_size</code> (int): Minimum subset size to display.</li> </ul> <code>{}</code> <p>Returns:</p> Name Type Description <code>upset</code> <code>UpSet</code> <p>The UpSet plot object.</p> <code>tuple</code> <code>(UpSet, DataFrame)</code> <p>Returned if</p> <p><code>return_contents=True</code>. The DataFrame contains set membership as a</p> <p>multi-index.</p> Example <p>Basic usage with set size categories:     <pre><code>upplot, size_upset = scplt.plot_upset(\n    pdata_filter, classes=\"size\", sort_categories_by=\"-input\"\n)\nuplot = upplot.plot()\nuplot[\"intersections\"].set_ylabel(\"Subset size\")\nuplot[\"totals\"].set_xlabel(\"Protein count\")\nplt.show()\n</code></pre></p> <p>Optional styling of the plot can also be done:     <pre><code>upplot.style_subsets(\n    present=[\"sc\"], absent=[\"2k\", \"5k\", \"10k\", \"20k\"],\n    edgecolor=\"black\", facecolor=\"darkorange\", linewidth=2, label=\"sc only\"\n)\nupplot.style_subsets(\n    absent=[\"sc\"], present=[\"2k\", \"5k\", \"10k\", \"20k\"],\n    edgecolor=\"white\", facecolor=\"#7F7F7F\", linewidth=2, label=\"in all but sc\"\n)\n</code></pre></p> See Also <p>plot_venn: Plot a Venn diagram for 2 to 3 sets. plot_rankquant: Rank-based visualization of protein/peptide distributions.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_upset(pdata, classes, return_contents = False, **kwargs):\n    \"\"\"\n    Plot an UpSet diagram of shared proteins or peptides across groups.\n\n    This function generates an UpSet plot for &gt;2 sets based on presence/absence\n    data across specified sample-level classes. Uses the `upsetplot` package\n    for visualization.\n\n    Args:\n        pdata (pAnnData): Input pAnnData object.\n\n        classes (str or list of str): Sample-level classes to partition proteins\n            or peptides into sets.\n\n        return_contents (bool): If True, return both the UpSet object and the\n            underlying set contents used for plotting.\n\n        **kwargs: Additional keyword arguments passed to `upsetplot.UpSet`.  \n            See the [upsetplot documentation](https://upsetplot.readthedocs.io/en/stable/)  \n            for more details. Common arguments include:\n\n            - `sort_categories_by` (str): How to sort categories. Options are\n              `\"cardinality\"`, `\"input\"`, `\"-cardinality\"`, or `\"-input\"`.\n            - `min_subset_size` (int): Minimum subset size to display.\n\n    Returns:\n        upset (upsetplot.UpSet): The UpSet plot object.\n\n        tuple (upsetplot.UpSet, pandas.DataFrame): Returned if\n        `return_contents=True`. The DataFrame contains set membership as a\n        multi-index.\n\n    Example:\n        Basic usage with set size categories:\n            ```python\n            upplot, size_upset = scplt.plot_upset(\n                pdata_filter, classes=\"size\", sort_categories_by=\"-input\"\n            )\n            uplot = upplot.plot()\n            uplot[\"intersections\"].set_ylabel(\"Subset size\")\n            uplot[\"totals\"].set_xlabel(\"Protein count\")\n            plt.show()\n            ```\n\n        Optional styling of the plot can also be done:\n            ```python\n            upplot.style_subsets(\n                present=[\"sc\"], absent=[\"2k\", \"5k\", \"10k\", \"20k\"],\n                edgecolor=\"black\", facecolor=\"darkorange\", linewidth=2, label=\"sc only\"\n            )\n            upplot.style_subsets(\n                absent=[\"sc\"], present=[\"2k\", \"5k\", \"10k\", \"20k\"],\n                edgecolor=\"white\", facecolor=\"#7F7F7F\", linewidth=2, label=\"in all but sc\"\n            )\n            ```\n\n    See Also:\n        plot_venn: Plot a Venn diagram for 2 to 3 sets.  \n        plot_rankquant: Rank-based visualization of protein/peptide distributions.\n    \"\"\"\n\n    upset_contents = utils.get_upset_contents(pdata, classes = classes)\n    upplot = upsetplot.UpSet(upset_contents, subset_size=\"count\", show_counts=True, facecolor = 'black', **kwargs)\n\n    if return_contents:\n        return upplot, upset_contents\n    else:\n        return upplot\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_venn","title":"plot_venn","text":"<pre><code>plot_venn(ax, pdata, classes, set_colors='default', return_contents=False, label_order=None, **kwargs)\n</code></pre> <p>Plot a Venn diagram of shared proteins or peptides across groups.</p> <p>This function generates a 2- or 3-set Venn diagram based on presence/absence data across specified sample-level classes. For more than 3 sets, use <code>plot_upset()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>classes</code> <code>str or list of str</code> <p>Sample-level classes to partition proteins or peptides into sets.</p> required <code>set_colors</code> <code>str or list of str</code> <p>Colors for the sets.</p> <ul> <li><code>\"default\"</code>: use internal color palette.</li> <li>list of str: custom color list with length equal to the number of sets.</li> </ul> <code>'default'</code> <code>return_contents</code> <code>bool</code> <p>If True, return both the axis and the underlying set contents used for plotting.</p> <code>False</code> <code>label_order</code> <code>list of str</code> <p>Custom order of set labels. Must contain the same elements as <code>classes</code>.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to matplotlib-venn functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis containing the Venn diagram. Returned if <code>return_contents=False</code></p> <code>tuple</code> <code>(Axes, dict)</code> <p>Returned if <code>return_contents=True</code>.</p> <p>The dictionary maps class labels to sets of feature identifiers.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If number of sets is not 2 or 3.</p> <code>ValueError</code> <p>If <code>label_order</code> does not contain the same elements as <code>classes</code>.</p> <code>ValueError</code> <p>If custom <code>set_colors</code> length does not match number of sets.</p> Example <p>Plot a 2-set Venn diagram of shared proteins:     <pre><code>fig, ax = plt.subplots()\nscplt.plot_venn(\n    ax, pdata_1mo_snpc, classes=\"sample\",\n    set_colors=[\"#1f77b4\", \"#ff7f0e\"]\n)\n</code></pre></p> See Also <p>plot_upset: Plot an UpSet diagram for &gt;3 sets. plot_rankquant: Rank-based visualization of protein/peptide distributions.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_venn(ax, pdata, classes, set_colors = 'default', return_contents = False, label_order=None, **kwargs):\n    \"\"\"\n    Plot a Venn diagram of shared proteins or peptides across groups.\n\n    This function generates a 2- or 3-set Venn diagram based on presence/absence\n    data across specified sample-level classes. For more than 3 sets, use\n    `plot_upset()` instead.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot.\n        pdata (pAnnData): Input pAnnData object.\n        classes (str or list of str): Sample-level classes to partition proteins\n            or peptides into sets.\n        set_colors (str or list of str): Colors for the sets.\n\n            - `\"default\"`: use internal color palette.\n            - list of str: custom color list with length equal to the number of sets.\n\n        return_contents (bool): If True, return both the axis and the underlying\n            set contents used for plotting.\n        label_order (list of str, optional): Custom order of set labels. Must\n            contain the same elements as `classes`.\n        **kwargs: Additional keyword arguments passed to matplotlib-venn functions.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axis containing the Venn diagram. Returned if `return_contents=False`\n\n        tuple (matplotlib.axes.Axes, dict): Returned if `return_contents=True`.\n        The dictionary maps class labels to sets of feature identifiers.\n\n    Raises:\n        ValueError: If number of sets is not 2 or 3.\n        ValueError: If `label_order` does not contain the same elements as `classes`.\n        ValueError: If custom `set_colors` length does not match number of sets.\n\n    Example:\n        Plot a 2-set Venn diagram of shared proteins:\n            ```python\n            fig, ax = plt.subplots()\n            scplt.plot_venn(\n                ax, pdata_1mo_snpc, classes=\"sample\",\n                set_colors=[\"#1f77b4\", \"#ff7f0e\"]\n            )\n            ```\n\n    See Also:\n        plot_upset: Plot an UpSet diagram for &gt;3 sets.  \n        plot_rankquant: Rank-based visualization of protein/peptide distributions.\n    \"\"\"\n    upset_contents = utils.get_upset_contents(pdata, classes, upsetForm=False)\n\n    num_keys = len(upset_contents)\n    if set_colors == 'default':\n        set_colors = get_color('colors', n=num_keys)\n    elif len(set_colors) != num_keys:\n        raise ValueError(\"The number of colors provided must match the number of sets.\")\n\n    if label_order is not None:\n        if set(label_order) != set(upset_contents.keys()):\n            raise ValueError(\"`label_order` must contain the same elements as `classes`.\")\n        set_labels = label_order\n        set_list = [set(upset_contents[label]) for label in set_labels]\n    else:\n        set_labels = list(upset_contents.keys())\n        set_list = [set(value) for value in upset_contents.values()]\n\n    try:\n        # New API (matplotlib-venn \u2265 0.12)\n        from matplotlib_venn.layout.venn2 import DefaultLayoutAlgorithm as Venn2Layout\n        from matplotlib_venn.layout.venn3 import DefaultLayoutAlgorithm as Venn3Layout\n        from matplotlib_venn import venn2, venn2_circles, venn3, venn3_circles\n        USE_LAYOUT = True\n    except ImportError:\n        # Older API (no layout subpackage)\n        from matplotlib_venn import venn2_unweighted, venn3_unweighted, venn2_circles, venn3_circles\n        USE_LAYOUT = False\n\n    if USE_LAYOUT:\n        venn_functions = {\n            2: lambda: (venn2(set_list, ax = ax, set_labels=set_labels, set_colors=tuple(set_colors), alpha=0.5, layout_algorithm=Venn2Layout(fixed_subset_sizes=(1,1,1)), **kwargs),\n                        venn2_circles(subsets=(1, 1, 1), ax = ax,  linewidth=1)),\n            3: lambda: (venn3(set_list, ax = ax, set_labels=set_labels, set_colors=tuple(set_colors), alpha=0.5, layout_algorithm=Venn3Layout(fixed_subset_sizes=(1,1,1,1,1,1,1)), **kwargs),\n                        venn3_circles(subsets=(1, 1, 1, 1, 1, 1, 1), ax = ax, linewidth=1))\n        }\n    else:\n        venn_functions = { \n            2: lambda: (venn2_unweighted(set_list, ax = ax, set_labels=set_labels, set_colors=tuple(set_colors), alpha=0.5, **kwargs), \n                        venn2_circles(subsets=(1, 1, 1), ax = ax, linewidth=1)), \n            3: lambda: (venn3_unweighted(set_list, ax = ax, set_labels=set_labels, set_colors=tuple(set_colors), alpha=0.5, **kwargs), \n                        venn3_circles(subsets=(1, 1, 1, 1, 1, 1, 1), ax = ax, linewidth=1)) }        \n\n    if num_keys in venn_functions:\n        ax = venn_functions[num_keys]()\n    else:\n        raise ValueError(\"Venn diagrams only accept either 2 or 3 sets. For more than 3 sets, use the plot_upset function.\")\n\n    if return_contents:\n        return ax, upset_contents\n    else:\n        return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.plot_volcano","title":"plot_volcano","text":"<pre><code>plot_volcano(ax, pdata=None, values=None, method='ttest', fold_change_mode='mean', label=5, label_type='Gene', color=None, alpha=0.5, pval=0.05, log2fc=1, linewidth=0.5, fontsize=8, no_marks=False, classes=None, de_data=None, return_df=False, **kwargs)\n</code></pre> <p>Plot a volcano plot of differential expression results.</p> <p>This function calculates differential expression (DE) between two groups and visualizes results as a volcano plot. Alternatively, it can use pre-computed DE results (e.g. from <code>pdata.de()</code>).</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis on which to plot.</p> required <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object. Required if <code>de_data</code> is not provided.</p> <code>None</code> <code>values</code> <code>list or dict</code> <p>Values to compare between groups.</p> <ul> <li> <p>Legacy list format: <code>[\"group1\", \"group2\"]</code></p> </li> <li> <p>Dictionary format: list of dicts specifying multiple conditions,   e.g. <code>[{\"cellline\": \"HCT116\", \"treatment\": \"DMSO\"},          {\"cellline\": \"HCT116\", \"treatment\": \"DrugX\"}]</code>.</p> </li> </ul> <code>None</code> <code>method</code> <code>str</code> <p>Statistical test method. Default is <code>\"ttest\"</code>. Options are <code>\"ttest\"</code>, <code>\"mannwhitneyu\"</code>, <code>\"wilcoxon\"</code>.</p> <code>'ttest'</code> <code>fold_change_mode</code> <code>str</code> <p>Method for computing fold change.</p> <ul> <li> <p><code>\"mean\"</code>: log2(mean(group1) / mean(group2))</p> </li> <li> <p><code>\"pairwise_median\"</code>: median of all pairwise log2 ratios.</p> </li> </ul> <code>'mean'</code> <code>label</code> <code>int, list, or None</code> <p>Features to highlight.</p> <ul> <li> <p>If int: label top and bottom n features.</p> </li> <li> <p>If list of str: label only the specified features.</p> </li> <li> <p>If list of two ints: <code>[top, bottom]</code> to label asymmetric counts.</p> </li> <li> <p>If None: no labels plotted.</p> </li> </ul> <code>5</code> <code>label_type</code> <code>str</code> <p>Label content type. Currently <code>\"Gene\"</code> is recommended.</p> <code>'Gene'</code> <code>color</code> <code>dict</code> <p>Dictionary mapping significance categories to colors. Defaults to grey/red/blue.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Point transparency. Default is 0.5.</p> <code>0.5</code> <code>pval</code> <code>float</code> <p>P-value threshold for significance. Default is 0.05.</p> <code>0.05</code> <code>log2fc</code> <code>float</code> <p>Log2 fold change threshold for significance. Default is 1.</p> <code>1</code> <code>linewidth</code> <code>float</code> <p>Line width for threshold lines. Default is 0.5.</p> <code>0.5</code> <code>fontsize</code> <code>int</code> <p>Font size for feature labels. Default is 8.</p> <code>8</code> <code>no_marks</code> <code>bool</code> <p>If True, suppress coloring of significant points and plot all points in grey. Default is False.</p> <code>False</code> <code>classes</code> <code>str</code> <p>Sample class column to use for group comparison.</p> <code>None</code> <code>de_data</code> <code>DataFrame</code> <p>Pre-computed DE results. Must contain <code>\"log2fc\"</code>, <code>\"p_value\"</code>, and <code>\"significance\"</code> columns.</p> <code>None</code> <code>return_df</code> <code>bool</code> <p>If True, return both the axis and the DataFrame used for plotting. Default is False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>matplotlib.pyplot.scatter</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>Axis with the volcano plot if <code>return_df=False</code>.</p> <code>tuple</code> <code>(Axes, DataFrame)</code> <p>Returned if <code>return_df=True</code>.</p> Usage Tips <p>mark_volcano: Highlight specific features on an existing volcano plot. - For selective highlighting, set <code>no_marks=True</code> to render all points   in grey, then call <code>mark_volcano()</code> to add specific features of interest.</p> <p>add_volcano_legend: Add standard legend handles for volcano plots. - Use the helper function <code>add_volcano_legend(ax)</code> to add standard   significance legend handles.</p> Example <p>Dictionary-style input:     <pre><code>values = [\n    {\"cellline\": \"HCT116\", \"treatment\": \"DMSO\"},\n    {\"cellline\": \"HCT116\", \"treatment\": \"DrugX\"}\n]\ncolors = sns.color_palette(\"Paired\")[4:6]\ncolor_dict = dict(zip(['downregulated', 'upregulated'], colors))\nax, df = plot_volcano(ax, pdata, classes=\"cellline\", values=values)\n</code></pre> Legacy input:     <pre><code>ax, df = plot_volcano(ax, pdata, classes=\"cellline\", values=[\"A\", \"B\"], color=color_dict)\nadd_volcano_legend(ax)\n</code></pre></p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def plot_volcano(ax, pdata=None, values=None, method='ttest', fold_change_mode='mean', label=5,\n                 label_type='Gene', color=None, alpha=0.5, pval=0.05, log2fc=1, linewidth=0.5,\n                 fontsize=8, no_marks=False, classes=None, de_data=None, return_df=False, **kwargs):\n    \"\"\"\n    Plot a volcano plot of differential expression results.\n\n    This function calculates differential expression (DE) between two groups\n    and visualizes results as a volcano plot. Alternatively, it can use\n    pre-computed DE results (e.g. from `pdata.de()`).\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis on which to plot.\n        pdata (pAnnData, optional): Input pAnnData object. Required if `de_data`\n            is not provided.\n        values (list or dict, optional): Values to compare between groups.\n\n            - Legacy list format: `[\"group1\", \"group2\"]`\n\n            - Dictionary format: list of dicts specifying multiple conditions,\n              e.g. `[{\"cellline\": \"HCT116\", \"treatment\": \"DMSO\"},\n                     {\"cellline\": \"HCT116\", \"treatment\": \"DrugX\"}]`.\n\n        method (str): Statistical test method. Default is `\"ttest\"`. Options are `\"ttest\"`, `\"mannwhitneyu\"`, `\"wilcoxon\"`.\n        fold_change_mode (str): Method for computing fold change.\n\n            - `\"mean\"`: log2(mean(group1) / mean(group2))\n\n            - `\"pairwise_median\"`: median of all pairwise log2 ratios.\n\n        label (int, list, or None): Features to highlight.\n\n            - If int: label top and bottom *n* features.\n\n            - If list of str: label only the specified features.\n\n            - If list of two ints: `[top, bottom]` to label asymmetric counts.\n\n            - If None: no labels plotted.\n\n        label_type (str): Label content type. Currently `\"Gene\"` is recommended.\n        color (dict, optional): Dictionary mapping significance categories\n            to colors. Defaults to grey/red/blue.\n        alpha (float): Point transparency. Default is 0.5.\n        pval (float): P-value threshold for significance. Default is 0.05.\n        log2fc (float): Log2 fold change threshold for significance. Default is 1.\n        linewidth (float): Line width for threshold lines. Default is 0.5.\n        fontsize (int): Font size for feature labels. Default is 8.\n        no_marks (bool): If True, suppress coloring of significant points and\n            plot all points in grey. Default is False.\n        classes (str, optional): Sample class column to use for group comparison.\n        de_data (pandas.DataFrame, optional): Pre-computed DE results. Must contain\n            `\"log2fc\"`, `\"p_value\"`, and `\"significance\"` columns.\n        return_df (bool): If True, return both the axis and the DataFrame used\n            for plotting. Default is False.\n        **kwargs: Additional keyword arguments passed to `matplotlib.pyplot.scatter`.\n\n    Returns:\n        ax (matplotlib.axes.Axes): Axis with the volcano plot if `return_df=False`.\n        tuple (matplotlib.axes.Axes, pandas.DataFrame): Returned if `return_df=True`.\n\n    Usage Tips:\n        mark_volcano: Highlight specific features on an existing volcano plot.  \n        - For selective highlighting, set `no_marks=True` to render all points\n          in grey, then call `mark_volcano()` to add specific features of interest.\n\n        add_volcano_legend: Add standard legend handles for volcano plots.\n        - Use the helper function `add_volcano_legend(ax)` to add standard\n          significance legend handles.\n\n    Example:\n        Dictionary-style input:\n            ```python\n            values = [\n                {\"cellline\": \"HCT116\", \"treatment\": \"DMSO\"},\n                {\"cellline\": \"HCT116\", \"treatment\": \"DrugX\"}\n            ]\n            colors = sns.color_palette(\"Paired\")[4:6]\n            color_dict = dict(zip(['downregulated', 'upregulated'], colors))\n            ax, df = plot_volcano(ax, pdata, classes=\"cellline\", values=values)\n            ```\n        Legacy input:\n            ```python\n            ax, df = plot_volcano(ax, pdata, classes=\"cellline\", values=[\"A\", \"B\"], color=color_dict)\n            add_volcano_legend(ax)\n            ```\n\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from adjustText import adjust_text\n    import matplotlib.patheffects as PathEffects\n\n    if de_data is None and pdata is None:\n        raise ValueError(\"Either de_data or pdata must be provided.\")\n\n    if de_data is not None:\n        volcano_df = de_data.copy()\n    else:\n        if values is None:\n          raise ValueError(\"If pdata is provided, values must also be provided.\")\n        if isinstance(values, list) and isinstance(values[0], dict):\n          volcano_df = pdata.de(values=values, method=method, pval=pval, log2fc=log2fc, fold_change_mode=fold_change_mode)\n        else:\n            volcano_df = pdata.de(class_type=classes, values=values, method=method, pval=pval, log2fc=log2fc, fold_change_mode=fold_change_mode)\n\n    df = volcano_df.copy()\n    volcano_df = volcano_df.dropna(subset=['p_value']).copy()\n    volcano_df = volcano_df[volcano_df[\"significance\"] != \"not comparable\"]\n\n    default_color = {'not significant': 'grey', 'upregulated': 'red', 'downregulated': 'blue'}\n    if color:\n        default_color.update(color)\n    elif no_marks:\n        default_color = {k: 'grey' for k in default_color}\n\n    scatter_kwargs = dict(s=20, edgecolors='none')\n    scatter_kwargs.update(kwargs)\n    colors = volcano_df['significance'].astype(str).map(default_color)\n\n    ax.scatter(volcano_df['log2fc'], volcano_df['-log10(p_value)'],\n               c=colors, alpha=alpha, **scatter_kwargs)\n\n    ax.axhline(-np.log10(pval), color='black', linestyle='--', linewidth=linewidth)\n    ax.axvline(log2fc, color='black', linestyle='--', linewidth=linewidth)\n    ax.axvline(-log2fc, color='black', linestyle='--', linewidth=linewidth)\n\n    ax.set_xlabel('$log_{2}$ fold change')\n    ax.set_ylabel('-$log_{10}$ p value')\n\n    log2fc_clean = volcano_df['log2fc'].replace([np.inf, -np.inf], np.nan).dropna()\n    if log2fc_clean.empty:\n        max_abs_log2fc = 1  # default range if nothing valid\n    else:\n        max_abs_log2fc = log2fc_clean.abs().max() + 0.5\n    ax.set_xlim(-max_abs_log2fc, max_abs_log2fc)\n\n\n    if not no_marks and label not in [None, 0, [0, 0]]:\n        if isinstance(label, int):\n            upregulated = volcano_df[volcano_df['significance'] == 'upregulated'].sort_values('significance_score', ascending=False)\n            downregulated = volcano_df[volcano_df['significance'] == 'downregulated'].sort_values('significance_score', ascending=True)\n            label_df = pd.concat([upregulated.head(label), downregulated.head(label)])\n        elif isinstance(label, list):\n            if len(label) == 2 and all(isinstance(i, int) for i in label):\n                upregulated = volcano_df[volcano_df['significance'] == 'upregulated'].sort_values('significance_score', ascending=False)\n                downregulated = volcano_df[volcano_df['significance'] == 'downregulated'].sort_values('significance_score', ascending=True)\n                label_df = pd.concat([upregulated.head(label[0]), downregulated.head(label[1])])\n            else:\n                label_lower = [str(l).lower() for l in label]\n                label_df = volcano_df[\n                volcano_df.index.str.lower().isin(label_lower) |\n                volcano_df['Genes'].str.lower().isin(label_lower)\n            ]\n\n        else:\n            raise ValueError(\"label must be int or list\")\n\n        texts = []\n        for i in range(len(label_df)):\n            gene = label_df.iloc[i].get('Genes', label_df.index[i])\n            txt = plt.text(label_df.iloc[i]['log2fc'],\n                           label_df.iloc[i]['-log10(p_value)'],\n                           s=gene,\n                           fontsize=fontsize,\n                           bbox=dict(facecolor='white', edgecolor='black', boxstyle='round', alpha=0.6))\n            txt.set_path_effects([PathEffects.withStroke(linewidth=3, foreground='w')])\n            texts.append(txt)\n\n        adjust_text(texts, expand=(2, 2), arrowprops=dict(arrowstyle='-&gt;', color='k', zorder=5))\n\n    # Add group names and DE counts to plot\n    def format_group(values_entry, classes):\n        if isinstance(values_entry, dict):\n            return \"/\".join(str(v) for v in values_entry.values())\n        elif isinstance(values_entry, list) and isinstance(classes, list) and len(values_entry) == len(classes):\n            return \"/\".join(str(v) for v in values_entry)\n        return str(values_entry)\n\n    group1 = group2 = \"\"\n    if isinstance(values, list) and len(values) == 2:\n        group1 = format_group(values[0], classes)\n        group2 = format_group(values[1], classes)\n\n    up_count = (volcano_df['significance'] == 'upregulated').sum()\n    down_count = (volcano_df['significance'] == 'downregulated').sum()\n\n    bbox_style = dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor='black')\n\n    ax.annotate(group1, xy=(0.98, 1.07), xycoords='axes fraction',\n                ha='right', va='bottom', fontsize=fontsize, weight='bold', bbox=bbox_style)\n    ax.annotate(f'n={up_count}', xy=(0.98, 1.015), xycoords='axes fraction',\n                ha='right', va='bottom', fontsize=fontsize, color=default_color.get('upregulated', 'red'))\n\n    ax.annotate(group2, xy=(0.02, 1.07), xycoords='axes fraction',\n                ha='left', va='bottom', fontsize=fontsize, weight='bold', bbox=bbox_style)\n    ax.annotate(f'n={down_count}', xy=(0.02, 1.015), xycoords='axes fraction',\n                ha='left', va='bottom', fontsize=fontsize, color=default_color.get('downregulated', 'blue'))\n\n    if return_df:\n        return ax, df\n    else:\n        return ax\n</code></pre>"},{"location":"reference/plotting/#src.scpviz.plotting.resolve_plot_colors","title":"resolve_plot_colors","text":"<pre><code>resolve_plot_colors(adata, classes, cmap, layer='X')\n</code></pre> <p>Resolve colors for PCA or abundance plots.</p> <p>This helper function determines how samples should be colored in plotting functions based on categorical or continuous class values. It returns mapped color values, a colormap (if applicable), and legend handles.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>AnnData object (protein or peptide level).</p> required <code>classes</code> <code>str</code> <p>Class used for coloring. Can be:</p> <ul> <li> <p>An <code>.obs</code> column name (categorical or continuous).</p> </li> <li> <p>A gene or protein identifier, in which case coloring is based   on abundance values from the specified <code>layer</code>.</p> </li> </ul> required <code>cmap</code> <code>str, list, or matplotlib colormap</code> <p>Colormap to use.</p> <ul> <li> <p><code>\"default\"</code>: uses <code>get_color()</code> scheme.</p> </li> <li> <p>list of colors: categorical mapping.</p> </li> <li> <p>colormap name or object: continuous mapping.</p> </li> </ul> required <code>layer</code> <code>str</code> <p>Data layer to extract abundance values from when <code>classes</code> is a gene/protein. Default is <code>\"X\"</code>.</p> <code>'X'</code> <p>Returns:</p> Name Type Description <code>color_mapped</code> <code>array - like</code> <p>Values mapped to colors for plotting.</p> <code>cmap_resolved</code> <code>matplotlib colormap or None</code> <p>Colormap object for continuous coloring; None if categorical.</p> <code>legend_elements</code> <code>list or None</code> <p>Legend handles for categorical coloring; None if continuous.</p> Source code in <code>src/scpviz/plotting.py</code> <pre><code>def resolve_plot_colors(adata, classes, cmap, layer=\"X\"):\n    \"\"\"\n    Resolve colors for PCA or abundance plots.\n\n    This helper function determines how samples should be colored in plotting\n    functions based on categorical or continuous class values. It returns mapped\n    color values, a colormap (if applicable), and legend handles.\n\n    Args:\n        adata (anndata.AnnData): AnnData object (protein or peptide level).\n        classes (str): Class used for coloring. Can be:\n\n            - An `.obs` column name (categorical or continuous).\n\n            - A gene or protein identifier, in which case coloring is based\n              on abundance values from the specified `layer`.\n\n        cmap (str, list, or matplotlib colormap): Colormap to use.\n\n            - `\"default\"`: uses `get_color()` scheme.\n\n            - list of colors: categorical mapping.\n\n            - colormap name or object: continuous mapping.\n\n        layer (str): Data layer to extract abundance values from when `classes`\n            is a gene/protein. Default is `\"X\"`.\n\n    Returns:      \n        color_mapped (array-like): Values mapped to colors for plotting.\n        cmap_resolved (matplotlib colormap or None): Colormap object for continuous coloring; None if categorical.\n        legend_elements (list or None): Legend handles for categorical coloring; None if continuous.\n\n    \"\"\"\n\n    import matplotlib.cm as cm\n    import matplotlib.colors as mcolors\n    import matplotlib.patches as mpatches\n    import numpy as np\n\n    legend_elements = None\n\n    # Case 1: No coloring, all grey\n    if classes is None:\n        color_mapped = ['grey'] * len(adata)\n        legend_elements = [mpatches.Patch(color='grey', label='All samples')]\n        return color_mapped, None, legend_elements\n\n    # Case 2: Single categorical column from obs\n    elif isinstance(classes, str) and classes in adata.obs.columns:\n        y = utils.get_samplenames(adata, classes)\n        class_labels = sorted(set(y))\n        if cmap == 'default':\n            palette = get_color('colors', n=len(class_labels))\n            color_dict = {c: palette[i] for i, c in enumerate(class_labels)}\n        elif isinstance(cmap, list):\n            color_dict = {c: cmap[i] for i, c in enumerate(class_labels)}\n        elif isinstance(cmap, dict):\n            color_dict = cmap\n        else:\n            cmap_obj = cm.get_cmap(cmap)\n            palette = [mcolors.to_hex(cmap_obj(i / max(len(class_labels) - 1, 1))) for i in range(len(class_labels))]\n            color_dict = {c: palette[i] for i, c in enumerate(class_labels)}\n        color_mapped = [color_dict[val] for val in y]\n        legend_elements = [mpatches.Patch(color=color_dict[c], label=c) for c in class_labels]\n        return color_mapped, None, legend_elements\n\n    # Case 3: Multiple categorical columns from obs (combined class)\n    elif isinstance(classes, list) and all(c in adata.obs.columns for c in classes):\n        y = utils.get_samplenames(adata, classes)\n        class_labels = sorted(set(y))\n        if cmap == 'default':\n            palette = get_color('colors', n=len(class_labels))\n            color_dict = {c: palette[i] for i, c in enumerate(class_labels)}\n        elif isinstance(cmap, list):\n            color_dict = {c: cmap[i] for i, c in enumerate(class_labels)}\n        elif isinstance(cmap, dict):\n            color_dict = cmap\n        else:\n            cmap_obj = cm.get_cmap(cmap)\n            palette = [mcolors.to_hex(cmap_obj(i / max(len(class_labels) - 1, 1))) for i in range(len(class_labels))]\n            color_dict = {c: palette[i] for i, c in enumerate(class_labels)}\n        color_mapped = [color_dict[val] for val in y]\n        legend_elements = [mpatches.Patch(color=color_dict[c], label=c) for c in class_labels]\n        return color_mapped, None, legend_elements\n\n    # Case 4: Continuous coloring by protein abundance (accession)\n    elif isinstance(classes, str) and classes in adata.var_names:\n        X = adata.layers[layer] if layer in adata.layers else adata.X\n        if hasattr(X, \"toarray\"):\n            X = X.toarray()\n        idx = list(adata.var_names).index(classes)\n        color_mapped = X[:, idx]\n        if cmap == 'default':\n            cmap = 'viridis'\n        cmap = cm.get_cmap(cmap) if isinstance(cmap, str) else cmap\n\n        # Add default colorbar handling for abundance-based coloring\n        norm = mcolors.Normalize(vmin=color_mapped.min(), vmax=color_mapped.max())\n        sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n        sm.set_array([])  # required for colorbar\n\n        return color_mapped, cmap, None\n\n    # Case 5: Gene name (mapped to accession)\n    elif isinstance(classes, str):\n        if \"Genes\" in adata.var.columns:\n            gene_map = adata.var[\"Genes\"].to_dict()\n            match = [acc for acc, gene in gene_map.items() if gene == classes]\n            if match:\n                return resolve_plot_colors(adata, match[0], cmap, layer)\n        raise ValueError(\"Invalid classes input. Must be None, a protein in var_names, or an obs column/list.\")\n\n    else:\n        raise ValueError(\"Invalid classes input.\")\n</code></pre>"},{"location":"reference/utils/","title":"Utilities","text":"<p>Utility functions for scpviz.</p> <p>This module provides a collection of helper and processing functions used throughout the scpviz package. They fall into four main categories:</p>"},{"location":"reference/utils/#src.scpviz.utils--text-utility-functions","title":"Text Utility Functions","text":"<p>Functions:</p> Name Description <code>format_log_prefix</code> <p>Return standardized log prefixes for messages.</p> <code>format_class_filter</code> <p>Standardize class/value inputs for filtering.</p>"},{"location":"reference/utils/#src.scpviz.utils--data-processing-functions","title":"Data Processing Functions","text":"<p>Functions:</p> Name Description <code>get_samplenames</code> <p>Resolve sample names for given classes from <code>.obs</code>.</p> <code>get_classlist</code> <p>Return unique class values for specified <code>.obs</code> columns.</p> <code>get_adata_layer</code> <p>Safely extract data from <code>.X</code> or <code>.layers</code>.</p> <code>get_adata</code> <p>Retrieve <code>.prot</code> or <code>.pep</code> AnnData from pAnnData.</p> <code>get_abundance</code> <p>Wrapper to extract abundance data from pAnnData or AnnData.</p> <code>resolve_accessions</code> <p>Map gene names or accessions to <code>.var_names</code>.</p> <code>get_upset_contents</code> <p>Build contents for UpSet plots from pAnnData.</p> <code>get_upset_query</code> <p>Query features present/absent in UpSet contents.</p> <code>filter</code> <p>Legacy sample filtering (use <code>.filter_sample_values</code> instead).</p> <code>resolve_class_filter</code> <p>Resolve class/value pairs and apply filtering.</p> <code>get_pep_prot_mapping</code> <p>Determine peptide-to-protein mapping column.</p>"},{"location":"reference/utils/#src.scpviz.utils--api-functions","title":"API Functions","text":"<p>Functions:</p> Name Description <code>get_uniprot_fields_worker</code> <p>Low-level UniProt REST API query function (batch up to 1024).</p> <code>get_uniprot_fields</code> <p>High-level UniProt API wrapper with batching.</p>"},{"location":"reference/utils/#src.scpviz.utils--statistical-test-functions","title":"Statistical Test Functions","text":"<p>Functions:</p> Name Description <code>pairwise_log2fc</code> <p>Compute pairwise median log2 fold change between groups.</p> <code>get_pca_importance</code> <p>Identify most important features for PCA components.</p> <code>get_protein_clusters</code> <p>Retrieve hierarchical clusters from stored linkage.</p> <p>Warning</p> <p>Many of the functions in this module are internal helpers and not intended for direct end-user use. For filtering and abundance queries, prefer the corresponding <code>pAnnData</code> methods.</p> Example <p>To use this module, import it and call functions from your code as follows:     <pre><code>from scpviz import utils as scutils\n</code></pre></p> Todo <ul> <li>Add corrections for differential expression.</li> <li>Add more examples for each function.</li> </ul>"},{"location":"reference/utils/#src.scpviz.utils.convert_identifiers","title":"convert_identifiers","text":"<pre><code>convert_identifiers(ids, from_type: str, to_type, pdata=None, use_cache: bool = True, return_type: str = 'dict', verbose: bool = True)\n</code></pre> <p>Convert identifiers between UniProt-compatible types.</p> <p>Supports mapping between protein accessions, gene names, STRING IDs, and organism IDs. Multiple output types may be requested at once.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list of str</code> <p>Input identifiers.</p> required <code>from_type</code> <code>str</code> <p>Source identifier type ('accession', 'gene'). 'organism_id' cannot be used as a source.</p> required <code>to_type</code> <code>str or list of str</code> <p>Target identifier type(s). May include any of: ['gene', 'string', 'organism_id'].</p> required <code>pdata</code> <code>pAnnData</code> <p>pAnnData object providing cached accession\u2013gene mappings. If provided, <code>use_cache</code> is automatically set to True.</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use cached mappings from <code>pdata</code>. (default: True)</p> <code>True</code> <code>return_type</code> <code>str</code> <p>Output format: - 'dict': {input \u2192 {to_type \u2192 value}} - 'df': DataFrame with columns [from_type, *to_type] - 'both': (dict, DataFrame)</p> <code>'dict'</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress messages.</p> <code>True</code> <p>Returns:</p> Type Description <p>dict, pandas.DataFrame, or tuple: Depending on <code>return_type</code>.</p> Example <p>convert_identifiers([\"P12345\", \"Q9XYZ1\"], \"accession\", \"gene\", pdata=pdata) convert_identifiers([\"P12345\"], \"accession\", [\"gene\", \"string\", \"organism_id\"], return_type=\"df\")</p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def convert_identifiers(\n    ids,\n    from_type: str,\n    to_type,\n    pdata=None,\n    use_cache: bool = True,\n    return_type: str = \"dict\",\n    verbose: bool = True,\n):\n    \"\"\"\n    Convert identifiers between UniProt-compatible types.\n\n    Supports mapping between protein accessions, gene names, STRING IDs,\n    and organism IDs. Multiple output types may be requested at once.\n\n    Args:\n        ids (list of str): Input identifiers.\n        from_type (str): Source identifier type ('accession', 'gene').\n            'organism_id' cannot be used as a source.\n        to_type (str or list of str): Target identifier type(s).\n            May include any of: ['gene', 'string', 'organism_id'].\n        pdata (pAnnData, optional): pAnnData object providing cached\n            accession\u2013gene mappings. If provided, `use_cache` is\n            automatically set to True.\n        use_cache (bool): Whether to use cached mappings from `pdata`.\n            (default: True)\n        return_type (str): Output format:\n            - 'dict': {input \u2192 {to_type \u2192 value}}\n            - 'df': DataFrame with columns [from_type, *to_type]\n            - 'both': (dict, DataFrame)\n        verbose (bool): Whether to print progress messages.\n\n    Returns:\n        dict, pandas.DataFrame, or tuple: Depending on `return_type`.\n\n    Example:\n        &gt;&gt;&gt; convert_identifiers([\"P12345\", \"Q9XYZ1\"], \"accession\", \"gene\", pdata=pdata)\n        &gt;&gt;&gt; convert_identifiers([\"P12345\"], \"accession\", [\"gene\", \"string\", \"organism_id\"], return_type=\"df\")\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if not ids:\n        empty_df = pd.DataFrame(columns=[from_type] + ([to_type] if isinstance(to_type, str) else list(to_type)))\n        return {} if return_type != \"df\" else empty_df\n\n    if pdata is not None:\n        use_cache = True\n\n    from_col, to_cols, search_fields = _map_uniprot_field(from_type, to_type)\n    if isinstance(to_type, str):\n        to_type = [to_type]\n\n    # canonical UniProt field map (consistent with standardize_uniprot_columns)\n    _FIELD_MAP = {\n        \"accession\": \"accession\",\n        \"gene\": \"gene_primary\",\n        \"string\": \"xref_string\",\n        \"organism_id\": \"organism_id\",\n    }\n\n    # --- Logging\n    if verbose:\n        print(f\"{format_log_prefix('search', indent=1)} Converting from '{from_type}' to {to_type} for {len(ids)} identifiers...\")\n        if pdata is not None:\n            cacheable_types = {\"accession\", \"gene\"}\n            api_needed = [t for t in to_type if t not in cacheable_types]\n            if set([from_type] + to_type).issubset(cacheable_types):\n                print(f\"{format_log_prefix('info_only', indent=2)} Using cached mapping from pdata (no UniProt queries).\")\n            elif api_needed:\n                api_list = \", \".join(api_needed)\n                print(f\"{format_log_prefix('info_only', indent=2)} Using cached mapping for gene/accession; UniProt lookup required for: {api_list}.\")\n        else:\n            print(f\"{format_log_prefix('info_only', indent=2)} No pdata provided \u2014 querying UniProt for all target fields.\")\n\n    # --- Tier 1: cache lookup (only accession &lt;-&gt; gene)\n    resolved = {id_: {t: None for t in to_type} for id_ in ids}\n    to_query = list(ids)\n\n    if pdata is not None and use_cache and {\"accession\", \"gene\"}.issuperset({from_type, *to_type}):\n        if from_type == \"accession\" and \"gene\" in to_type:\n            _, acc_to_gene = pdata.get_identifier_maps(on=\"protein\")\n            for acc in ids:\n                if acc in acc_to_gene:\n                    resolved[acc][\"gene\"] = acc_to_gene[acc]\n        elif from_type == \"gene\" and \"accession\" in to_type:\n            gene_to_acc, _ = pdata.get_identifier_maps(on=\"protein\")\n            for gene in ids:\n                if gene in gene_to_acc:\n                    resolved[gene][\"accession\"] = gene_to_acc[gene]\n\n        # Filter unmapped\n        to_query = [x for x, v in resolved.items() if not any(vv for vv in v.values())]\n\n    # --- Tier 3: UniProt API\n    df = pd.DataFrame()\n    if len(to_query) &gt; 0:\n        # Hybrid case: gene \u2192 STRING / organism_id\n        if from_type == \"gene\":\n            gene_to_acc = convert_identifiers(to_query, \"gene\", \"accession\", pdata=pdata, use_cache=use_cache, verbose=False)\n            accs = [v.get(\"accession\") for v in gene_to_acc.values() if v.get(\"accession\")]\n            if accs:\n                df = get_uniprot_fields(accs, search_fields=search_fields, standardize=True)\n                df = standardize_uniprot_columns(df)\n                df = df.drop_duplicates(subset=\"accession\", keep=\"first\")\n\n                # Build per-target maps\n                per_target_maps = {}\n                for t in to_type:\n                    col = _FIELD_MAP[t]\n                    if col in df.columns:\n                        per_target_maps[t] = dict(zip(df[\"accession\"], df[col]))\n                    else:\n                        per_target_maps[t] = {}\n\n                # Assign results\n                for g, acc_dict in gene_to_acc.items():\n                    acc = acc_dict.get(\"accession\")\n                    for t in to_type:\n                        resolved[g][t] = per_target_maps[t].get(acc) if acc else None\n            else:\n                for g in to_query:\n                    for t in to_type:\n                        resolved[g][t] = None\n\n        else:\n            # Direct mapping (accession \u2192 X)\n            df = get_uniprot_fields(to_query, search_fields=search_fields, standardize=True)\n\n            # --- Clean up STRING results if present\n            if not df.empty:\n                if \"xref_string\" in df.columns and isinstance(df[\"xref_string\"], pd.Series):\n                    df[\"xref_string\"] = (\n                        df[\"xref_string\"]\n                        .astype(str)\n                        .apply(lambda s: s.replace(\";\", \"\").strip() if isinstance(s, str) else np.nan)\n                        .replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n                    )\n                elif \"string\" in to_type and verbose:\n                    print(f\"{format_log_prefix('warn_only', indent=3)} UniProt did not return 'xref_string' field \u2014 possible API schema drift.\")\n\n            if not df.empty and from_col in df.columns:\n                per_target_maps = {}\n                for t in to_type:\n                    col = _FIELD_MAP[t]\n                    if col in df.columns:\n                        per_target_maps[t] = dict(zip(df[from_col], df[col]))\n                    else:\n                        per_target_maps[t] = {}\n\n                for id_ in to_query:\n                    for t in to_type:\n                        resolved[id_][t] = per_target_maps[t].get(id_)\n            else:\n                for id_ in to_query:\n                    for t in to_type:\n                        resolved[id_][t] = None\n\n    # --- Reporting\n    resolved_count = sum(\n        any(vv is not None and not pd.isna(vv) for vv in v.values()) for v in resolved.values()\n    )\n    missing = [k for k, v in resolved.items() if all(vv is None or pd.isna(vv) for vv in v.values())]\n\n    if verbose:\n        local_resolved = len(ids) - len(to_query)\n        api_resolved = resolved_count - local_resolved\n        print(f\"{format_log_prefix('result_only', indent=2)} {resolved_count}/{len(ids)} identifiers successfully converted \"\n            f\"({local_resolved} local, {api_resolved} via UniProt).\")\n        if missing:\n            print(f\"{format_log_prefix('warn_only', indent=2)} {len(missing)} identifiers could not be resolved:\")\n            print(\"        \" + \", \".join(missing[:10]) + (\"...\" if len(missing) &gt; 10 else \"\"))\n\n    # --- Output\n    result_df = pd.DataFrame({from_type: list(resolved.keys())})\n    for t in to_type:\n        result_df[t] = [resolved[i][t] for i in result_df[from_type]]\n\n    if return_type == \"dict\":\n        return resolved\n    elif return_type == \"df\":\n        return result_df\n    elif return_type == \"both\":\n        return resolved, result_df\n    else:\n        raise ValueError(\"Invalid return_type. Choose from {'dict', 'df', 'both'}.\")\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.filter","title":"filter","text":"<pre><code>filter(pdata, class_type, values, exact_cases=False, debug=False)\n</code></pre> <p>Legacy-style filtering of samples in pAnnData or AnnData objects.</p> <p>This function filters samples based on metadata values using the older <code>(class_type, values)</code> interface. For pAnnData objects, it automatically delegates to <code>.filter_sample_values()</code> after converting the input into the recommended dictionary-style format.</p> <p>Warning</p> <p>For pAnnData users, prefer <code>.filter_sample_values()</code> with dictionary-style input, as it is more flexible and consistent. The <code>filter()</code> utility is retained primarily for backward compatibility and direct AnnData usage.</p> <p>Parameters:</p> Name Type Description Default <code>pdata</code> <code>pAnnData or AnnData</code> <p>Input data object to filter.</p> required <code>class_type</code> <code>str or list of str</code> <p>Metadata field(s) in <code>.obs</code> to filter on. Example: <code>\"treatment\"</code>, or <code>[\"cell_type\", \"treatment\"]</code>.</p> required <code>values</code> <code>list, dict, or list of dict</code> <p>Metadata values to match. - If <code>exact_cases=False</code>: Provide a dictionary or list-of-values per class. - If <code>exact_cases=True</code>: Provide a list of dictionaries specifying   exact combinations across fields.</p> required <code>exact_cases</code> <code>bool</code> <p>Whether to interpret <code>values</code> as exact combinations (AND logic). Defaults to False, which applies OR logic within each class type.</p> <code>False</code> <code>debug</code> <code>bool</code> <p>If True, print the query string used for filtering.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>filtered</code> <code>pAnnData or AnnData</code> <p>A filtered object of the same type as <code>pdata</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input types are invalid, if fields are missing in <code>.obs</code>, or if <code>values</code> format does not match <code>exact_cases</code>.</p> Example <p>Filter samples by a single metadata field:     <pre><code>samples = utils.filter(pdata, class_type=\"treatment\", values=\"kd\")\n</code></pre></p> <p>Filter by multiple fields with OR logic:      <pre><code>samples = utils.filter(\n        adata,\n        class_type=[\"cell_type\", \"treatment\"],\n        values=[[\"wt\", \"kd\"], [\"control\", \"treatment\"]]\n    ) \n# returns samples where cell_type is either 'wt' or 'kd' and treatment is either 'control' or 'treatment'\n</code></pre></p> <p>Filter by exact case combinations:     <pre><code>samples = utils.filter(\n        adata,\n        class_type=[\"cell_type\", \"treatment\"],\n        values=[{\"cell_type\": \"wt\", \"treatment\": \"control\"},\n                {\"cell_type\": \"kd\", \"treatment\": \"treatment\"}],\n        exact_cases=True\n    )\n# returns samples where cell_type is 'wt' and treatment is 'kd', or cell_type is 'control' and treatment is 'treatment'\n</code></pre></p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def filter(pdata, class_type, values, exact_cases = False, debug = False):\n    \"\"\"\n    Legacy-style filtering of samples in pAnnData or AnnData objects.\n\n    This function filters samples based on metadata values using the older\n    `(class_type, values)` interface. For pAnnData objects, it automatically\n    delegates to `.filter_sample_values()` after converting the input into the\n    recommended dictionary-style format.\n\n    !!! warning\n\n        For pAnnData users, prefer `.filter_sample_values()` with dictionary-style\n        input, as it is more flexible and consistent. The `filter()` utility is\n        retained primarily for backward compatibility and direct AnnData usage.\n\n\n    Args:\n        pdata (pAnnData or AnnData): Input data object to filter.\n        class_type (str or list of str): Metadata field(s) in `.obs` to filter on.\n            Example: `\"treatment\"`, or `[\"cell_type\", \"treatment\"]`.\n        values (list, dict, or list of dict): Metadata values to match.\n            - If `exact_cases=False`: Provide a dictionary or list-of-values per class.\n            - If `exact_cases=True`: Provide a list of dictionaries specifying\n              exact combinations across fields.\n        exact_cases (bool): Whether to interpret `values` as exact combinations (AND logic).\n            Defaults to False, which applies OR logic within each class type.\n        debug (bool): If True, print the query string used for filtering.\n\n    Returns:\n        filtered (pAnnData or AnnData): A filtered object of the same type as `pdata`.\n\n\n    Raises:\n        ValueError: If input types are invalid, if fields are missing in `.obs`,\n            or if `values` format does not match `exact_cases`.\n\n    Example:\n        Filter samples by a single metadata field:\n            ```python\n            samples = utils.filter(pdata, class_type=\"treatment\", values=\"kd\")\n            ```\n\n        Filter by multiple fields with OR logic: \n            ```python\n            samples = utils.filter(\n                    adata,\n                    class_type=[\"cell_type\", \"treatment\"],\n                    values=[[\"wt\", \"kd\"], [\"control\", \"treatment\"]]\n                ) \n            # returns samples where cell_type is either 'wt' or 'kd' and treatment is either 'control' or 'treatment'\n            ```\n\n        Filter by exact case combinations:\n            ```python \n            samples = utils.filter(\n                    adata,\n                    class_type=[\"cell_type\", \"treatment\"],\n                    values=[{\"cell_type\": \"wt\", \"treatment\": \"control\"},\n                            {\"cell_type\": \"kd\", \"treatment\": \"treatment\"}],\n                    exact_cases=True\n                )\n            # returns samples where cell_type is 'wt' and treatment is 'kd', or cell_type is 'control' and treatment is 'treatment'\n            ```\n    \"\"\"\n\n    if hasattr(pdata, \"filter_sample_values\"):\n        warnings.warn(\n            \"You passed a pAnnData object to `filter()`. \"\n            \"It is recommended to use `pdata.filter_sample_values()` directly.\",\n            UserWarning)\n\n        print(\"UserWarning: It is recommended to use the class method `.filter_sample_values()` with dictionary-style input for cleaner and more consistent filtering.\")\n\n    formatted_values = format_class_filter(class_type, values, exact_cases)\n\n    # pAnnData input\n    if hasattr(pdata, \"filter_sample_values\"):\n        return pdata.filter_sample_values(\n            values=formatted_values,\n            exact_cases=exact_cases,\n            debug=debug,\n            return_copy=True\n        )\n\n    # plain AnnData input\n    elif isinstance(pdata, ad.AnnData):\n        adata = pdata\n        obs_keys = adata.obs.columns\n\n        if exact_cases:\n            if not isinstance(formatted_values, list) or not all(isinstance(v, dict) for v in formatted_values):\n                raise ValueError(\"When exact_cases=True, `values` must be a list of dictionaries.\")\n\n            for case in formatted_values:\n                if not case:\n                    raise ValueError(\"Empty dictionary found in values.\")\n                for key in case:\n                    if key not in obs_keys:\n                        raise ValueError(f\"Field '{key}' not found in adata.obs.\")\n\n            query = \" | \".join([\n                \" &amp; \".join([\n                    f\"(adata.obs['{k}'] == '{v}')\" for k, v in case.items()\n                ])\n                for case in formatted_values\n            ])\n\n        else:\n            if not isinstance(formatted_values, dict):\n                raise ValueError(\"When exact_cases=False, `values` must be a dictionary.\")\n\n            for key in formatted_values:\n                if key not in obs_keys:\n                    raise ValueError(f\"Field '{key}' not found in adata.obs.\")\n\n            query_parts = []\n            for k, v in formatted_values.items():\n                v_list = v if isinstance(v, list) else [v]\n                part = \" | \".join([f\"(adata.obs['{k}'] == '{val}')\" for val in v_list])\n                query_parts.append(f\"({part})\")\n            query = \" &amp; \".join(query_parts)\n\n        if debug:\n            print(f\"Filter query: {query}\")\n\n        return adata[eval(query)]\n\n    else:\n        raise ValueError(\"Input must be a pAnnData or AnnData object.\")\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.format_class_filter","title":"format_class_filter","text":"<pre><code>format_class_filter(classes, class_value, exact_cases=False)\n</code></pre> <p>Convert legacy-style filter input into dictionary-style format.</p> <p>This function standardizes <code>(classes, class_value)</code> input into the dictionary format expected by <code>pAnnData.filter_sample_values()</code>. It supports both loose OR-style filtering and exact case matching across multiple metadata fields.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>str or list of str</code> <p>Metadata field(s) to filter on. Example: <code>\"treatment\"</code> or <code>[\"cellline\", \"treatment\"]</code>.</p> required <code>class_value</code> <code>str, list of str, or list of list</code> <p>Values to filter by. - str: May be underscore-joined (e.g. <code>\"kd_AS\"</code>). - list of str: Multiple values, interpreted as OR (if <code>exact_cases=False</code>)   or split into combinations (if <code>exact_cases=True</code>). - list of list: Each inner list defines a full set of values across classes.</p> required <code>exact_cases</code> <code>bool</code> <p>If True, return a list of dictionaries representing exact combinations across fields. If False, return a dictionary with OR logic applied.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>formatted</code> <code>dict or list of dict</code> <p>Dictionary-style filter input compatible</p> <p>with <code>.filter_sample_values()</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input shapes are inconsistent with the number of classes, or if <code>class_value</code> entries are not valid strings/lists.</p> Example <p>Single class with OR logic:     <pre><code>format_class_filter(\"treatment\", [\"kd\", \"sc\"])\n</code></pre> <pre><code>{'treatment': ['kd', 'sc']}\n</code></pre></p> <p>Multiple classes with loose matching:     <pre><code>format_class_filter([\"cellline\", \"treatment\"], [\"AS\", \"kd\"])\n</code></pre> <pre><code>{'cellline': 'AS', 'treatment': 'kd'}\n</code></pre></p> <p>Multiple classes with exact cases (underscore-joined strings):     <pre><code>format_class_filter(\n    [\"cellline\", \"treatment\"],\n    [\"AS_kd\", \"BE_sc\"],\n    exact_cases=True\n)\n</code></pre> <pre><code>[{'cellline': 'AS', 'treatment': 'kd'},\n {'cellline': 'BE', 'treatment': 'sc'}]\n</code></pre></p> <p>Multiple classes with exact cases (list of lists):     <pre><code>format_class_filter(\n    [\"cellline\", \"treatment\"],\n    [[\"AS\", \"kd\"], [\"BE\", \"sc\"]],\n    exact_cases=True\n)\n</code></pre> <pre><code># [{'cellline': 'AS', 'treatment': 'kd'},\n {'cellline': 'BE', 'treatment': 'sc'}]\n</code></pre></p> <p>Note</p> <p>This function is primarily used internally by <code>utils.filter()</code> and <code>pAnnData.filter_sample_values()</code>. End users should generally call <code>.filter_sample_values()</code> directly on <code>pAnnData</code> objects instead of using this helper.</p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def format_class_filter(classes, class_value, exact_cases=False):\n    \"\"\"\n    Convert legacy-style filter input into dictionary-style format.\n\n    This function standardizes `(classes, class_value)` input into the dictionary\n    format expected by `pAnnData.filter_sample_values()`. It supports both loose\n    OR-style filtering and exact case matching across multiple metadata fields.\n\n    Args:\n        classes (str or list of str): Metadata field(s) to filter on.\n            Example: `\"treatment\"` or `[\"cellline\", \"treatment\"]`.\n        class_value (str, list of str, or list of list): Values to filter by.\n            - str: May be underscore-joined (e.g. `\"kd_AS\"`).\n            - list of str: Multiple values, interpreted as OR (if `exact_cases=False`)\n              or split into combinations (if `exact_cases=True`).\n            - list of list: Each inner list defines a full set of values across classes.\n        exact_cases (bool): If True, return a list of dictionaries representing\n            exact combinations across fields. If False, return a dictionary with\n            OR logic applied.\n\n    Returns:\n        formatted (dict or list of dict): Dictionary-style filter input compatible\n        with `.filter_sample_values()`.\n\n    Raises:\n        ValueError: If input shapes are inconsistent with the number of classes,\n            or if `class_value` entries are not valid strings/lists.\n\n    Example:\n        Single class with OR logic:\n            ```python\n            format_class_filter(\"treatment\", [\"kd\", \"sc\"])\n            ```\n            ```\n            {'treatment': ['kd', 'sc']}\n            ```\n\n        Multiple classes with loose matching:\n            ```python\n            format_class_filter([\"cellline\", \"treatment\"], [\"AS\", \"kd\"])\n            ```\n            ```\n            {'cellline': 'AS', 'treatment': 'kd'}\n            ```\n\n        Multiple classes with exact cases (underscore-joined strings):\n            ```python\n            format_class_filter(\n                [\"cellline\", \"treatment\"],\n                [\"AS_kd\", \"BE_sc\"],\n                exact_cases=True\n            )\n            ```\n            ```\n            [{'cellline': 'AS', 'treatment': 'kd'},\n             {'cellline': 'BE', 'treatment': 'sc'}]\n            ```\n\n        Multiple classes with exact cases (list of lists):\n            ```python \n            format_class_filter(\n                [\"cellline\", \"treatment\"],\n                [[\"AS\", \"kd\"], [\"BE\", \"sc\"]],\n                exact_cases=True\n            )\n            ```\n            ```\n            # [{'cellline': 'AS', 'treatment': 'kd'},\n             {'cellline': 'BE', 'treatment': 'sc'}]\n            ```\n\n    !!! warning \"Note\"\n\n        This function is primarily used internally by `utils.filter()` and\n        `pAnnData.filter_sample_values()`. End users should generally call\n        `.filter_sample_values()` directly on `pAnnData` objects instead of\n        using this helper.\n    \"\"\"\n\n    if isinstance(classes, str):\n        # Simple case: one class\n        if isinstance(class_value, list) and exact_cases:\n            return [{classes: val} for val in class_value]\n        else:\n            return {classes: class_value}\n\n    elif isinstance(classes, list):\n        if exact_cases:\n            if isinstance(class_value, str):\n                class_value = [class_value]\n\n            formatted = []\n            for entry in class_value:\n                if isinstance(entry, str):\n                    values = entry.split('_')\n                elif isinstance(entry, list):\n                    values = entry\n                else:\n                    raise ValueError(\"Each class_value entry must be a string or a list.\")\n\n                if len(values) != len(classes):\n                    raise ValueError(\"Each class_value entry must match the number of classes.\")\n                formatted.append({cls: val for cls, val in zip(classes, values)})\n\n            return formatted\n\n        else:\n            # loose match \u2014 OR within each class\n            if isinstance(class_value, str):\n                values = class_value.split('_')\n            else:\n                values = class_value\n            if len(values) != len(classes):\n                raise ValueError(\"class_value must align with the number of classes.\")\n            return {cls: val for cls, val in zip(classes, values)}\n\n    else:\n        raise ValueError(\"Invalid input: `classes` should be a string or list of strings.\")\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.format_log_prefix","title":"format_log_prefix","text":"<pre><code>format_log_prefix(level: str, indent=None) -&gt; str\n</code></pre> <p>Return a standardized log prefix with emoji and label.</p> <p>This helper formats log message prefixes consistently across scpviz, with optional indentation for nested output. Used internally for user-facing messages, warnings, errors, and updates.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level keyword. Must be one of:</p> <ul> <li><code>\"user\"</code>: \ud83e\udded [USER]  </li> <li><code>\"search\"</code>: \ud83d\udd0d [SEARCH]  </li> <li><code>\"info\"</code>: \u2139\ufe0f [INFO]  </li> <li><code>\"result\"</code>: \u2705 [OK]  </li> <li><code>\"warn\"</code>: \u26a0\ufe0f [WARN]  </li> <li><code>\"error\"</code>: \u274c [ERROR]  </li> <li><code>\"info_only\"</code>: \u2139\ufe0f  </li> <li><code>\"filter_conditions\"</code>: \ud83d\udd38 (indented)  </li> <li><code>\"result_only\"</code>: \u2705  </li> <li><code>\"blank\"</code>: empty string  </li> <li><code>\"update\"</code>: \ud83d\udd04 [UPDATE]  </li> <li><code>\"api\"</code>: \ud83c\udf10 [API]</li> <li><code>\"update_only\"</code>: \ud83d\udd04  </li> <li><code>\"warn_only\"</code>: \u26a0\ufe0f</li> <li><code>\"user_only\"</code>: \ud83e\udded</li> </ul> required <code>indent</code> <code>int or None</code> <p>Indentation level override. Options:</p> <ul> <li><code>1</code>: no indent  </li> <li><code>2</code>: 5 spaces  </li> <li><code>3</code>: 10 spaces  </li> </ul> <p>If None, uses built-in default spacing (applied to most levels).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A formatted log prefix with emoji and label.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown <code>level</code> string is provided.</p> Example <p>Format an info prefix with default spacing: <pre><code>from scpviz.utils import format_log_prefix\nformat_log_prefix(\"info\")\n</code></pre></p> <pre><code>    \u2139\ufe0f [INFO]\n</code></pre> <p>Format a warning prefix with explicit indent: <pre><code>format_log_prefix(\"warn\", indent=3)\n</code></pre></p> <pre><code>        \u26a0\ufe0f [WARN]\n</code></pre> Source code in <code>src/scpviz/utils.py</code> <pre><code>def format_log_prefix(level: str, indent=None) -&gt; str:\n    \"\"\"\n    Return a standardized log prefix with emoji and label.\n\n    This helper formats log message prefixes consistently across scpviz,\n    with optional indentation for nested output. Used internally for\n    user-facing messages, warnings, errors, and updates.\n\n    Args:\n        level (str): Logging level keyword. Must be one of:\n\n            - `\"user\"`: \ud83e\udded [USER]  \n            - `\"search\"`: \ud83d\udd0d [SEARCH]  \n            - `\"info\"`: \u2139\ufe0f [INFO]  \n            - `\"result\"`: \u2705 [OK]  \n            - `\"warn\"`: \u26a0\ufe0f [WARN]  \n            - `\"error\"`: \u274c [ERROR]  \n            - `\"info_only\"`: \u2139\ufe0f  \n            - `\"filter_conditions\"`: \ud83d\udd38 (indented)  \n            - `\"result_only\"`: \u2705  \n            - `\"blank\"`: empty string  \n            - `\"update\"`: \ud83d\udd04 [UPDATE]  \n            - `\"api\"`: \ud83c\udf10 [API]\n            - `\"update_only\"`: \ud83d\udd04  \n            - `\"warn_only\"`: \u26a0\ufe0f\n            - `\"user_only\"`: \ud83e\udded\n\n        indent (int or None, optional): Indentation level override. Options:\n\n            - `1`: no indent  \n            - `2`: 5 spaces  \n            - `3`: 10 spaces  \n\n            If None, uses built-in default spacing (applied to most levels).\n\n    Returns:\n        str (str): A formatted log prefix with emoji and label.\n\n    Raises:\n        ValueError: If an unknown `level` string is provided.\n\n    Example:\n        Format an info prefix with default spacing:\n        ```python\n        from scpviz.utils import format_log_prefix\n        format_log_prefix(\"info\")\n        ```\n\n        ```\n            \u2139\ufe0f [INFO]\n        ```\n\n        Format a warning prefix with explicit indent:\n        ```python\n        format_log_prefix(\"warn\", indent=3)\n        ```\n\n        ```\n                \u26a0\ufe0f [WARN]\n        ```\n    \"\"\"\n    level = level.lower()\n    base_prefixes = {\n        \"user\": \"\ud83e\udded [USER]\",\n        \"search\": \"\ud83d\udd0d [SEARCH]\",\n        \"info\": \"\u2139\ufe0f [INFO]\",\n        \"result\": \"\u2705 [OK]\",\n        \"warn\": \"\u26a0\ufe0f [WARN]\",\n        \"error\": \"\u274c [ERROR]\",\n        \"info_only\": \"\u2139\ufe0f\",\n        \"filter_conditions\": \"     \ud83d\udd38 \",\n        \"result_only\": \"\u2705\",\n        \"blank\": \"\",\n        \"update\": \"\ud83d\udd04 [UPDATE]\",\n        \"api\": \"\ud83c\udf10 [API]\",\n        \"update_only\": \"\ud83d\udd04\",\n        \"warn_only\": \"\u26a0\ufe0f\",\n        \"user_only\": \"\ud83e\udded\"\n    }\n\n    if level not in base_prefixes:\n        raise ValueError(f\"Unknown log level: {level}\")\n\n    prefix = base_prefixes[level]\n\n    if indent is None:\n        # Use default built-in spacing for all except info_only\n        if level in [\"info\", \"search\", \"result\", \"warn\", \"error\"]:\n            return \"     \" + prefix\n        else:\n            return prefix  # Default case, no indent (e.g. info_only)\n    else:\n        # Explicit indent override\n        indent_spaces = {1: 0, 2: 5, 3: 10}\n        space = \" \" * indent_spaces.get(indent, 0)\n        return f\"{space}{prefix}\"\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_abundance","title":"get_abundance","text":"<pre><code>get_abundance(pdata, *args, **kwargs)\n</code></pre> <p>Wrapper to extract abundance from either pAnnData or AnnData.</p> <p>This is a convenience wrapper that dispatches to the appropriate method: - If <code>pdata</code> is a <code>pAnnData</code> object, it calls <code>pdata.get_abundance()</code>. - If <code>pdata</code> is an <code>AnnData</code> object, it falls back to the internal   helper <code>_get_abundance_from_adata</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pdata</code> <code>pAnnData or AnnData</code> <p>Input object to extract abundance from.</p> required <code>*args</code> <p>Positional arguments forwarded to <code>get_abundance</code>.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments forwarded to <code>get_abundance</code>.</p> <code>{}</code> Note <p>See <code>pAnnData.get_abundance</code> for full parameter documentation. Briefly,</p> <pre><code>- namelist (list of str, optional): List of accessions or gene names to extract.\n- layer (str): Data layer name (default = \"X\").\n- on (str): \"protein\" or \"peptide\".\n- classes (str or list of str, optional): Sample-level `.obs` column(s) to include.\n- log (bool): If True, applies log2 transform to abundance values.\n- x_label (str): Label features by \"gene\" or \"accession\".\n</code></pre> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Long-form abundance DataFrame, optionally with</p> <p>sample metadata and protein/peptide annotations.</p> See Also <ul> <li>:func:<code>pAnnData.get_abundance</code> (EditingMixin): Full-featured version with detailed docs.</li> <li>get_adata_layer: Helper to access abundance matrices from AnnData layers.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_abundance(pdata, *args, **kwargs):\n    \"\"\"\n    Wrapper to extract abundance from either pAnnData or AnnData.\n\n    This is a convenience wrapper that dispatches to the appropriate method:\n    - If `pdata` is a `pAnnData` object, it calls `pdata.get_abundance()`.\n    - If `pdata` is an `AnnData` object, it falls back to the internal\n      helper `_get_abundance_from_adata`.\n\n    Args:\n        pdata (pAnnData or anndata.AnnData): Input object to extract abundance from.\n        *args: Positional arguments forwarded to `get_abundance`.\n        **kwargs: Keyword arguments forwarded to `get_abundance`.\n\n    Note:\n        See `pAnnData.get_abundance` for full parameter documentation. Briefly,\n\n            - namelist (list of str, optional): List of accessions or gene names to extract.\n            - layer (str): Data layer name (default = \"X\").\n            - on (str): \"protein\" or \"peptide\".\n            - classes (str or list of str, optional): Sample-level `.obs` column(s) to include.\n            - log (bool): If True, applies log2 transform to abundance values.\n            - x_label (str): Label features by \"gene\" or \"accession\".\n\n    Returns:\n        df (pandas.DataFrame): Long-form abundance DataFrame, optionally with\n        sample metadata and protein/peptide annotations.\n\n    See Also:\n        - :func:`pAnnData.get_abundance` (EditingMixin): Full-featured version with detailed docs.\n        - get_adata_layer: Helper to access abundance matrices from AnnData layers.\n    \"\"\"\n    if hasattr(pdata, \"get_abundance\"):\n        return pdata.get_abundance(*args, **kwargs)\n    else:\n        # Fall back to internal logic (can move to a private function if cleaner)\n        return _get_abundance_from_adata(pdata, *args, **kwargs)\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_adata","title":"get_adata","text":"<pre><code>get_adata(pdata, on='protein')\n</code></pre> <p>Retrieve the protein- or peptide-level AnnData object from a pAnnData container.</p> <p>Parameters:</p> Name Type Description Default <code>pdata</code> <code>pAnnData</code> <p>The parent pAnnData object containing both protein- and peptide-level data.</p> required <code>on</code> <code>str</code> <p>Which data object to return. - <code>\"protein\"</code>: return <code>pdata.prot</code> - <code>\"peptide\"</code>: return <code>pdata.pep</code> </p> <code>'protein'</code> <p>Returns:</p> Name Type Description <code>adata</code> <code>AnnData</code> <p>The requested AnnData object.</p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_adata(pdata, on = 'protein'):\n    \"\"\"\n    Retrieve the protein- or peptide-level AnnData object from a pAnnData container.\n\n    Args:\n        pdata (pAnnData): The parent pAnnData object containing both protein- and peptide-level data.\n\n        on (str): Which data object to return.  \n            - `\"protein\"`: return `pdata.prot`  \n            - `\"peptide\"`: return `pdata.pep`  \n\n    Returns:\n        adata (anndata.AnnData): The requested AnnData object.\n    \"\"\"\n\n    if on in ('protein','prot'):\n        return pdata.prot\n    elif on in ('peptide','pep'):\n        return pdata.pep\n    else:\n        raise ValueError(\"Invalid value for 'on'. Options are 'protein' or 'peptide'.\")\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_adata_layer","title":"get_adata_layer","text":"<pre><code>get_adata_layer(adata, layer)\n</code></pre> <p>Safely extract layer data as dense numpy array.</p> <p>This helper returns the requested layer as a dense <code>numpy.ndarray</code>, ensuring compatibility for downstream operations. Supports both <code>.X</code> and <code>.layers[...]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>AnnData object containing data matrices.</p> required <code>layer</code> <code>str</code> <p>Layer key. - <code>\"X\"</code>: return the main data matrix. - any other str: return the corresponding entry from <code>.layers</code>. E.g. \"X_norm\"</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>ndarray</code> <p>Dense matrix representation of the requested layer.</p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_adata_layer(adata, layer):\n    \"\"\"\n    Safely extract layer data as dense numpy array.\n\n    This helper returns the requested layer as a dense `numpy.ndarray`,\n    ensuring compatibility for downstream operations. Supports both\n    `.X` and `.layers[...]`.\n\n    Args:\n        adata (anndata.AnnData): AnnData object containing data matrices.\n\n        layer (str): Layer key.  \n            - `\"X\"`: return the main data matrix.  \n            - any other str: return the corresponding entry from `.layers`. E.g. \"X_norm\"\n\n    Returns:\n        data (numpy.ndarray): Dense matrix representation of the requested layer.\n    \"\"\"\n    if layer == \"X\":\n        data = adata.X\n    elif layer in adata.layers:\n        data = adata.layers[layer]\n    else:\n        raise ValueError(f\"Layer '{layer}' not found in .layers and is not 'X'.\")\n\n    return data.toarray() if hasattr(data, 'toarray') else data\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_classlist","title":"get_classlist","text":"<pre><code>get_classlist(adata, classes=None, order=None)\n</code></pre> <p>Retrieve unique class values for specified metadata columns. Useful  for plot legends.</p> <p>Unlike <code>get_samplenames</code>, which returns one identifier per row/sample, this function extracts the set of unique class values for grouping purposes (e.g., plotting categories). Supports optional reordering.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>AnnData object containing sample metadata.</p> required <code>classes</code> <code>str or list of str</code> <p>Column(s) in <code>.obs</code> to use.</p> <ul> <li>None: combine all metadata columns up to the first <code>_quant</code> column.  </li> <li>str: return unique values from one column.  </li> <li>list of str: return unique combined values across multiple columns.  </li> </ul> <code>None</code> <code>order</code> <code>list of str</code> <p>Custom order of categories. Must exactly match the unique values; otherwise, a <code>ValueError</code> is raised.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>class_list</code> <code>list of str</code> <p>Unique class values in <code>.obs</code>, optionally reordered.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid columns are provided, or if <code>order</code> does not</p> Example <p>Get unique values from one metadata column:     <pre><code>classes = get_classlist(adata, classes=\"cell_type\")\n</code></pre></p> <p>Combine two columns and return unique class labels:     <pre><code>classes = get_classlist(adata, classes=[\"cell_type\", \"treatment\"])\n</code></pre></p> <p>Reorder categories explicitly:     <pre><code>classes = get_classlist(\n    adata, classes=\"cell_type\", order=[\"A\", \"B\", \"C\"]\n    )\n</code></pre></p> Related Functions <p>get_samplenames: Return per-sample names (not unique class values).</p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_classlist(adata, classes = None, order = None):\n    \"\"\"\n    Retrieve unique class values for specified metadata columns. Useful \n    for plot legends.\n\n    Unlike `get_samplenames`, which returns one identifier per row/sample,\n    this function extracts the set of unique class values for grouping\n    purposes (e.g., plotting categories). Supports optional reordering.\n\n    Args:\n        adata (anndata.AnnData): AnnData object containing sample metadata.\n\n        classes (str or list of str, optional): Column(s) in `.obs` to use.\n\n            - None: combine all metadata columns up to the first `_quant` column.  \n            - str: return unique values from one column.  \n            - list of str: return unique combined values across multiple columns.  \n\n        order (list of str, optional): Custom order of categories. Must exactly\n            match the unique values; otherwise, a `ValueError` is raised.\n\n    Returns:\n        class_list (list of str): Unique class values in `.obs`, optionally reordered.\n\n    Raises:\n        ValueError: If invalid columns are provided, or if `order` does not\n        match the unique class list.\n\n    Example:\n        Get unique values from one metadata column:\n            ```python\n            classes = get_classlist(adata, classes=\"cell_type\")\n            ```\n\n        Combine two columns and return unique class labels:\n            ```python\n            classes = get_classlist(adata, classes=[\"cell_type\", \"treatment\"])\n            ```\n\n        Reorder categories explicitly:\n            ```python\n            classes = get_classlist(\n                adata, classes=\"cell_type\", order=[\"A\", \"B\", \"C\"]\n                )\n            ```\n\n    Related Functions:\n        get_samplenames: Return per-sample names (not unique class values).\n    \"\"\"\n\n    if classes is None:\n        # combine all .obs columns per row into one string\n        # NOTE: might break, should use better method to filter out file-related columns\n        quant_col_index = adata.obs.columns.get_loc(next(col for col in adata.obs.columns if \"_quant\" in col))\n        selected_columns = adata.obs.iloc[:, :quant_col_index]\n        classes_list = selected_columns.apply(lambda x: '_'.join(x), axis=1).unique()\n        classes = selected_columns.columns.tolist()\n    elif isinstance(classes, str):\n        # check if classes is one of the columns of adata.obs\n        if classes not in adata.obs.columns:\n            raise ValueError(f\"Invalid value for 'classes'. '{classes}' is not a column in adata.obs.\")\n        classes_list = adata.obs[classes].unique()\n    elif isinstance(classes, list):\n        # check if list has length 1\n        if len(classes) == 1:\n            classes_list = adata.obs[classes[0]].unique()\n        # check if all classes are columns of adata.obs\n        else:\n            if not all([c in adata.obs.columns for c in classes]):\n                raise ValueError(f\"Invalid value for 'classes'. Not all elements in '{classes}' are columns in adata.obs.\")\n            classes_list = adata.obs[classes].apply(lambda x: '_'.join(x), axis=1).unique()\n    else:\n        raise ValueError(\"Invalid value for 'classes'. Must be None, a string or a list of strings.\")\n\n    if isinstance(classes_list, str):\n        classes_list = [classes_list]\n    if isinstance(order, str):\n        order = [order]\n\n    if order is not None:\n        # check if order list matches classes_list\n        missing_elements = set(classes_list) - set(order)\n        extra_elements = set(order) - set(classes_list)\n        # Print missing and extra elements if any\n        if missing_elements or extra_elements:\n            if missing_elements:\n                print(f\"Missing elements in 'order': {missing_elements}\")\n            if extra_elements:\n                print(f\"Extra elements in 'order': {extra_elements}\")\n            raise ValueError(\"The 'order' list does not match 'classes_list'.\")\n        # if they match, then reorder classes_list to match order\n        classes_list = order\n\n    return classes_list\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_pca_importance","title":"get_pca_importance","text":"<pre><code>get_pca_importance(model: Union[dict, PCA], initial_feature_names: List[str], n: int = 1) -&gt; pd.DataFrame\n</code></pre> <p>Identify the most important features for each principal component.</p> <p>This function ranks features by their absolute PCA loading values and extracts the top contributors for each principal component.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PCA or dict</code> <p>Either a fitted PCA model from scikit-learn, or a dictionary with key <code>\"PCs\"</code> (array-like, shape: <code>(n_components, n_features)</code>).</p> required <code>initial_feature_names</code> <code>list of str</code> <p>Names of the features, typically <code>adata.var_names</code>.</p> required <code>n</code> <code>int</code> <p>Number of top features to return per principal component (default = 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>DataFrame with one row per principal component,</p> <code>DataFrame</code> <p>listing the top contributing features.</p> Example <p>Retrieve the top 5 features contributing to each PC:     <pre><code>from scpviz import utils as scutils\npdata.pca(n_components=5)\ndf = scutils.get_pca_importance(\n    pdata.prot.uns['pca'],\n    pdata.prot.var_names,\n    n=5\n)\n</code></pre></p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_pca_importance(model: Union[dict, 'sklearn.decomposition.PCA'], initial_feature_names: List[str], n: int = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Identify the most important features for each principal component.\n\n    This function ranks features by their absolute PCA loading values and\n    extracts the top contributors for each principal component.\n\n    Args:\n        model (sklearn.decomposition.PCA or dict): Either a fitted PCA model\n            from scikit-learn, or a dictionary with key `\"PCs\"`\n            (array-like, shape: `(n_components, n_features)`).\n        initial_feature_names (list of str): Names of the features, typically\n            `adata.var_names`.\n        n (int): Number of top features to return per principal component\n            (default = 1).\n\n    Returns:\n        df (pandas.DataFrame): DataFrame with one row per principal component,\n        listing the top contributing features.\n\n    Example:\n        Retrieve the top 5 features contributing to each PC:\n            ```python\n            from scpviz import utils as scutils\n            pdata.pca(n_components=5)\n            df = scutils.get_pca_importance(\n                pdata.prot.uns['pca'],\n                pdata.prot.var_names,\n                n=5\n            )\n            ```\n    \"\"\"\n\n    if isinstance(model, dict):\n        pcs = np.asarray(model[\"PCs\"])  # shape: n_components x n_features\n    else:\n        pcs = np.asarray(model.components_)  # shape: n_components x n_features\n\n    n_pcs = pcs.shape[0]\n\n    most_important = [\n        np.abs(pcs[i]).argsort()[-n:][::-1] for i in range(n_pcs)\n    ]\n    most_important_names = [\n        [initial_feature_names[idx] for idx in row] for row in most_important\n    ]\n\n    result = {\n        f\"PC{i + 1}\": most_important_names[i] for i in range(n_pcs)\n    }\n    df = pd.DataFrame(result.items(), columns=[\"Principal Component\", \"Top Features\"])\n    return df\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_pep_prot_mapping","title":"get_pep_prot_mapping","text":"<pre><code>get_pep_prot_mapping(pdata, return_series=False)\n</code></pre> <p>Retrieve the peptide-to-protein mapping column or mapping values.</p> <p>This function resolves the appropriate <code>.pep.var</code> column for peptide-to-protein mapping based on the data source recorded in <code>pdata.metadata[\"source\"]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pdata</code> <code>pAnnData</code> <p>The annotated proteomics object containing <code>.metadata</code> and <code>.pep</code>.</p> required <code>return_series</code> <code>bool</code> <p>If True, return a pandas Series of peptide-to-protein mappings. If False (default), return the column name as a string.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>col</code> <code>str</code> <p>Column name in <code>.pep.var</code> containing peptide-to-protein mapping,</p> <p>if <code>return_series=False</code>.</p> <code>mapping</code> <code>Series</code> <p>Series mapping peptides to proteins,</p> <p>if <code>return_series=True</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data source is unrecognized or no valid mapping column is found.</p> Note <p>The mapping column depends on the import source:</p> <ul> <li>Proteome Discoverer \u2192 <code>\"Master Protein Accessions\"</code></li> <li>DIA-NN \u2192 <code>\"Protein.Group\"</code></li> <li>MaxQuant \u2192 <code>\"Leading razor protein\"</code></li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_pep_prot_mapping(pdata, return_series=False):\n    \"\"\"\n    Retrieve the peptide-to-protein mapping column or mapping values.\n\n    This function resolves the appropriate `.pep.var` column for peptide-to-protein\n    mapping based on the data source recorded in `pdata.metadata[\"source\"]`.\n\n    Args:\n        pdata (pAnnData): The annotated proteomics object containing `.metadata` and `.pep`.\n        return_series (bool): If True, return a pandas Series of peptide-to-protein\n            mappings. If False (default), return the column name as a string.\n\n    Returns:\n        col (str): Column name in `.pep.var` containing peptide-to-protein mapping,\n        if `return_series=False`.\n        mapping (pandas.Series): Series mapping peptides to proteins,\n        if `return_series=True`.\n\n    Raises:\n        ValueError: If the data source is unrecognized or no valid mapping column is found.\n\n    Note:\n        The mapping column depends on the import source:\n\n        - Proteome Discoverer \u2192 `\"Master Protein Accessions\"`\n        - DIA-NN \u2192 `\"Protein.Group\"`\n        - MaxQuant \u2192 `\"Leading razor protein\"`\n    \"\"\"\n    source = pdata.metadata.get(\"source\", \"\").lower()\n\n    if source == \"proteomediscoverer\":\n        col = \"Master Protein Accessions\"\n    elif source == \"diann\":\n        col = \"Protein.Group\"\n    elif source == \"maxquant\":\n        col = \"Leading razor protein\"\n    else:\n        raise ValueError(f\"Unknown data source '{source}' \u2014 cannot determine peptide-to-protein mapping.\")\n\n    if return_series:\n        return pdata.pep.var[col]\n\n    return col\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_protein_clusters","title":"get_protein_clusters","text":"<pre><code>get_protein_clusters(pdata, on='prot', layer='X', t=5, criterion='maxclust')\n</code></pre> <p>Retrieve hierarchical clusters of proteins from stored linkage.</p> <p>This function uses linkage information stored in <code>pdata.stats</code> to partition proteins into clusters.</p> <p>Parameters:</p> Name Type Description Default <code>pdata</code> <code>pAnnData</code> <p>Input object containing <code>.stats</code> with clustering results.</p> required <code>on</code> <code>str</code> <p>Data level to use, <code>\"prot\"</code> (default) or <code>\"pep\"</code>.</p> <code>'prot'</code> <code>layer</code> <code>str</code> <p>Data layer name used when the linkage was computed (default = <code>\"X\"</code>).</p> <code>'X'</code> <code>t</code> <code>int or float</code> <p>Number of clusters (if <code>criterion=\"maxclust\"</code>) or distance threshold for clustering.</p> <code>5</code> <code>criterion</code> <code>str</code> <p>Clustering criterion passed to <code>scipy.cluster.hierarchy.fcluster</code>, e.g. <code>\"maxclust\"</code> or <code>\"distance\"</code>.</p> <code>'maxclust'</code> <p>Returns:</p> Name Type Description <code>clusters</code> <code>dict</code> <p>Mapping of <code>cluster_id \u2192 list of proteins</code>.</p> <code>None</code> <p>If no linkage is found in <code>pdata.stats</code>.</p> Note <p>Requires that a clustermap has been previously computed and linkage stored under <code>pdata.stats[f\"{on}_{layer}_clustermap\"]</code>.</p> Related Functions <ul> <li>plot_clustermap: Generates clustered heatmaps and stores linkage.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_protein_clusters(pdata, on='prot', layer='X', t=5, criterion='maxclust'):\n    \"\"\"\n    Retrieve hierarchical clusters of proteins from stored linkage.\n\n    This function uses linkage information stored in `pdata.stats` to\n    partition proteins into clusters.\n\n    Args:\n        pdata (pAnnData): Input object containing `.stats` with clustering results.\n        on (str): Data level to use, `\"prot\"` (default) or `\"pep\"`.\n        layer (str): Data layer name used when the linkage was computed (default = `\"X\"`).\n        t (int or float): Number of clusters (if `criterion=\"maxclust\"`) or distance\n            threshold for clustering.\n        criterion (str): Clustering criterion passed to `scipy.cluster.hierarchy.fcluster`,\n            e.g. `\"maxclust\"` or `\"distance\"`.\n\n    Returns:\n        clusters (dict): Mapping of `cluster_id \u2192 list of proteins`.\n        None: If no linkage is found in `pdata.stats`.\n\n    Note:\n        Requires that a clustermap has been previously computed and linkage\n        stored under `pdata.stats[f\"{on}_{layer}_clustermap\"]`.\n\n    Related Functions:\n        - plot_clustermap: Generates clustered heatmaps and stores linkage.\n    \"\"\"\n    from scipy.cluster.hierarchy import fcluster\n\n    key = f\"{on}_{layer}_clustermap\"\n    stats = pdata.stats.get(key)\n    if not stats or \"row_linkage\" not in stats:\n        print(f\"No linkage found for {key} in pdata.stats.\")\n        return None\n\n    linkage = stats[\"row_linkage\"]\n    labels = fcluster(linkage, t=t, criterion=criterion)\n    order = stats[\"row_order\"]\n\n    from collections import defaultdict\n    clusters = defaultdict(list)\n    for label, prot in zip(labels, order):\n        clusters[label].append(prot)\n\n    return dict(clusters)\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_samplenames","title":"get_samplenames","text":"<pre><code>get_samplenames(adata, classes)\n</code></pre> <p>Retrieve sample names for specified class values.</p> <p>This function resolves <code>.obs</code> metadata into sample-level identifiers (one name per row). It is typically used for plotting functions where sample names are required for labeling or grouping.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData</code> <p>AnnData object containing sample metadata.</p> required <code>classes</code> <code>str or list of str</code> <p>Column(s) in <code>.obs</code> used to build sample names.</p> <ul> <li>str: return vlaues from a single column.</li> <li>list of str: combine multiple columns per row with <code>\", \"</code>.</li> </ul> required <p>Returns:</p> Name Type Description <code>sample_names</code> <code>list of str</code> <p>Sample names dervied from <code>.obs</code>.</p> Example <p>Get sample names from a single metadata column:     <pre><code>samples = get_samplenames(adata, \"cell_type\")\n</code></pre></p> <p>Combine multiple columns into sample identifiers:     <pre><code>samples = get_samplenames(adata, [\"cell_type\", \"treatment\"])\n</code></pre></p> Related Functions <p>get_classlist: Return unique class values (not per-sample names).</p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_samplenames(adata, classes):\n    \"\"\"\n    Retrieve sample names for specified class values.\n\n    This function resolves `.obs` metadata into sample-level identifiers\n    (one name per row). It is typically used for plotting functions where\n    sample names are required for labeling or grouping.\n\n    Args:\n        adata (anndata.AnnData): AnnData object containing sample metadata.\n\n        classes (str or list of str): Column(s) in `.obs` used to build sample names.\n\n            - str: return vlaues from a single column.\n            - list of str: combine multiple columns per row with `\", \"`.\n\n    Returns:\n        sample_names (list of str): Sample names dervied from `.obs`.\n\n    Example:\n        Get sample names from a single metadata column:\n            ```python\n            samples = get_samplenames(adata, \"cell_type\")\n            ```\n\n        Combine multiple columns into sample identifiers:\n            ```python\n            samples = get_samplenames(adata, [\"cell_type\", \"treatment\"])\n            ```\n\n    Related Functions:\n        get_classlist: Return unique class values (not per-sample names).\n    \"\"\"\n    if classes is None:\n        return None\n    elif isinstance(classes, str):\n        return adata.obs[classes].values.tolist()\n    elif isinstance(classes, list):\n        return adata.obs[classes].apply(lambda row: ', '.join(row.values.astype(str)), axis=1).values.tolist()\n    else:\n        raise ValueError(\"Invalid input for 'classes'. It should be None, a string, or a list of strings.\")\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_uniprot_fields","title":"get_uniprot_fields","text":"<pre><code>get_uniprot_fields(prot_list, search_fields=['accession', 'id', 'protein_name', 'gene_primary', 'gene_names', 'organism_id', 'go', 'go_f', 'go_c', 'go_p', 'cc_interaction', 'xref_string'], batch_size=100, verbose=True, standardize=True, worker_verbose=False)\n</code></pre> <p>Retrieve UniProt metadata for a list of protein accessions.</p> <p>This function wraps <code>get_uniprot_fields_worker</code> to handle batching of protein IDs, returning results as a single DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>prot_list</code> <code>list of str</code> <p>List of protein accessions.</p> required <code>search_fields</code> <code>list of str</code> <p>UniProt fields to return. Defaults include accession, gene names, GO terms, and STRING IDs.</p> <code>['accession', 'id', 'protein_name', 'gene_primary', 'gene_names', 'organism_id', 'go', 'go_f', 'go_c', 'go_p', 'cc_interaction', 'xref_string']</code> <code>batch_size</code> <code>int</code> <p>Number of accessions per batch (max 1024, default=100).</p> <code>100</code> <code>verbose</code> <code>bool</code> <p>If True, print progress messages.</p> <code>True</code> <code>standardize</code> <code>bool</code> <p>If True (default), normalize UniProt column names to canonical lowercase keys (e.g., \"gene_primary\", \"organism_id\", \"xref_string\") for consistent downstream processing.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>DataFrame containing UniProt metadata for the input proteins.</p> Example <p>Query UniProt for a small set of proteins:     <pre><code>proteins = [\"P40925\", \"P40926\"]\ndf = get_uniprot_fields(proteins)\ndf[[\"Entry\", \"Gene Names\", \"Organism Id\"]].head()\n</code></pre></p> <p>Retrieve raw UniProt field names without renaming:     &gt;&gt;&gt; df_raw = get_uniprot_fields(proteins, standardize=False)</p> Related Functions <ul> <li>get_uniprot_fields_worker: Worker function that handles low-level UniProt API queries.</li> <li>standardize_uniprot_columns: Helper used internally for column normalization.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_uniprot_fields(\n    prot_list,\n    search_fields=['accession', 'id', 'protein_name', 'gene_primary', 'gene_names',\n                   'organism_id', 'go', 'go_f', 'go_c', 'go_p',\n                   'cc_interaction', 'xref_string'],\n    batch_size=100,\n    verbose=True,\n    standardize=True,\n    worker_verbose=False,\n):\n    \"\"\"\n    Retrieve UniProt metadata for a list of protein accessions.\n\n    This function wraps `get_uniprot_fields_worker` to handle batching of\n    protein IDs, returning results as a single DataFrame.\n\n    Args:\n        prot_list (list of str): List of protein accessions.\n        search_fields (list of str): UniProt fields to return.\n            Defaults include accession, gene names, GO terms, and STRING IDs.\n        batch_size (int): Number of accessions per batch (max 1024, default=100).\n        verbose (bool): If True, print progress messages.\n        standardize (bool): If True (default), normalize UniProt column names\n            to canonical lowercase keys (e.g., \"gene_primary\", \"organism_id\",\n            \"xref_string\") for consistent downstream processing.\n\n    Returns:\n        df (pandas.DataFrame): DataFrame containing UniProt metadata for the input proteins.\n\n    Example:\n        Query UniProt for a small set of proteins:\n            ```python\n            proteins = [\"P40925\", \"P40926\"]\n            df = get_uniprot_fields(proteins)\n            df[[\"Entry\", \"Gene Names\", \"Organism Id\"]].head()\n            ```\n\n        Retrieve raw UniProt field names without renaming:\n            &gt;&gt;&gt; df_raw = get_uniprot_fields(proteins, standardize=False)\n\n    Related Functions:\n        - get_uniprot_fields_worker: Worker function that handles low-level UniProt API queries.\n        - standardize_uniprot_columns: Helper used internally for column normalization.\n    \"\"\"\n\n    # --- Ensure 'accession' field comes first (UniProt requirement)\n    search_fields = [\"accession\"] + [f for f in search_fields if f != \"accession\"]\n\n    # --- Split IDs into batches\n    batches = [prot_list[i:i + batch_size] for i in range(0, len(prot_list), batch_size)]\n    all_results = []\n\n    for i, batch in enumerate(batches, start=1):\n        if verbose:\n            print(\n                f\"{format_log_prefix('api', indent=2)} Querying UniProt for batch {i}/{len(batches)} \"\n                f\"({len(batch)} proteins) [fields: {', '.join(search_fields)}]\"\n            )\n\n            if len(batches) &gt; 1:\n                print(f\"{format_log_prefix('info_only', indent=3)} Processing batch {i}/{len(batches)}...\")\n\n        try:\n            batch_df = get_uniprot_fields_worker(batch, search_fields, verbose=worker_verbose)\n            if standardize:\n                batch_df = standardize_uniprot_columns(batch_df)\n            all_results.append(batch_df)\n        except Exception as e:\n            print(f\"{format_log_prefix('warn')} Failed batch {i}: {e}\")\n            continue\n\n    if not all_results:\n        if verbose:\n            print(f\"{format_log_prefix('warn')} No results retrieved from UniProt.\")\n        return pd.DataFrame()\n\n    full_method_df = pd.concat(all_results, ignore_index=True)\n    if verbose:\n        print(f\"{format_log_prefix('result_only', 2)} Retrieved UniProt metadata for {len(full_method_df)} entries.\")\n\n    return full_method_df\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_uniprot_fields_worker","title":"get_uniprot_fields_worker","text":"<pre><code>get_uniprot_fields_worker(prot_list, search_fields=None, verbose=False)\n</code></pre> <p>Query UniProt for a batch of protein accessions.</p> <p>This function sends requests to the UniProt REST API for up to 1024 proteins at a time and returns the requested fields as a DataFrame. It handles isoform accessions, fallback queries, and UniProt ID redirects automatically.</p> <p>Parameters:</p> Name Type Description Default <code>prot_list</code> <code>list of str</code> <p>List of protein accessions or IDs.</p> required <code>search_fields</code> <code>list of str</code> <p>UniProt return fields. See: https://www.uniprot.org/help/return_fields</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print progress messages and missing accessions.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>DataFrame containing UniProt metadata for the input proteins.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>query_type</code> is unknown or the data source cannot be resolved.</p> <p>Info</p> <ul> <li>This function is intended as a worker and is usually called by   <code>get_uniprot_fields</code>.</li> <li>It automatically resolves canonical vs. isoform accessions and will   attempt UniProt ID mapping if some accessions cannot be found.</li> </ul> Related Functions <ul> <li>get_uniprot_fields: High-level batch UniProt query wrapper.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_uniprot_fields_worker(prot_list, search_fields=None, verbose = False):\n    \"\"\"\n    Query UniProt for a batch of protein accessions.\n\n    This function sends requests to the UniProt REST API for up to 1024 proteins\n    at a time and returns the requested fields as a DataFrame. It handles isoform\n    accessions, fallback queries, and UniProt ID redirects automatically.\n\n    Args:\n        prot_list (list of str): List of protein accessions or IDs.\n        search_fields (list of str): UniProt return fields.\n            See: https://www.uniprot.org/help/return_fields\n        verbose (bool): If True, print progress messages and missing accessions.\n\n    Returns:\n        df (pandas.DataFrame): DataFrame containing UniProt metadata for the input proteins.\n\n    Raises:\n        ValueError: If `query_type` is unknown or the data source cannot be resolved.\n\n    !!! info\n        - This function is intended as a **worker** and is usually called by\n          `get_uniprot_fields`.\n        - It automatically resolves canonical vs. isoform accessions and will\n          attempt UniProt ID mapping if some accessions cannot be found.\n\n    Related Functions:\n        - get_uniprot_fields: High-level batch UniProt query wrapper.\n    \"\"\"\n\n    base_url = 'https://rest.uniprot.org/uniprotkb/stream'\n    fields = \"%2C\".join(search_fields)\n    format_type = 'tsv'\n\n    def query_uniprot_batch(ids, query_type=\"accession\"):\n        if not ids:\n            return pd.DataFrame()\n\n        if query_type == \"accession\":\n            query_parts = [f\"%28accession%3A{id}%29\" for id in ids]\n        elif query_type == \"id\":\n            query_parts = [f\"%28id%3A{id}%29\" for id in ids]\n        else:\n            raise ValueError(f\"Unknown query_type: {query_type}\")\n\n        query = \"+OR+\".join(query_parts)\n        full_query = f\"%28{query}%29\"\n        url = f'{base_url}?fields={fields}&amp;format={format_type}&amp;query={full_query}'\n\n        if verbose:\n            print(f\"Querying UniProt ({query_type}, TSV mode) for {len(ids)} proteins\")\n\n        results = requests.get(url)\n        results.raise_for_status()\n\n        # Handle empty response gracefully\n        if not results.text.strip():\n            print(f\"{format_log_prefix('warn_only', 2)} UniProt returned empty response for {len(ids)} proteins.\")\n            return pd.DataFrame()\n\n        return pd.read_csv(io.StringIO(results.text), sep=\"\\t\")\n\n    if verbose:\n        print(f\"{format_log_prefix('API', 1)} Querying UniProt for {len(prot_list)} total proteins [TSV mode].\")\n\n    def resolve_uniprot_redirects(accessions, from_db='UniProtKB_AC-ID', to_db='UniProtKB'):\n        url = 'https://rest.uniprot.org/idmapping/run'\n        data = {'from': from_db, 'to': to_db, 'ids': ','.join(accessions)}\n\n        res = requests.post(url, data=data)\n        res.raise_for_status()\n        job_id = res.json()['jobId']\n\n        # Poll until job is complete\n        while True:\n            status = requests.get(f\"https://rest.uniprot.org/idmapping/status/{job_id}\").json()\n            if status.get(\"jobStatus\") == \"RUNNING\":\n                time.sleep(1)\n            else:\n                break\n\n        # Get results\n        results = requests.get(f\"https://rest.uniprot.org/idmapping/uniprotkb/results/{job_id}\").json()\n        mapping = {item['from']: item['to']['primaryAccession'] for item in results.get('results', [])}\n        return mapping\n\n    # Split isoform vs canonical accessions\n    isoform_ids = [acc for acc in prot_list if '-' in acc]\n    canonical_ids = [acc for acc in prot_list if '-' not in acc]\n\n    df_canonical = query_uniprot_batch(canonical_ids, query_type=\"accession\")\n    df_isoform = query_uniprot_batch(isoform_ids, query_type=\"accession\")\n\n    # Identify any isoforms that weren't found\n    found_isoform_ids = set(df_isoform['Entry']) if not df_isoform.empty else set()\n    missing_isoforms = [acc for acc in isoform_ids if acc not in found_isoform_ids]\n\n    if missing_isoforms and verbose:\n        print(f\"{format_log_prefix('info_only', 3)} Attempting fallback query for {len(missing_isoforms)} isoform base IDs\")\n\n    # Attempt fallback query using base accessions\n    fallback_ids = list(set([id.split('-')[0] for id in missing_isoforms]))\n    df_fallback = query_uniprot_batch(fallback_ids, query_type=\"id\")\n\n    # Combine all DataFrames\n    df = pd.concat([df_canonical, df_isoform, df_fallback], ignore_index=True)\n\n    # Final pass: insert missing rows if still unresolved\n    found_entries = set(df['Entry']) if 'Entry' in df.columns else set()\n    still_missing = set(prot_list) - found_entries\n\n    if still_missing:\n        if verbose:\n            print(f\"{format_log_prefix('info_only', 3)} Attempting UniProt ID redirect for {len(still_missing)} unresolved accessions.\")\n        redirect_map = resolve_uniprot_redirects(list(still_missing))\n        if redirect_map:\n            redirected_ids = list(redirect_map.values())\n            df_redirected = query_uniprot_batch(redirected_ids, query_type=\"accession\")\n\n            # Remap back to original accession\n            inv_map = {v: k for k, v in redirect_map.items()}\n            if 'Entry' in df_redirected.columns:\n                df_redirected['Entry'] = df_redirected['Entry'].apply(lambda x: inv_map.get(x, x))\n\n            df = pd.concat([df, df_redirected], ignore_index=True)\n\n            resolved = set(redirect_map.keys())\n            still_missing -= resolved\n\n    # Step 5: Fill in placeholders for totally missing accessions\n    if still_missing:\n        print(f\"{format_log_prefix('warn_only', 3)} Proteins not found in UniProt: {list(still_missing)[:5]}\") if verbose else None\n        missing_df = pd.DataFrame({'Entry': list(still_missing)})\n        for col in search_fields:\n            if col != 'accession' and col not in missing_df.columns:\n                missing_df[col] = np.nan\n        df = pd.concat([df, missing_df], ignore_index=True)\n\n    if 'STRING' in df.columns:\n        # keep first STRING ID (or join all if you prefer)\n        df['xref_string'] = df['STRING'].apply(\n            lambda s: str(s).split(';')[0].strip() if pd.notna(s) and str(s).strip() else np.nan\n        )\n        df.drop(columns=['STRING'], inplace=True)\n\n    return df\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_upset_contents","title":"get_upset_contents","text":"<pre><code>get_upset_contents(pdata, classes, on='protein', upsetForm=True, debug=False)\n</code></pre> <p>Construct contents for an UpSet plot from a pAnnData object.</p> <p>This function extracts feature sets (proteins or peptides) present in specified sample classes and returns them either as a dictionary or in an <code>upsetplot</code>-compatible format.</p> <p>Parameters:</p> Name Type Description Default <code>pdata</code> <code>pAnnData</code> <p>The pAnnData object containing <code>.prot</code> and <code>.pep</code>.</p> required <code>classes</code> <code>str or list of str</code> <p>Metadata column(s) in <code>.obs</code> to define sample groups. Examples: <code>\"cell_type\"</code>, or <code>[\"cell_type\", \"treatment\"]</code>.</p> required <code>on</code> <code>str</code> <p>Data level to use. Options are <code>\"protein\"</code> (default) or <code>\"peptide\"</code>.</p> <code>'protein'</code> <code>upsetForm</code> <code>bool</code> <p>If True, return an <code>UpSet</code>-compatible DataFrame via <code>upsetplot.from_contents</code>. If False, return a raw dictionary.</p> <code>True</code> <code>debug</code> <code>bool</code> <p>If True, print filtering steps and class resolution details.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>upset_data</code> <code>DataFrame</code> <p>Binary presence/absence DataFrame for use with <code>upsetplot.UpSet</code>, if <code>upsetForm=True</code>.</p> <code>upset_dict</code> <code>dict</code> <p>Mapping of class \u2192 list of present features, if <code>upsetForm=False</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>on</code> is not <code>\"protein\"</code> or <code>\"peptide\"</code>.</p> Example <p>Get contents for an UpSet plot of sample classes:     <pre><code>upset_data = get_upset_contents(pdata, classes=\"treatment\")\nfrom upsetplot import UpSet\nUpSet(upset_data, subset_size=\"count\").plot()\n</code></pre></p> <p>Retrieve raw dictionary of sets instead:     <pre><code>upset_dict = get_upset_contents(pdata, classes=\"treatment\", upsetForm=False)\n</code></pre></p> <p>Query proteins from a set and highlight them in a plot:     <pre><code>upset_data = scutils.get_upset_contents(pdata, classes=\"condition\")\nprot_df = scutils.get_upset_query(upset_data, present=[\"treated\"], absent=[\"control\"])\nscplt.plot_rankquant(ax, pdata, classes=\"condition\", cmap=cmaps, color=colors)\nscplt.mark_rankquant(ax, pdata, mark_df=prot_df, class_values=[\"treated\"], color=\"black\")\n</code></pre></p> Related Functions <ul> <li>plot_upset: Plot UpSet diagrams directly.</li> <li>plot_venn: Plot Venn diagrams for up to 3 sets.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_upset_contents(pdata, classes, on = 'protein', upsetForm = True, debug=False):\n    \"\"\"\n    Construct contents for an UpSet plot from a pAnnData object.\n\n    This function extracts feature sets (proteins or peptides) present in\n    specified sample classes and returns them either as a dictionary or\n    in an `upsetplot`-compatible format.\n\n    Args:\n        pdata (pAnnData): The pAnnData object containing `.prot` and `.pep`.\n        classes (str or list of str): Metadata column(s) in `.obs` to define sample groups.\n            Examples: `\"cell_type\"`, or `[\"cell_type\", \"treatment\"]`.\n        on (str): Data level to use. Options are `\"protein\"` (default) or `\"peptide\"`.\n        upsetForm (bool): If True, return an `UpSet`-compatible DataFrame via\n            `upsetplot.from_contents`. If False, return a raw dictionary.\n        debug (bool): If True, print filtering steps and class resolution details.\n\n    Returns:\n        upset_data (pandas.DataFrame): Binary presence/absence DataFrame for use with\n            `upsetplot.UpSet`, if `upsetForm=True`.\n        upset_dict (dict): Mapping of class \u2192 list of present features,\n            if `upsetForm=False`.\n\n    Raises:\n        ValueError: If `on` is not `\"protein\"` or `\"peptide\"`.\n\n    Example:\n        Get contents for an UpSet plot of sample classes:\n            ```python\n            upset_data = get_upset_contents(pdata, classes=\"treatment\")\n            from upsetplot import UpSet\n            UpSet(upset_data, subset_size=\"count\").plot()\n            ```\n\n        Retrieve raw dictionary of sets instead:\n            ```python\n            upset_dict = get_upset_contents(pdata, classes=\"treatment\", upsetForm=False)\n            ```\n\n        Query proteins from a set and highlight them in a plot:\n            ```python\n            upset_data = scutils.get_upset_contents(pdata, classes=\"condition\")\n            prot_df = scutils.get_upset_query(upset_data, present=[\"treated\"], absent=[\"control\"])\n            scplt.plot_rankquant(ax, pdata, classes=\"condition\", cmap=cmaps, color=colors)\n            scplt.mark_rankquant(ax, pdata, mark_df=prot_df, class_values=[\"treated\"], color=\"black\")\n            ```\n\n    Related Functions:\n        - plot_upset: Plot UpSet diagrams directly.\n        - plot_venn: Plot Venn diagrams for up to 3 sets.\n    \"\"\"\n\n    if on == 'protein':\n        adata = pdata.prot\n    elif on == 'peptide':\n        adata = pdata.pep\n    else:\n        raise ValueError(\"Invalid value for 'on'. Options are 'protein' or 'peptide'.\")\n\n    # Common error: if classes is a list with only one element, unpack it\n    if isinstance(classes, list) and len(classes) == 1:\n        classes = classes[0]\n\n    classes_list = get_classlist(adata, classes)\n    upset_dict = {}\n\n    for j, class_value in enumerate(classes_list):\n        data_filtered = resolve_class_filter(adata, classes, class_value, debug=True)\n\n        # get proteins that are present in the filtered data (at least one value is not NaN, not 0)\n        X = data_filtered.X.toarray()\n        mask_present = (~np.isnan(X)) &amp; (X != 0)\n        prot_present = data_filtered.var_names[mask_present.sum(axis=0) &gt; 0]\n        upset_dict[class_value] = prot_present.tolist()\n\n    if upsetForm:\n        upset_data = upsetplot.from_contents(upset_dict)\n        return upset_data\n\n    else:\n        return upset_dict\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.get_upset_query","title":"get_upset_query","text":"<pre><code>get_upset_query(upset_content, present, absent)\n</code></pre> <p>Query features from UpSet contents given inclusion and exclusion criteria.</p> <p>This function extracts the set of features (proteins or peptides) that are present in all specified groups and absent in others. It then queries UniProt metadata for the resulting accessions.</p> <p>Parameters:</p> Name Type Description Default <code>upset_content</code> <code>DataFrame</code> <p>Output from <code>get_upset_contents</code> with presence/absence encoding of features.</p> required <code>present</code> <code>list of str</code> <p>List of groups in which the features must be present.</p> required <code>absent</code> <code>list of str</code> <p>List of groups in which the features must be absent.</p> required <p>Returns:</p> Name Type Description <code>prot_query_df</code> <code>DataFrame</code> <p>DataFrame of features matching the query,</p> <p>annotated with UniProt metadata via <code>get_uniprot_fields</code>.</p> Example <p>Query proteins unique to one group and highlight them in a plot:     <pre><code>upset_data = scutils.get_upset_contents(pdata, classes=\"condition\")\nprot_df = scutils.get_upset_query(upset_data, present=[\"treated\"], absent=[\"control\"])\nscplt.plot_rankquant(ax, pdata, classes=\"condition\", cmap=cmaps, color=colors)\nscplt.mark_rankquant(ax, pdata, mark_df=prot_df, class_values=[\"treated\"], color=\"black\")\n</code></pre></p> Related Functions <ul> <li>get_upset_contents: Generate presence/absence sets for UpSet analysis.</li> <li>plot_upset: Plot UpSet diagrams from class-based sets.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def get_upset_query(upset_content, present, absent):\n    \"\"\"\n    Query features from UpSet contents given inclusion and exclusion criteria.\n\n    This function extracts the set of features (proteins or peptides) that are\n    present in all specified groups and absent in others. It then queries\n    UniProt metadata for the resulting accessions.\n\n    Args:\n        upset_content (pandas.DataFrame): Output from `get_upset_contents` with\n            presence/absence encoding of features.\n        present (list of str): List of groups in which the features must be present.\n        absent (list of str): List of groups in which the features must be absent.\n\n    Returns:\n        prot_query_df (pandas.DataFrame): DataFrame of features matching the query,\n        annotated with UniProt metadata via `get_uniprot_fields`.\n\n    Example:\n        Query proteins unique to one group and highlight them in a plot:\n            ```python\n            upset_data = scutils.get_upset_contents(pdata, classes=\"condition\")\n            prot_df = scutils.get_upset_query(upset_data, present=[\"treated\"], absent=[\"control\"])\n            scplt.plot_rankquant(ax, pdata, classes=\"condition\", cmap=cmaps, color=colors)\n            scplt.mark_rankquant(ax, pdata, mark_df=prot_df, class_values=[\"treated\"], color=\"black\")\n            ```\n\n    Related Functions:\n        - get_upset_contents: Generate presence/absence sets for UpSet analysis.\n        - plot_upset: Plot UpSet diagrams from class-based sets.\n    \"\"\"\n    prot_query = upsetplot.query(upset_content, present=present, absent=absent).data['id'].tolist()\n    prot_query_df = get_uniprot_fields(prot_query,verbose=False)\n\n    return prot_query_df\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.pairwise_log2fc","title":"pairwise_log2fc","text":"<pre><code>pairwise_log2fc(data1, data2)\n</code></pre> <p>Compute pairwise median log2 fold change (log2FC) between two groups.</p> <p>This function calculates all pairwise log2 ratios between features in two groups of samples and returns the median value per feature. It is primarily used as a helper for fold-change strategies in <code>pAnnData.de()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data1</code> <code>ndarray</code> <p>Array of shape <code>(n_samples_group1, n_features)</code> containing abundance values for group 1.</p> required <code>data2</code> <code>ndarray</code> <p>Array of shape <code>(n_samples_group2, n_features)</code> containing abundance values for group 2.</p> required <p>Returns:</p> Name Type Description <code>median_log2fc</code> <code>ndarray</code> <p>Array of shape <code>(n_features,)</code> containing</p> <p>the median pairwise log2 fold change for each feature.</p> Note <p>This is an internal helper for differential expression calculations. End users should call <code>pAnnData.de()</code> instead of using this function directly.</p> Related Functions <ul> <li>pAnnData.de: Differential expression analysis with multiple fold change strategies.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def pairwise_log2fc(data1, data2):\n    \"\"\"\n    Compute pairwise median log2 fold change (log2FC) between two groups.\n\n    This function calculates all pairwise log2 ratios between features in\n    two groups of samples and returns the median value per feature. It is\n    primarily used as a helper for fold-change strategies in `pAnnData.de()`.\n\n    Args:\n        data1 (numpy.ndarray): Array of shape `(n_samples_group1, n_features)`\n            containing abundance values for group 1.\n        data2 (numpy.ndarray): Array of shape `(n_samples_group2, n_features)`\n            containing abundance values for group 2.\n\n    Returns:\n        median_log2fc (numpy.ndarray): Array of shape `(n_features,)` containing\n        the median pairwise log2 fold change for each feature.\n\n    Note:\n        This is an internal helper for differential expression calculations.\n        End users should call `pAnnData.de()` instead of using this function directly.\n\n    Related Functions:\n        - pAnnData.de: Differential expression analysis with multiple fold change strategies.\n    \"\"\"\n    n1, n2 = data1.shape[0], data2.shape[0]\n\n    # data1[:, None, :] has shape (n1, 1, n_features)\n    # data2[None, :, :] has shape (1, n2, n_features)\n    # The result is an array of shape (n1, n2, n_features)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        pairwise_ratios = np.log2(data1[:, None, :] / data2[None, :, :])  # (n1, n2, features)\n        pairwise_flat = pairwise_ratios.reshape(-1, data1.shape[1])\n\n    # Identify columns that are entirely NaN\n    mask_all_nan = np.all(np.isnan(pairwise_flat), axis=0)\n    median_fc = np.full(data1.shape[1], np.nan, dtype=float)\n\n    # Compute only on valid columns\n    if not np.all(mask_all_nan):\n        valid_cols = ~mask_all_nan\n        median_fc[valid_cols] = np.nanmedian(pairwise_flat[:, valid_cols], axis=0)\n\n    # # Reshape to (n1*n2, n_features) and compute the median along the first axis.\n    # median_fc = np.nanmedian(pairwise_ratios.reshape(-1, data1.shape[1]), axis=0)\n    return median_fc\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.resolve_accessions","title":"resolve_accessions","text":"<pre><code>resolve_accessions(adata, namelist, gene_col='Genes', gene_map=None)\n</code></pre> <p>Resolve gene or accession names to accession IDs from <code>.var_names</code>.</p> <p>This function maps user-specified identifiers (gene names or accession IDs) to the canonical accession IDs in an AnnData or pAnnData object. It first checks <code>.var_names</code> for exact matches, then optionally resolves gene names via a specified column (default <code>\"Genes\"</code>). Unmatched names are reported.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData or pAnnData</code> <p>AnnData-like object containing <code>.var</code>.</p> required <code>namelist</code> <code>list of str</code> <p>Input identifiers to resolve (genes or accessions).</p> required <code>gene_col</code> <code>str</code> <p>Column in <code>.var</code> containing gene names (default: <code>\"Genes\"</code>).</p> <code>'Genes'</code> <code>gene_map</code> <code>dict</code> <p>Precomputed mapping of gene \u2192 accession. If None, a mapping is constructed from <code>gene_col</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>resolved</code> <code>list of str</code> <p>List of accession IDs corresponding to the input names.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If none of the provided names can be resolved to <code>.var_names</code> or the gene column.</p> Example <p>Resolve gene symbols to accession IDs:     <pre><code>accs = resolve_accessions(adata, namelist=[\"UBE4B\", \"GAPDH\"])\n</code></pre></p> <p>Resolve accessions directly:   <pre><code>accs = resolve_accessions(adata, namelist=[\"P12345\", \"Q67890\"])\n</code></pre></p> Related Functions <ul> <li>get_gene_maps: Build full accession \u2192 gene mapping dictionaries.</li> <li>get_abundance: Extract abundance values by gene or accession.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def resolve_accessions(adata, namelist, gene_col=\"Genes\", gene_map=None):\n    \"\"\"\n    Resolve gene or accession names to accession IDs from `.var_names`.\n\n    This function maps user-specified identifiers (gene names or accession IDs)\n    to the canonical accession IDs in an AnnData or pAnnData object. It first\n    checks `.var_names` for exact matches, then optionally resolves gene names\n    via a specified column (default `\"Genes\"`). Unmatched names are reported.\n\n    Args:\n        adata (AnnData or pAnnData): AnnData-like object containing `.var`.\n        namelist (list of str): Input identifiers to resolve (genes or accessions).\n        gene_col (str): Column in `.var` containing gene names (default: `\"Genes\"`).\n        gene_map (dict, optional): Precomputed mapping of gene \u2192 accession. If None,\n            a mapping is constructed from `gene_col`.\n\n    Returns:\n        resolved (list of str): List of accession IDs corresponding to the input names.\n\n    Raises:\n        ValueError: If none of the provided names can be resolved to `.var_names`\n            or the gene column.\n\n    Example:\n        Resolve gene symbols to accession IDs:\n            ```python\n            accs = resolve_accessions(adata, namelist=[\"UBE4B\", \"GAPDH\"])\n            ```\n\n        Resolve accessions directly:    \n            ```python\n            accs = resolve_accessions(adata, namelist=[\"P12345\", \"Q67890\"])\n            ```\n\n    Related Functions:\n        - get_gene_maps: Build full accession \u2192 gene mapping dictionaries.\n        - get_abundance: Extract abundance values by gene or accession.\n    \"\"\"\n    import pandas as pd\n\n    if not namelist:\n        return None\n\n    var_names = adata.var_names.astype(str)\n\n    # Use passed-in gene_map or build one\n    if gene_map is None:\n        gene_map = {}\n        if gene_col in adata.var.columns:\n            for acc, gene in zip(var_names, adata.var[gene_col]):\n                if pd.notna(gene):\n                    gene_map[str(gene)] = acc\n\n    resolved, unmatched = [], []\n    for name in namelist:\n        name = str(name)\n        if name in var_names:\n            resolved.append(name)\n        elif name in gene_map:\n            resolved.append(gene_map[name])\n        else:\n            unmatched.append(name)\n\n    if not resolved:\n        raise ValueError(\n            f\"No valid names found in `namelist`: {namelist}.\\n\"\n            f\"Check against .var_names or '{gene_col}' column.\"\n        )\n\n    if unmatched:\n        print(f\"{format_log_prefix('warn')}resolve_accessions: Unmatched names:\")\n        for u in unmatched:\n            print(f\"  - {u}\")\n\n    return resolved\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.resolve_class_filter","title":"resolve_class_filter","text":"<pre><code>resolve_class_filter(adata, classes, class_value, debug=False, *, filter_func=None)\n</code></pre> <p>Resolve <code>(classes, class_value)</code> inputs and apply filtering.</p> <p>This helper standardizes class/value pairs into dictionary-style filters and applies them to an AnnData or pAnnData object. It is primarily used internally by plotting and analysis functions.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>AnnData or pAnnData</code> <p>Input data object to filter.</p> required <code>classes</code> <code>str or list of str</code> <p>Metadata field(s) used for filtering.</p> required <code>class_value</code> <code>str or list of str</code> <p>Values corresponding to <code>classes</code>.</p> required <code>debug</code> <code>bool</code> <p>If True, print resolved class/value pairs.</p> <code>False</code> <code>filter_func</code> <code>callable</code> <p>Filtering function to apply. Defaults to <code>utils.filter</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>filtered</code> <code>AnnData or pAnnData</code> <p>Subset of the input object, same type as <code>adata</code>.</p> <p>Warning</p> <p>This is an internal helper for use inside functions such as <code>plot_rankquant</code> and <code>plot_raincloud</code>. End users should call <code>pAnnData.filter_sample_values()</code> instead.</p> Related Functions <ul> <li>filter: Legacy utility for sample filtering.</li> <li>format_class_filter: Standardizes filter inputs.</li> <li>pAnnData.filter_sample_values: Recommended user-facing filter method.</li> </ul> Source code in <code>src/scpviz/utils.py</code> <pre><code>def resolve_class_filter(adata, classes, class_value, debug=False, *, filter_func=None):\n    \"\"\"\n    Resolve `(classes, class_value)` inputs and apply filtering.\n\n    This helper standardizes class/value pairs into dictionary-style filters\n    and applies them to an AnnData or pAnnData object. It is primarily used\n    internally by plotting and analysis functions.\n\n    Args:\n        adata (AnnData or pAnnData): Input data object to filter.\n        classes (str or list of str): Metadata field(s) used for filtering.\n        class_value (str or list of str): Values corresponding to `classes`.\n        debug (bool): If True, print resolved class/value pairs.\n        filter_func (callable, optional): Filtering function to apply.\n            Defaults to `utils.filter`.\n\n    Returns:\n        filtered (AnnData or pAnnData): Subset of the input object, same type as `adata`.\n\n    !!! warning\n        This is an internal helper for use inside functions such as\n        `plot_rankquant` and `plot_raincloud`. End users should call\n        `pAnnData.filter_sample_values()` instead.\n\n    Related Functions:\n        - filter: Legacy utility for sample filtering.\n        - format_class_filter: Standardizes filter inputs.\n        - pAnnData.filter_sample_values: Recommended user-facing filter method.\n    \"\"\"\n\n    from scpviz import utils  # safe to do here in case of circular import\n\n    if isinstance(classes, str):\n        values = class_value\n    else:\n        values = class_value.split('_')\n\n    if debug:\n        print(f\"Classes: {classes}, Values: {values}\")\n\n    if filter_func is None:\n        filter_func = utils.filter\n\n    return filter_func(adata, classes, values, debug=debug)\n</code></pre>"},{"location":"reference/utils/#src.scpviz.utils.standardize_uniprot_columns","title":"standardize_uniprot_columns","text":"<pre><code>standardize_uniprot_columns(df)\n</code></pre> <p>Normalize UniProt DataFrame column names to a consistent lowercase, snake_case schema.</p> <p>This ensures stability across UniProt REST API version changes while keeping the user informed only when critical fields are affected.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Raw UniProt metadata table.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: Copy of the DataFrame with standardized column names.</p> Source code in <code>src/scpviz/utils.py</code> <pre><code>def standardize_uniprot_columns(df):\n    \"\"\"\n    Normalize UniProt DataFrame column names to a consistent lowercase, snake_case schema.\n\n    This ensures stability across UniProt REST API version changes while keeping\n    the user informed only when critical fields are affected.\n\n    Args:\n        df (pd.DataFrame): Raw UniProt metadata table.\n\n    Returns:\n        pd.DataFrame: Copy of the DataFrame with standardized column names.\n    \"\"\"\n    if df is None or not isinstance(df, pd.DataFrame) or df.shape[1] == 0:\n        return df\n\n    rename_map = {}\n    aliases = {\n        # identifiers\n        \"entry\": \"accession\",\n        \"entry_name\": \"id\",\n        \"accession\": \"accession\",\n        \"primaryaccession\": \"accession\",\n\n        # gene fields\n        \"gene_names_primary\": \"gene_primary\",\n        \"gene_name_primary\": \"gene_primary\",\n        \"gene_primary_name\": \"gene_primary\",\n        \"gene_primary\": \"gene_primary\",\n        \"gene_primaryname\": \"gene_primary\",\n        \"gene_primary_name_\": \"gene_primary\",\n        \"gene_primaryname_\": \"gene_primary\",\n\n        # organism fields\n        \"organism_id\": \"organism_id\",\n        \"organism_identifier\": \"organism_id\",\n        \"organismid\": \"organism_id\",\n\n        # STRING / cross-reference\n        \"cross_reference_string\": \"xref_string\",\n        \"xref_string_id\": \"xref_string\",\n        \"crossreference_string\": \"xref_string\",\n        \"string\": \"xref_string\",\n        \"string_id\": \"xref_string\",\n        \"xref_string\": \"xref_string\",\n    }\n\n    # critical canonical fields we care about if changed or missing\n    critical_fields = {\"accession\", \"gene_primary\", \"organism_id\", \"xref_string\"}\n\n    # known benign patterns \u2014 don't warn if these change\n    benign_patterns = {\n        \"gene_ontology\",\n        \"go\",\n        \"gene_names\",      # non-primary gene list\n        \"protein_name\",    # descriptive only\n        \"cc_interaction\",  # crossref metadata\n    }\n\n    for col in df.columns:\n        norm = (\n            re.sub(r\"[^a-z0-9]+\", \"_\", col.lower())\n            .strip(\"_\")\n            .replace(\"__\", \"_\")\n        )\n\n        mapped = aliases.get(norm, None)\n\n        if mapped:\n            rename_map[col] = mapped\n        else:\n            # warn only if this looks like a drifted critical column\n            if (\n                any(k in norm for k in [\"accession\", \"gene\", \"organism\", \"string\"])\n                and not any(p in norm for p in benign_patterns)\n            ):\n                warnings.warn(\n                    f\"[standardize_uniprot_columns] \u26a0\ufe0f Unrecognized UniProt column '{col}' \"\n                    f\"(normalized='{norm}') \u2014 may affect critical mapping.\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n            rename_map[col] = norm  # keep normalized fallback name\n\n    df = df.rename(columns=rename_map)\n    # verify that all critical fields exist at least once\n    missing_critical = [c for c in critical_fields if c not in df.columns]\n    if missing_critical:\n        warnings.warn(\n            f\"[standardize_uniprot_columns] Missing expected UniProt columns: {', '.join(missing_critical)}\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    return df.rename(columns=rename_map)\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/","title":"Analysis &amp; Enrichment","text":"<p>Mixins for statistical analysis and biological interpretation.</p>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin","title":"AnalysisMixin","text":"<p>Provides core statistical and dimensionality reduction tools for analyzing single-cell proteomics data.</p> <p>This mixin includes functionality for:</p> <ul> <li>Differential expression (DE) analysis using t-tests, Mann\u2013Whitney U, or Wilcoxon signed-rank tests  </li> <li>Ranking proteins or peptides by abundance within groups  </li> <li>Coefficient of Variation (CV) computation  </li> <li>Missing value imputation (global or group-wise) using statistical or KNN-based methods  </li> <li>Dimensionality reduction and clustering using PCA, UMAP, and Leiden  </li> <li>Neighbor graph construction for downstream manifold learning  </li> <li>Cleaning <code>.X</code> matrices by replacing NaNs  </li> <li>Row-wise normalization across multiple strategies  </li> </ul> <p>All functions are compatible with both protein- and peptide-level data and support use of AnnData layers.</p> <p>Methods:</p> Name Description <code>cv</code> <p>Compute coefficient of variation (CV) for each feature across or within sample groups.</p> <code>de</code> <p>Perform differential expression analysis between two sample groups.</p> <code>rank</code> <p>Rank features by mean abundance, compute standard deviation and numeric rank.</p> <code>impute</code> <p>Impute missing values globally or within groups using mean, median, min, or KNN.</p> <code>neighbor</code> <p>Compute neighborhood graph using PCA (or another embedding) for clustering or UMAP.</p> <code>leiden</code> <p>Run Leiden clustering on neighborhood graph, storing labels in <code>.obs['leiden']</code>.</p> <code>umap</code> <p>Perform UMAP dimensionality reduction using previously computed neighbors.</p> <code>pca</code> <p>Run PCA on normalized expression matrix, handling NaN exclusion and reinsertion of features.</p> <code>clean_X</code> <p>Replace NaNs in <code>.X</code> or a specified layer, optionally backing up the original.</p> <code>_normalize_helper</code> <p>Internal helper to compute per-sample scaling across multiple normalization methods.</p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>class AnalysisMixin:\n    \"\"\"\n    Provides core statistical and dimensionality reduction tools for analyzing single-cell proteomics data.\n\n    This mixin includes functionality for:\n\n    - Differential expression (DE) analysis using t-tests, Mann\u2013Whitney U, or Wilcoxon signed-rank tests  \n    - Ranking proteins or peptides by abundance within groups  \n    - Coefficient of Variation (CV) computation  \n    - Missing value imputation (global or group-wise) using statistical or KNN-based methods  \n    - Dimensionality reduction and clustering using PCA, UMAP, and Leiden  \n    - Neighbor graph construction for downstream manifold learning  \n    - Cleaning `.X` matrices by replacing NaNs  \n    - Row-wise normalization across multiple strategies  \n\n    All functions are compatible with both protein- and peptide-level data and support use of AnnData layers.\n\n    Functions:\n        cv: Compute coefficient of variation (CV) for each feature across or within sample groups.\n        de: Perform differential expression analysis between two sample groups.\n        rank: Rank features by mean abundance, compute standard deviation and numeric rank.\n        impute: Impute missing values globally or within groups using mean, median, min, or KNN.\n        neighbor: Compute neighborhood graph using PCA (or another embedding) for clustering or UMAP.\n        leiden: Run Leiden clustering on neighborhood graph, storing labels in `.obs['leiden']`.\n        umap: Perform UMAP dimensionality reduction using previously computed neighbors.\n        pca: Run PCA on normalized expression matrix, handling NaN exclusion and reinsertion of features.\n        clean_X: Replace NaNs in `.X` or a specified layer, optionally backing up the original.\n        _normalize_helper: Internal helper to compute per-sample scaling across multiple normalization methods.\n    \"\"\"\n\n    def cv(self, classes = None, on = 'protein', layer = \"X\", debug = False):\n        \"\"\"\n        Compute the coefficient of variation (CV) for each feature across sample groups.\n\n        This method calculates CV for each protein or peptide across all samples in each group,\n        storing the result as new columns in `.var`, one per group.\n\n        Args:\n            classes (str or list of str, optional): Sample-level class or list of classes used to define groups.\n            on (str): Whether to compute CV on \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use for computation (default is \"X\").\n            debug (bool): If True, prints debug information while filtering groups.\n\n        Returns:\n            None\n\n        Example:\n            Compute per-group CV for proteins using a custom normalization layer:\n                ```python\n                pdata.cv(classes=[\"group\", \"condition\"], on=\"protein\", layer=\"X_norm\")\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        adata = self.prot if on == 'protein' else self.pep\n        classes_list = utils.get_classlist(adata, classes)\n\n        for j, class_value in enumerate(classes_list):\n            data_filtered = utils.resolve_class_filter(adata, classes, class_value)\n\n            cv_data = data_filtered.X.toarray() if layer == \"X\" else data_filtered.layers[layer].toarray() if layer in data_filtered.layers else None\n            if cv_data is None:\n                raise ValueError(f\"Layer '{layer}' not found in adata.layers.\")\n\n            adata.var['CV: '+ class_value] = variation(cv_data, axis=0)\n\n        self._history.append(f\"{on}: Coefficient of Variation (CV) calculated for {layer} data by {classes}. CV stored in var['CV: {class_value}'].\") # type: ignore[attr-defined]\n\n    # TODO: implement methods for calculdating fold change, 1. mean, 2. prot pairwise median, or 3. pep pairwise median (will need to refer to RS)\n    def de(self, values=None, class_type=None, method='ttest', layer='X', pval=0.05, log2fc=1.0, fold_change_mode='mean'):\n        \"\"\"\n        Perform differential expression (DE) analysis on proteins across sample groups.\n\n        This method compares protein abundance between two sample groups using a specified\n        statistical test and fold change method. Input groups can be defined using either\n        legacy-style (`class_type` + `values`) or dictionary-style filters.\n\n        Args:\n            values (list of dict or list of list): Sample group filters to compare.\n\n                - Dictionary-style (recommended): [{'cellline': 'HCT116', 'treatment': 'DMSO'}, {...}]\n                - Legacy-style (if `class_type` is provided): [['HCT116', 'DMSO'], ['HCT116', 'DrugX']]\n\n            class_type (str or list of str, optional): Legacy-style class label(s) to interpret `values`.\n\n            method (str): Statistical test to use. Options: \"ttest\", \"mannwhitneyu\", \"wilcoxon\".\n\n            layer (str): Name of the data layer to use (default is \"X\").\n\n            pval (float): P-value cutoff used for labeling significance.\n\n            log2fc (float): Minimum log2 fold change threshold for significance labeling.\n\n            fold_change_mode (str): Strategy for computing fold change. Options:\n\n                - \"mean\": log2(mean(group1) / mean(group2))\n                - \"pairwise_median\": median of all pairwise log2 ratios\n                - \"pep_pairwise_median\": median of peptide-level pairwise log2 ratios, aggregated per protein\n\n        Returns:\n            pd.DataFrame: DataFrame with DE statistics including log2 fold change, p-values, and significance labels.\n\n        Example:\n            Legacy-style DE comparison using class types and value combinations:\n                ```python\n                pdata.de(\n                    class_type=[\"cellline\", \"treatment\"],\n                    values=[[\"HCT116\", \"DMSO\"], [\"HCT116\", \"DrugX\"]]\n                )\n                ```\n\n            Dictionary-style (recommended) DE comparison:\n                ```python\n                pdata.de(\n                    values=[\n                        {\"cellline\": \"HCT116\", \"treatment\": \"DMSO\"},\n                        {\"cellline\": \"HCT116\", \"treatment\": \"DrugX\"}\n                    ]\n                )\n                ```\n        \"\"\"\n\n        # --- Handle legacy input ---\n        if values is None:\n            raise ValueError(\"Please provide `values` (new format) or both `class_type` and `values` (legacy format).\")\n\n        if class_type is not None:\n            values = utils.format_class_filter(class_type, values, exact_cases=True)\n\n        if not isinstance(values, list) or len(values) != 2:\n            raise ValueError(\"`values` must be a list of two group dictionaries (or legacy value pairs).\")\n\n        if values[0] == values[1]:\n            raise ValueError(\"Both groups in `values` refer to the same condition. Please provide two distinct groups.\")\n\n        group1_dict, group2_dict = (\n            [values[0]] if not isinstance(values[0], list) else values[0],\n            [values[1]] if not isinstance(values[1], list) else values[1]\n        )\n\n\n        # --- Sample filtering ---\n        pdata_case1 = self._filter_sample_values(values=group1_dict, exact_cases=True, return_copy=True, verbose=False, cleanup=False) # type: ignore[attr-defined], FilteringMixin\n        pdata_case2 = self._filter_sample_values(values=group2_dict, exact_cases=True, return_copy=True, verbose=False, cleanup=False) # type: ignore[attr-defined], FilteringMixin\n\n        def _label(d):\n            if isinstance(d, dict):\n                return '_'.join(str(v) for v in d.values())\n            return str(d)\n\n        group1_string = _label(group1_dict)\n        group2_string = _label(group2_dict)\n        comparison_string = f'{group1_string} vs {group2_string}'\n\n        log_prefix = format_log_prefix(\"user\")\n        n1, n2 = len(pdata_case1.prot), len(pdata_case2.prot)\n        print(f\"{log_prefix} Running differential expression [protein]\")\n        print(f\"   \ud83d\udd38 Comparing groups: {comparison_string}\")\n        print(f\"   \ud83d\udd38 Group sizes: {n1} vs {n2} samples\")\n        print(f\"   \ud83d\udd38 Method: {method} | Fold Change: {fold_change_mode} | Layer: {layer}\")\n        print(f\"   \ud83d\udd38 P-value threshold: {pval} | Log2FC threshold: {log2fc}\")\n\n        # --- Get layer data ---\n        data1 = utils.get_adata_layer(pdata_case1.prot, layer)\n        data2 = utils.get_adata_layer(pdata_case2.prot, layer)\n\n        # Shape: (samples, features)\n        data1 = np.asarray(data1)\n        data2 = np.asarray(data2)\n\n        # --- Compute fold change ---\n        if fold_change_mode == 'mean':\n            with np.errstate(all='ignore'):\n                group1_mean = np.nanmean(data1, axis=0)\n                group2_mean = np.nanmean(data2, axis=0)\n\n                # Identify zeros or NaNs in either group\n                mask_invalid = (group1_mean == 0) | (group2_mean == 0) | np.isnan(group1_mean) | np.isnan(group2_mean)\n                log2fc_vals = np.log2(group1_mean / group2_mean)\n                log2fc_vals[mask_invalid] = np.nan\n\n                n_invalid = np.sum(mask_invalid)\n                if n_invalid &gt; 0:\n                    print(f\"{format_log_prefix('info',2)} {n_invalid} proteins were not comparable (zero or NaN mean in one group).\")\n\n        elif fold_change_mode == 'pairwise_median':\n            mask_invalid = ( # Detect invalid features (any 0 or NaN in either group)\n                np.any((data1 == 0) | np.isnan(data1), axis=0) |\n                np.any((data2 == 0) | np.isnan(data2), axis=0)\n            )\n            # Compute median pairwise log2FC\n            log2fc_vals = utils.pairwise_log2fc(data1, data2)\n            log2fc_vals[mask_invalid] = np.nan # Mark invalid features as NaN\n            n_invalid = np.sum(mask_invalid)\n            if n_invalid &gt; 0:\n                print(f\"{format_log_prefix('info',2)} {n_invalid} proteins were not comparable (zero or NaN mean in one group).\")\n\n        elif fold_change_mode == 'pep_pairwise_median':\n            # --- Validate .pep presence ---\n            if self.pep is None:\n                raise ValueError(\"Peptide-level data (.pep) is required for fold_change_mode='pep_pairwise_median', but self.pep is None.\")\n\n            # --- Handle peptide layer fallback ---\n            actual_layer = layer\n            if layer != 'X' and not (hasattr(self.pep, \"layers\") and layer in self.pep.layers):\n                warnings.warn(\n                    f\"Layer '{layer}' not found in .pep.layers. Falling back to 'X'.\",\n                    UserWarning\n                )\n                actual_layer = 'X'\n\n            # Get peptide data\n            pep_data1 = np.asarray(utils.get_adata_layer(pdata_case1.pep, actual_layer))\n            pep_data2 = np.asarray(utils.get_adata_layer(pdata_case2.pep, actual_layer))\n\n            # Detect invalid peptides (any 0 or NaN in either group)\n            mask_invalid_pep = (\n                np.any((pep_data1 == 0) | np.isnan(pep_data1), axis=0) |\n                np.any((pep_data2 == 0) | np.isnan(pep_data2), axis=0)\n            )\n\n            # Compute per-peptide pairwise log2FCs\n            pep_log2fc = utils.pairwise_log2fc(pep_data1, pep_data2)\n            pep_log2fc[mask_invalid_pep] = np.nan  # mark invalids\n\n            n_invalid_pep = np.sum(mask_invalid_pep)\n            if n_invalid_pep &gt; 0:\n                print(f\"{format_log_prefix('info',2)} {n_invalid_pep} peptides were not comparable (zero or NaN mean in one group).\")\n\n            # Map peptides to proteins\n            pep_to_prot = utils.get_pep_prot_mapping(self, return_series=True)\n\n            # Aggregate peptide log2FCs into protein-level log2FCs\n            prot_log2fc = pd.Series(index=self.prot.var_names, dtype=float)\n            not_comparable_prot = []\n\n            for prot in self.prot.var_names:\n                matching_peptides = pep_to_prot[pep_to_prot == prot].index\n                if len(matching_peptides) == 0:\n                    continue\n\n                idxs = self.pep.var_names.get_indexer(matching_peptides)\n                valid_idxs = idxs[idxs &gt;= 0]\n                if len(valid_idxs) == 0:\n                    continue\n\n                valid_log2fc = pep_log2fc[valid_idxs]\n\n                if np.all(np.isnan(valid_log2fc)):\n                    prot_log2fc[prot] = np.nan\n                    not_comparable_prot.append(prot)\n                else:\n                    prot_log2fc[prot] = np.nanmedian(pep_log2fc[valid_idxs])\n\n            log2fc_vals = prot_log2fc.values\n            if len(not_comparable_prot) &gt; 0:\n                print(f\"{format_log_prefix('info',2)} {len(not_comparable_prot)} proteins were not comparable (all peptides invalid or missing).\")\n\n        else:\n            raise ValueError(f\"Unsupported fold_change_mode: {fold_change_mode}\")\n\n        # --- Statistical test ---\n        pvals = []\n        stats = []\n        for i in range(data1.shape[1]):\n            x1, x2 = data1[:, i], data2[:, i]\n            try:\n                if method == 'ttest':\n                    res = ttest_ind(x1, x2, nan_policy='omit')\n                elif method == 'mannwhitneyu':\n                    res = mannwhitneyu(x1, x2, alternative='two-sided')\n                elif method == 'wilcoxon':\n                    res = wilcoxon(x1, x2)\n                else:\n                    raise ValueError(f\"Unsupported test method: {method}\")\n                pvals.append(res.pvalue)\n                stats.append(res.statistic)\n            except Exception as e:\n                pvals.append(np.nan)\n                stats.append(np.nan)\n\n        # --- Compile results ---\n        var = self.prot.var.copy()\n        df_stats = pd.DataFrame(index=self.prot.var_names)\n        df_stats['Genes'] = var['Genes'] if 'Genes' in var.columns else var.index\n        df_stats[group1_string] = np.nanmean(data1, axis=0)\n        df_stats[group2_string] = np.nanmean(data2, axis=0)\n        df_stats['log2fc'] = log2fc_vals\n        df_stats['p_value'] = pvals\n        df_stats['test_statistic'] = stats\n\n        df_stats['-log10(p_value)'] = -np.log10(df_stats['p_value'].replace(0, np.nan).astype(float))\n        df_stats['significance_score'] = df_stats['-log10(p_value)'] * df_stats['log2fc']\n        df_stats['significance'] = 'not significant'\n        mask_not_comparable = df_stats['log2fc'].isna()\n        df_stats.loc[mask_not_comparable, 'significance'] = 'not comparable'\n        df_stats.loc[(df_stats['p_value'] &lt; pval) &amp; (df_stats['log2fc'] &gt; log2fc), 'significance'] = 'upregulated'\n        df_stats.loc[(df_stats['p_value'] &lt; pval) &amp; (df_stats['log2fc'] &lt; -log2fc), 'significance'] = 'downregulated'\n        df_stats['significance'] = pd.Categorical(df_stats['significance'], categories=['upregulated', 'downregulated', 'not significant', 'not comparable'], ordered=True)\n\n        df_stats = df_stats.sort_values(by='significance')\n\n        # --- Store and return ---\n        self._stats[comparison_string] = df_stats # type: ignore[attr-defined]\n        self._append_history(f\"prot: DE for {class_type} {values} using {method} and fold_change_mode='{fold_change_mode}'. Stored in .stats['{comparison_string}'].\") # type: ignore[attr-defined], HistoryMixin\n\n        sig_counts = df_stats['significance'].value_counts().to_dict()\n        n_up = sig_counts.get('upregulated', 0)\n        n_down = sig_counts.get('downregulated', 0)\n        n_ns = sig_counts.get('not significant', 0)\n\n        print(f\"{format_log_prefix('result_only', indent=2)} DE complete. Results stored in:\")\n        print(f'       \u2022 .stats[\"{comparison_string}\"]')\n        print(f\"       \u2022 Columns: log2fc, p_value, significance, etc.\")\n        print(f\"       \u2022 Upregulated: {n_up} | Downregulated: {n_down} | Not significant: {n_ns}\")\n\n        return df_stats\n\n    # TODO: Need to figure out how to make this interface with plot functions, probably do reordering by each class_value within the loop?\n    def rank(self, classes = None, on = 'protein', layer = \"X\"):\n        \"\"\"\n        Rank proteins or peptides by average abundance across sample groups.\n\n        This method computes the average and standard deviation for each feature within \n        each group and assigns a rank (highest to lowest) based on the group-level mean.\n        The results are stored in `.var` with one set of columns per group.\n\n        Args:\n            classes (str or list of str, optional): Sample-level class/grouping column(s) in `.obs`.\n            on (str): Whether to compute ranks on \"protein\" or \"peptide\" data.\n            layer (str): Name of the data layer to use (default is \"X\").\n\n        Returns:\n            None\n\n        Example:\n            Rank proteins by average abundance across treatment groups:\n                ```python\n                pdata.rank(classes=\"treatment\", on=\"protein\", layer=\"X_norm\")\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        adata = self.prot if on == 'protein' else self.pep\n        classes_list = utils.get_classlist(adata, classes)\n\n        for class_value in classes_list:\n            rank_data = utils.resolve_class_filter(adata, classes, class_value)\n            if layer == \"X\":\n                layer_data = rank_data.X.toarray()\n            elif layer in rank_data.layers:\n                layer_data = rank_data.layers[layer].toarray()\n            else:\n                raise ValueError(f\"Layer '{layer}' not found in layers.\")\n\n            # Convert sparse to dense if needed\n            if hasattr(layer_data, 'toarray'):\n                layer_data = layer_data.toarray()\n\n            # Transpose to get DataFrame of shape (features, samples)\n            rank_df = pd.DataFrame(layer_data.T, index=rank_data.var.index, columns=rank_data.obs_names)\n\n            # Compute stats\n            avg_col = f\"Average: {class_value}\"\n            std_col = f\"Stdev: {class_value}\"\n            rank_col = f\"Rank: {class_value}\"\n\n            with np.errstate(invalid='ignore', divide='ignore'):\n                rank_df[avg_col] = np.nanmean(layer_data, axis=0)\n                rank_df[std_col] = np.nanstd(layer_data, axis=0)\n\n            # Sort by average (descending), assign rank\n            rank_df.sort_values(by=avg_col, ascending=False, inplace=True)\n            rank_df[rank_col] = np.where(rank_df[avg_col].isna(), np.nan, np.arange(1, len(rank_df) + 1))\n\n            # Reindex back to original order in adata.var\n            rank_df = rank_df.reindex(adata.var.index)\n\n            adata.var[avg_col] = rank_df[avg_col]\n            adata.var[std_col] = rank_df[std_col]\n            adata.var[rank_col] = rank_df[rank_col]\n\n        self._history.append(f\"{on}: Ranked {layer} data. Ranking, average and stdev stored in var.\") # type: ignore[attr-defined], HistoryMixin\n\n    def impute(self, classes=None, layer=\"X\", method='mean', on='protein', min_scale=1, set_X=True, **kwargs):\n        \"\"\"\n        Impute missing values across samples globally or within groups.\n\n        This method imputes missing values in the specified data layer using one of several strategies.\n        It supports both global (across all samples) and group-wise imputation based on sample classes.\n\n        Args:\n            classes (str or list of str, optional): Sample-level class/grouping column(s). If None, imputation is global.\n            layer (str): Data layer to impute from (default is \"X\").\n            method (str): Imputation strategy to use. Options include:\n\n                - \"mean\": Fill missing values with the mean of each feature.\n                - \"median\": Fill missing values with the median of each feature.\n                - \"min\": Fill with the minimum observed value (0 if all missing).\n                - \"knn\": Use K-nearest neighbors (only supported for global imputation).\n\n            on (str): Whether to impute \"protein\" or \"peptide\" data.\n            min_scale (float): Scaled multiplication of minimum value for imputation, i.e. 0.2 would be 20% of minimum value (default is 1).\n            set_X (bool): If True, updates `.X` to use the imputed result.\n            **kwargs: Additional arguments passed to the imputer (e.g., `n_neighbors` for KNN).\n\n        Returns:\n            None\n\n        Example:\n            Globally impute missing values using the median strategy:\n                ```python\n                pdata.impute(method=\"median\", on=\"protein\")\n                ```\n\n            Group-wise imputation based on treatment:\n                ```python\n                pdata.impute(classes=\"treatment\", method=\"mean\", on=\"protein\")\n                ```\n\n        Note:\n            - KNN imputation is only supported for global (non-grouped) mode.\n            - Features that are entirely missing within a group or across all samples are skipped and preserved as NaN.\n            - Imputed results are stored in a new layer named `\"X_impute_&lt;method&gt;\"`.\n            - Imputation summaries are printed to the console by group or overall.\n        \"\"\"\n        from sklearn.impute import SimpleImputer, KNNImputer\n        from scipy import sparse\n        from scpviz import utils\n\n\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            return\n\n        adata = self.prot if on == 'protein' else self.pep\n        if layer != \"X\" and layer not in adata.layers:\n            raise ValueError(f\"Layer '{layer}' not found in .{on}.\")\n\n        impute_data = adata.layers[layer] if layer != \"X\" else adata.X\n        was_sparse = sparse.issparse(impute_data)\n        impute_data = impute_data.toarray() if was_sparse else impute_data.copy()\n        original_data = impute_data.copy()\n\n        layer_name = f\"X_impute_{method}\"\n\n        if method not in {\"mean\", \"median\", \"min\",\"knn\"}:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        if classes is None:\n            # Global imputation\n            if method == 'min':\n                min_vals = np.nanmin(impute_data, axis=0)\n                min_vals = np.where(np.isnan(min_vals), 0, min_vals)\n                min_vals = min_vals * min_scale\n                mask = np.isnan(impute_data)\n                impute_data[mask] = np.take(min_vals, np.where(mask)[1])\n            elif method == 'knn':\n                n_neighbors = kwargs.get('n_neighbors', 3)\n                imputer = KNNImputer(n_neighbors=n_neighbors)\n                impute_data = imputer.fit_transform(impute_data)\n            else:\n                imputer = SimpleImputer(strategy=method, keep_empty_features=True)\n                nan_columns = np.isnan(impute_data).all(axis=0)  # features fully missing in this group\n                impute_data = imputer.fit_transform(impute_data)\n                impute_data[:, nan_columns] = np.nan\n\n            min_message = \"\" if method != 'min' else f\"Minimum scaled by {min_scale}.\"\n            print(f\"{format_log_prefix('user')} Global imputation using '{method}'. Layer saved as '{layer_name}'. {min_message}\")\n            skipped_features = np.sum(np.isnan(impute_data).all(axis=0))\n\n        else:\n            # Group-wise imputation\n            if method == 'knn':\n                raise ValueError(\"KNN imputation is not supported for group-wise imputation.\")\n\n            sample_names = utils.get_samplenames(adata, classes)\n            sample_names = np.array(sample_names)\n            unique_groups = np.unique(sample_names)\n\n            for group in unique_groups:\n                idx = np.where(sample_names == group)[0]\n                group_data = impute_data[idx, :]\n\n                if method == 'min':\n                    min_vals = np.nanmin(group_data, axis=0)\n                    min_vals = np.where(np.isnan(min_vals), 0, min_vals)\n                    min_vals = min_vals * min_scale\n                    mask = np.isnan(group_data)\n                    group_data[mask] = np.take(min_vals, np.where(mask)[1])\n                    imputed_group = group_data\n                else:\n                    imputer = SimpleImputer(strategy=method, keep_empty_features=True)\n                    nan_columns = np.isnan(group_data).all(axis=0)  # features fully missing in this group\n                    imputed_group = imputer.fit_transform(group_data)\n                    imputed_group[:, nan_columns] = np.nan # restore fully missing features\n\n                impute_data[idx, :] = imputed_group\n\n            min_message = \"\" if method != 'min' else f\"Minimum scaled by {min_scale}.\"\n            print(f\"{format_log_prefix('user')} Group-wise imputation using '{method}' on class(es): {classes}. Layer saved as '{layer_name}'. {min_message}\")\n\n        summary_lines = []\n        if classes is None:\n            num_imputed = np.sum(np.isnan(original_data) &amp; ~np.isnan(impute_data))\n            # Row-wise missingness\n            was_missing = np.isnan(original_data).any(axis=1)\n            now_complete = ~np.isnan(impute_data).any(axis=1)\n            now_incomplete = np.isnan(impute_data).any(axis=1)\n\n            fully_imputed_samples = np.sum(was_missing &amp; now_complete)\n            partially_imputed_samples = np.sum(was_missing &amp; now_incomplete)\n            skipped_features = np.sum(np.isnan(impute_data).all(axis=0))\n\n            summary_lines.append(\n                f\"{format_log_prefix('result_only', indent=2)} {num_imputed} values imputed.\"\n            )\n            summary_lines.append(\n                f\"{format_log_prefix('info_only', indent=2)} {fully_imputed_samples} samples fully imputed, {partially_imputed_samples} samples partially imputed, {skipped_features} skipped feature(s) with all missing values.\"\n            )\n\n        else:\n            sample_names = utils.get_samplenames(adata, classes)\n            sample_names = np.array(sample_names)\n            unique_groups = np.unique(sample_names)\n\n            counts_by_group = {}\n            fully_by_group = {}\n            partial_by_group = {}\n            missing_features_by_group = {}\n            total_samples_by_group = {}\n\n            for group in unique_groups:\n                idx = np.where(sample_names == group)[0]\n                before = original_data[idx, :]\n                after = impute_data[idx, :]\n\n                # count imputed values\n                mask = np.isnan(before) &amp; ~np.isnan(after)\n                counts_by_group[group] = np.sum(mask)\n\n                # count fully and partially imputed samples\n                was_missing = np.isnan(before).any(axis=1)\n                now_complete = ~np.isnan(after).any(axis=1)\n                now_incomplete = np.isnan(after).any(axis=1)\n                now_missing = np.sum(np.isnan(before).all(axis=0))\n\n                fully_by_group[group] = np.sum(was_missing &amp; now_complete)\n                partial_by_group[group] = np.sum(was_missing &amp; now_incomplete)\n                missing_features_by_group[group] = now_missing\n                total_samples_by_group[group] = len(idx)\n\n            # Compute dynamic width based on longest group name\n            group_width = max(max(len(str(g)) for g in unique_groups), 20)\n\n            # Summary totals\n            total = sum(counts_by_group.values())\n            summary_lines.append(f\"{format_log_prefix('result_only', indent=2)} {total} values imputed total.\")\n            summary_lines.append(f\"{format_log_prefix('info_only', indent=2)} Group-wise summary:\")\n\n            # Header row (aligned with computed width)\n            header = (f\"{'Group':&lt;{group_width}} | Values Imputed | Skipped Features | Samples Imputed (Partial,Fully)/Total\")\n            divider = \"-\" * len(header)\n            summary_lines.append(f\"{' ' * 5}{header}\")\n            summary_lines.append(f\"{' ' * 5}{divider}\")\n\n            # Data rows\n            for group in unique_groups:\n                count = counts_by_group[group]\n                fully = fully_by_group[group]\n                partial = partial_by_group[group]\n                skipped = missing_features_by_group[group]\n                total_samples = total_samples_by_group[group]\n                summary_lines.append(\n                    f\"{' ' * 5}{group:&lt;{group_width}} | {count:&gt;14} | {skipped:&gt;16} | {partial:&gt;7}, {fully:&gt;5} / {total_samples:&lt;3}\"\n                )\n\n        print(\"\\n\".join(summary_lines))\n\n        adata.layers[layer_name] = sparse.csr_matrix(impute_data) if was_sparse else impute_data\n\n        if set_X:\n            self.set_X(layer=layer_name, on=on) # type: ignore[attr-defined], EditingMixin\n\n        self._history.append( # type: ignore[attr-defined]\n            f\"{on}: Imputed layer '{layer}' using '{method}' (grouped by {classes if classes else 'ALL'}). Stored in '{layer_name}'.\"\n        )\n\n    def neighbor(self, on = 'protein', layer = \"X\", use_rep='X_pca', user_indent=0,**kwargs):\n        \"\"\"\n        Compute a neighbor graph based on protein or peptide data.\n\n        This method builds a nearest-neighbors graph for downstream analysis using \n        `scanpy.pp.neighbors`. It optionally performs PCA before constructing the graph \n        if a valid representation is not already available.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use (default is \"X\").\n            use_rep (str): Key in `.obsm` to use for computing neighbors. Default is `\"X_pca\"`.\n                If `\"X_pca\"` is requested but not found, PCA will be run automatically.\n            **kwargs: Additional keyword arguments passed to `scanpy.pp.neighbors()`.\n\n        Returns:\n            None\n\n        Example:\n            Compute neighbors using default PCA representation:\n                ```python\n                pdata.neighbor(on=\"protein\", layer=\"X\")\n                ```\n\n            Use a custom representation stored in `.obsm[\"X_umap\"]`:\n                ```python\n                pdata.neighbor(on=\"protein\", use_rep=\"X_umap\", n_neighbors=15)\n                ```\n\n        Note:\n            - The neighbor graph is stored in `.obs[\"distances\"]` and `.obs[\"connectivities\"]`.\n            - Neighbor metadata is stored in `.uns[\"neighbors\"]`.\n            - Automatically calls `self.set_X()` if a non-default layer is specified.\n            - PCA is computed automatically if `use_rep='X_pca'` and not already present.\n\n        Todo:\n            Allow users to supply a custom `KNeighborsTransformer` or precomputed neighbor graph.\n                ```python\n                from sklearn.neighbors import KNeighborsTransformer\n                transformer = KNeighborsTransformer(n_neighbors=10, metric='manhattan', algorithm='kd_tree')\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on.lower() in [\"prot\", \"protein\"]:\n            adata = self.prot\n        elif on.lower() in [\"pep\", \"peptide\"]:\n            adata = self.pep\n\n        if layer == \"X\":\n            # do nothing\n            pass\n        elif layer in adata.layers.keys():\n            self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n        log_prefix = format_log_prefix(\"user\") if user_indent == 0 else format_log_prefix(\"user_only\",2)\n        print(f\"{log_prefix} Computing neighbors [{on}] using layer: {layer}\")\n\n        if use_rep == 'X_pca':\n            if 'pca' not in adata.uns:\n                print(f\"{format_log_prefix('info_only',indent=2)} PCA not found in AnnData object. Running PCA with default settings.\")\n                self.pca(on = on, layer = layer)\n        else:\n            if use_rep not in adata.obsm:\n                raise ValueError(f\"PCA key '{use_rep}' not found in obsm. Please run PCA first and specify a valid key.\")\n            print(f\"{format_log_prefix('info_only',indent=2)} Using '{use_rep}' found in obsm for neighbor graph.\")\n\n        if use_rep == 'X_pca':\n            sc.pp.neighbors(adata, **kwargs)\n        else:\n            sc.pp.neighbors(adata, use_rep=use_rep, **kwargs)\n\n        self._append_history(f'{on}: Neighbors fitted on {layer}, using {use_rep}, stored in obs[\"distances\"] and obs[\"connectivities\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only',indent=2)} Neighbors computed on {layer}, using {use_rep}. Results stored in:\")\n        print(f\"       \u2022 obs['distances'] (pairwise distances)\")\n        print(f\"       \u2022 obs['connectivities'] (connectivity graph)\")\n        print(f\"       \u2022 uns['neighbors'] (neighbor graph metadata)\")\n\n    def leiden(self, on = 'protein', layer = \"X\", **kwargs):\n        \"\"\"\n        Perform Leiden clustering on protein or peptide data.\n\n        This method runs community detection using the Leiden algorithm based on a precomputed\n        neighbor graph using `scanpy.tl.leiden()`. If neighbors are not already computed, they will be generated automatically.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use for clustering (default is \"X\").\n            **kwargs: Additional keyword arguments passed to `scanpy.tl.leiden()`.\n\n        Returns:\n            None\n\n        Example:\n            Perform Leiden clustering using the default PCA-based neighbors:\n                ```python\n                pdata.leiden(on=\"protein\", layer=\"X\", resolution=0.25)\n                ```\n\n        Note:\n            - Cluster labels are stored in `.obs[\"leiden\"]`.\n            - Neighbor graphs are automatically computed if not present in `.uns[\"neighbors\"]`.\n            - Automatically sets `.X` to the specified layer if it is not already active.\n        \"\"\"\n        # uses sc.tl.leiden with default resolution of 0.25\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein':\n            adata = self.prot\n        elif on == 'peptide':\n            adata = self.pep\n\n        log_prefix = format_log_prefix(\"user\")\n        print(f\"{log_prefix} Performing Leiden clustering [{on}] using layer: {layer}\")\n\n        if 'resolution' in kwargs:\n            resolution = kwargs.pop(\"resolution\", 0.25)\n\n        if 'neighbors' not in adata.uns:\n            print(f\"{format_log_prefix('info_only', indent=2)} Neighbors not found in AnnData object. Running neighbors with default settings.\")\n            self.neighbor(on = on, layer = layer, **kwargs)\n\n        if layer == \"X\":\n            # do nothing\n            pass\n        elif layer in adata.layers.keys():\n            self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n        sc.tl.leiden(adata, resolution)\n\n        self._append_history(f'{on}: Leiden clustering fitted on {layer}, stored in obs[\"leiden\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only', indent=2)} Leiden clustering complete. Results stored in:\")\n        print(f\"       \u2022 obs['leiden'] (cluster labels)\")\n\n    def umap(self, on = 'protein', layer = \"X\", **kwargs):\n        \"\"\"\n        Compute UMAP dimensionality reduction on protein or peptide data.\n\n        This method runs UMAP (Uniform Manifold Approximation and Projection) on the selected data layer using `scanpy.tl.umap()`.\n        If neighbor graphs are not already computed, they will be generated automatically.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use for UMAP (default is \"X\").\n            **kwargs: Additional keyword arguments passed to `scanpy.tl.umap()`, `scanpy.tl.neighbor()` or the scpviz `pca` function.\n                Example:\n                    \"n_neighbors\": neighbor argument\n                    \"min_dist\": umap argument\n                    \"metric\": neighbor argument\n                    \"spread\": umap argument\n                    \"random_state\": umap argument\n                    \"n_pcs\": neighbor argument\n\n        Returns:\n            None\n\n        Example:\n            Run UMAP using default settings:\n                ```python\n                pdata.umap(on=\"protein\", layer=\"X\")\n                ```\n        Note:\n            - UMAP coordinates are stored in `.obsm[\"X_umap\"]`.\n            - UMAP settings are stored in `.uns[\"umap\"]`.\n            - Automatically computes neighbor graphs if not already available.\n            - Will call `.set_X()` if a non-default layer is used.\n        \"\"\"\n        # uses sc.tl.umap\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein':\n            adata = self.prot\n        elif on == 'peptide':\n            adata = self.pep\n\n        log_prefix = format_log_prefix(\"user\")\n        print(f\"{log_prefix} Computing UMAP [{on}] using layer: {layer}\")\n\n        if \"n_neighbors\" in kwargs or \"metric\" in kwargs or \"n_pcs\" in kwargs:\n                    n_neighbors = kwargs.pop(\"n_neighbors\", None)\n                    metric = kwargs.pop(\"metric\", None)\n                    n_pcs = kwargs.pop(\"n_pcs\", None)\n\n                    # Prepare a readable message\n                    neighbor_args = []\n                    if n_neighbors is not None:\n                        neighbor_args.append(f\"n_neighbors={n_neighbors}\")\n                    else:\n                        n_neighbors = 15  # default value\n                    if metric is not None:\n                        neighbor_args.append(f\"metric='{metric}'\")\n                    else:\n                        metric = \"euclidean\"  # default value\n                    if n_pcs is not None:\n                        neighbor_args.append(f\"n_pcs={n_pcs}\")\n                    else:\n                        n_pcs = 50\n                    arg_str = \", \".join(neighbor_args)\n\n                    print(f\"{format_log_prefix('info_only', indent=2)} {arg_str} provided. \"\n                        f\"Re-running neighbors with these settings before UMAP.\")\n\n                    self.neighbor(on=on, layer=layer, n_neighbors=n_neighbors, metric=metric, user_indent=2)\n                    self._append_history(f\"{on}: Neighbors re-computed with {arg_str} before UMAP\")  # type: ignore[attr-defined], HistoryMixin\n        else:\n            # check if neighbor has been run before, look for distances and connectivities in obsp\n            if 'neighbors' not in adata.uns:\n                print(f\"{format_log_prefix('info_only', indent=2)} Neighbors not found in AnnData object. Running neighbors with default settings.\")\n                self.neighbor(on = on, layer = layer)\n                self._append_history(f\"{on}: Neighbors computed with default settings before UMAP\")  # type: ignore[attr-defined], HistoryMixin\n            else:\n                print(f\"{format_log_prefix('info_only', indent=2)} Using existing neighbors found in AnnData object.\")\n\n        if layer == \"X\":\n            # do nothing\n            pass\n        elif layer in adata.layers.keys():\n            self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n        sc.tl.umap(adata, **kwargs)\n\n        self._append_history(f'{on}: UMAP fitted on {layer}, stored in obsm[\"X_umap\"] and uns[\"umap\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only', indent=2)} UMAP complete. Results stored in:\")\n        print(f\"       \u2022 obsm['X_umap'] (UMAP coordinates)\")\n        print(f\"       \u2022 uns['umap'] (UMAP settings)\")\n\n    def pca(self, on = 'protein', layer = \"X\", **kwargs):\n        \"\"\"\n        Perform PCA (Principal Component Analysis) on protein or peptide data.\n\n        This method performs PCA on the selected data layer, after z-score normalization and removal of\n        NaN-containing features. The results are stored in `.obsm[\"X_pca\"]` and `.uns[\"pca\"]`.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use for PCA (default is \"X\").\n            **kwargs: Additional keyword arguments passed to `scanpy.tl.pca()`. For example,\n                `key_added` to store PCA in a different key.\n\n        Returns:\n            None\n\n        Note:\n            - Features (columns) with NaN values are excluded before PCA and then padded with zeros.\n            - PCA scores are stored in `.obsm['X_pca']`.\n            - Principal component loadings, variance ratios, and total variances are stored in `.uns['pca']`.\n            - If you store PCs under a custom key using `key_added`, remember to set `use_rep` when calling `.neighbor()` or `.umap()`.\n        \"\"\"\n\n        # uses sc.tl.pca\n        # for kwargs can use key_added to store PCA in a different key - then for neighbors need to specify key by use_rep\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein':\n            adata = self.prot\n        elif on == 'peptide':\n            adata = self.pep\n\n        # make sample array\n        if layer == \"X\":\n            X = adata.X.toarray()\n        elif layer in adata.layers.keys():\n            X = adata.layers[layer].toarray()\n\n        log_prefix = format_log_prefix(\"user\")\n        print(f\"{log_prefix} Performing PCA [{on}] using layer: {layer}, removing NaN features.\")\n        print(f\"   \ud83d\udd38 BEFORE (samples \u00d7 proteins): {X.shape}\")\n        Xnorm = (X - X.mean(axis=0)) / X.std(axis=0)\n        nan_cols = np.isnan(Xnorm).any(axis=0)\n        Xnorm = Xnorm[:, ~nan_cols]\n        print(f\"   \ud83d\udd38 AFTER  (samples \u00d7 proteins): {Xnorm.shape}\")\n\n        # TODO: fix bug here (ValueError: n_components=59 must be between 1 and min(n_samples, n_features)=31 with svd_solver='arpack')\n        pca_data = sc.tl.pca(Xnorm, return_info=True, **kwargs)\n        adata.obsm['X_pca'] = pca_data[0]\n        PCs = np.zeros((pca_data[1].shape[0], nan_cols.shape[0]))\n\n        # fill back the 0s where column was NaN in the original data, and thus not used in PCA\n        counter = 0\n        for i in range(PCs.shape[1]):\n            if not nan_cols[i]:\n                PCs[:, i] = pca_data[1][:, counter]\n                counter += 1\n\n        adata.uns['pca'] = {'PCs': PCs, 'variance_ratio': pca_data[2], 'variance': pca_data[3]}\n\n        subpdata = \"prot\" if on == 'protein' else \"pep\"\n\n        self._append_history(f'{on}: PCA fitted on {layer}, stored in obsm[\"X_pca\"] and varm[\"PCs\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only',indent=2)} PCA complete, fitted on {layer}. Results stored in:\")\n        print(f\"       \u2022 .{subpdata}.obsm['X_pca']\")\n        print(f\"       \u2022 .{subpdata}.uns['pca'] (includes PCs, variance, variance ratio)\")\n        var_pc1, var_pc2 = pca_data[2][:2]\n        print(f\"       \u2022 Variance explained by PC1/PC2: {var_pc1*100:.2f}% , {var_pc2*100:.2f}%\") \n\n    def harmony(self, key, on = 'protein'):\n        \"\"\"\n        Perform batch correction using Harmony integration.\n\n        This method applies Harmony-based batch correction (via `scanpy.external.pp.harmony_integrate`)\n        on PCA-reduced protein or peptide data to mitigate batch effects across samples.\n\n        Args:\n            key (str): Column name in `.obs` representing the batch variable to correct.\n            on (str): Whether to use \"protein\" or \"peptide\" data. Accepts \"prot\"/\"protein\" or \"pep\"/\"peptide\" (default: \"protein\").\n\n        Returns:\n            None\n\n        Example:\n            Perform Harmony integration on protein-level PCA embeddings:\n                ```python\n                pdata.harmony(key=\"batch\", on=\"protein\")\n                ```\n\n            Apply Harmony on peptide-level data instead:\n                ```python\n                pdata.harmony(key=\"run_id\", on=\"peptide\")\n                ```\n\n        Note:\n            - Harmony requires prior PCA computation. If PCA is missing, it will be computed automatically.\n            - The Harmony-corrected coordinates are stored in `.obsm[\"X_pca_harmony\"]`.\n            - Updates the processing history via `.history`.\n\n        Todo:\n            Add optional arguments for controlling Harmony parameters (e.g., `max_iter_harmony`, `theta`, `lambda`).\n        \"\"\"\n\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein' or on == 'prot':\n            adata = self.prot\n        elif on == 'peptide' or on == 'pep':\n            adata = self.pep\n\n        log_prefix = format_log_prefix(\"user\")\n        print(f\"{log_prefix} Performing Harmony batch correction on [{on}] PCA.\")\n\n        # check if pca has been run before, look for distances and connectivities in obsp\n        if 'pca' not in adata.uns:\n            print(f\"{format_log_prefix('info_only', indent=2)} PCA not found in AnnData object. Running PCA with default settings.\")\n            self.pca(on = on, layer = \"X\")\n\n        # check that key is valid column in adata.obs\n        if key not in adata.obs.columns:\n            raise ValueError(f\"Batch key '{key}' not found in adata.obs.\")\n\n        sc.external.pp.harmony_integrate(adata, key)\n\n        self._append_history(f'{on}: Harmony batch correction applied on key {key}, stored in obsm[\"X_pca_harmony\"] and uns[\"umap\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only', indent=2)} Harmony batch correction complete. Results stored in:\")\n        print(f\"       \u2022 obsm['X_pca_harmony'] (PCA coordinates)\")\n\n    def nanmissingvalues(self, on = 'protein', limit = 0.5):\n        \"\"\"\n        Set columns (proteins or peptides) with excessive missing values to NaN.\n\n        This method scans all features and replaces their corresponding columns with NaN\n        if the fraction of missing values exceeds the given threshold. It helps ensure\n        downstream normalization and imputation steps are applied to meaningful features only.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data. Accepts \"prot\"/\"protein\" or \"pep\"/\"peptide\" (default: \"protein\").\n            limit (float): Proportion threshold for missing values (default: 0.5). \n                Features with more than `limit \u00d7 100%` missing values are set entirely to NaN.\n\n        Returns:\n            None\n\n        !!! warning \"Deprecation Notice\"\n            This function may be deprecated in future releases.  \n            Use [`annotate_found`](reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing_mixins.annotate_found)  \n            and [`filter_prot_found`](reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing_mixins.filter_prot_found)  \n            for more robust and configurable detection-based filtering.\n\n        Example:\n            Mask proteins with more than 50% missing values:\n                ```python\n                pdata.nanmissingvalues(on=\"protein\", limit=0.5)\n                ```\n\n            Apply the same filter for peptide-level data:\n                ```python\n                pdata.nanmissingvalues(on=\"peptide\", limit=0.3)\n                ```\n\n        Note:\n            - The missing-value fraction is computed per feature across all samples.\n            - This operation modifies the `.X` matrix in-place.\n            - The updated data are stored back into `.prot` or `.pep`.\n        \"\"\"\n        import scipy.sparse\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein':\n            adata = self.prot\n\n        elif on == 'peptide':\n            adata = self.pep\n\n        if scipy.sparse.issparse(adata.X):\n            X = adata.X.toarray()\n        else:\n            X = adata.X\n        missing_proportion = np.isnan(X).mean(axis=0)\n        columns_to_nan = missing_proportion &gt; limit\n        X[:, columns_to_nan] = np.nan\n        adata.X = scipy.sparse.csr_matrix(X) if scipy.sparse.issparse(adata.X) else X\n\n        if on == 'protein':\n            self.prot = adata\n        elif on == 'peptide':\n            self.pep = adata\n\n    def normalize(self, classes = None, layer = \"X\", method = 'sum', on = 'protein', set_X = True, force = False, use_nonmissing = False, **kwargs):  \n        \"\"\"\n        Normalize sample intensities across protein or peptide data.\n\n        This method performs global or group-wise normalization of the selected data layer.\n        It supports multiple normalization strategies ranging from simple scaling\n        (e.g., sum, median) to advanced approaches such as `reference_feature` and\n        [`directlfq`]((https://doi.org/10.1016/j.mcpro.2023.100581)).\n\n        Args:\n            classes (str or list, optional): Sample-level grouping column(s) in `.obs` to\n                perform group-wise normalization. If None, normalization is applied globally.\n            layer (str, optional): Data layer to normalize from (default: `\"X\"`).\n            method (str, optional): Normalization strategy to apply. Options include:\n                `'sum'`, `'median'`, `'mean'`, `'max'`, `'reference_feature'`,\n                `'robust_scale'`, `'quantile_transform'`, `'directlfq'`.\n            on (str, optional): Whether to use `\"protein\"` or `\"peptide\"` data.\n            set_X (bool, optional): Whether to set `.X` to the normalized result (default: True).\n            force (bool, optional): Proceed with normalization even if samples exceed the\n                allowed fraction of missing values (default: False).\n            use_nonmissing (bool, optional): If True, only use columns with no missing values\n                across all samples when computing scaling factors (default: False).\n            **kwargs: Additional keyword arguments for normalization methods.\n                - `reference_columns` (list): For `'reference_feature'`, specify columns or\n                gene names to normalize against.\n                - `max_missing_fraction` (float): Maximum allowed fraction of missing values\n                per sample (default: 0.5).\n                - `n_neighbors` (int): For methods requiring neighbor-based computations.\n                - `input_type_to_use` (str): For `'directlfq'`, specify `'pAnnData'`,\n                `'diann_precursor_ms1'`, or `'diann_precursor_ms1_and_ms2'`.\n                - `path` (str): For `'directlfq'`, path to the `report.tsv` or `report.parquet`\n                file from DIA-NN output.\n\n        Returns:\n            None\n\n        Example:\n            Perform global normalization using the median intensity:\n                ```python\n                pdata.normalize(on=\"protein\", method=\"median\")\n                ```\n\n            Apply group-wise normalization by treatment class using sum-scaling:\n                ```python\n                pdata.normalize(classes=\"treatment\", method=\"sum\", on=\"protein\")\n                ```\n\n            Run reference-feature normalization using specific genes:\n                ```python\n                pdata.normalize(\n                    on=\"protein\",\n                    method=\"reference_feature\",\n                    reference_columns=[\"ACTB\", \"GAPDH\"]\n                )\n                ```\n\n        !!! tip \"About `directlfq` normalization\"\n            - The `directlfq` method aggregates peptide-level data to protein-level intensities\n            and stores results in a new protein-layer (e.g. `'X_norm_directlfq'`).\n            - It does not support group-wise normalization.\n            - Processing time may scale with dataset size.\n            - For algorithmic and benchmarking details, see:  \n            **Ammar, Constantin et al. (2023)**  \n            *Accurate Label-Free Quantification by directLFQ to Compare Unlimited Numbers of Proteomes.*  \n            *Molecular &amp; Cellular Proteomics*, 22(7):100581.  \n            [https://doi.org/10.1016/j.mcpro.2023.100581](https://doi.org/10.1016/j.mcpro.2023.100581)\n\n\n\n        Note:\n            - Results are stored in a new layer named `'X_norm_&lt;method&gt;'`.\n            - The normalized layer replaces `.X` if `set_X=True`.\n            - Normalization operations are recorded in `.history`.\n            - For consistency across runs, consider running `.impute()` before normalization.\n\n        Todo:\n            - Add optional z-score and percentile normalization modes.\n            - Add support for specifying external scaling factors.\n        \"\"\"\n\n\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            return\n\n        adata = self.prot if on == 'protein' else self.pep\n        if layer != \"X\" and layer not in adata.layers:\n            raise ValueError(f\"Layer {layer} not found in .{on}.\")\n\n        normalize_data = adata.layers[layer] if layer != \"X\" else adata.X\n        was_sparse = sparse.issparse(normalize_data)\n        normalize_data = normalize_data.toarray() if was_sparse else normalize_data.copy()\n        original_data = normalize_data.copy()\n\n        layer_name = 'X_norm_' + method\n        normalize_funcs = ['sum', 'median', 'mean', 'max', 'reference_feature', 'robust_scale', 'quantile_transform','directlfq']\n\n        if method not in normalize_funcs:\n            raise ValueError(f\"Unsupported normalization method: {method}\")\n\n        # Special handling for directlfq\n        if method == \"directlfq\":\n            if classes is not None:\n                print(f\"{format_log_prefix('warn')} 'directlfq' does not support group-wise normalization. Proceeding with global normalization.\")\n                classes = None\n\n            print(f\"{format_log_prefix('user')} Running directlfq normalization on peptide-level data.\")\n            print(f\"{format_log_prefix('info_only', indent=2)} Note: please be patient, directlfq can take a minute to run depending on data size. Output files will be produced.\")\n            normalize_data = self._normalize_helper_directlfq(**kwargs)\n\n            adata = self.prot  # directlfq always outputs protein-level intensities\n            adata.layers[layer_name] = sparse.csr_matrix(normalize_data) if was_sparse else normalize_data\n\n            if set_X:\n                self.set_X(layer=layer_name, on=\"protein\")  # type: ignore[attr-defined]\n\n            self._history.append(  # type: ignore[attr-defined]\n                f\"protein: Normalized layer using directlfq (input_type={kwargs.get('input_type_to_use', 'default')}). Stored in `{layer_name}`.\"\n            )\n            print(f\"{format_log_prefix('result_only', indent=2)} directlfq normalization complete. Results are stored in layer '{layer_name}'.\")\n            return\n\n        # --- standard normalization ---\n        # Build the header message early\n        if classes is None:\n            msg = f\"{format_log_prefix('user')} Global normalization using '{method}'\"\n        else:\n            msg = f\"{format_log_prefix('info_only')} Group-wise normalization using '{method}' on class(es): {classes}\"\n\n        if use_nonmissing and method in {'sum', 'mean', 'median', 'max'}:\n            msg += \" (using only fully observed columns)\"\n        msg += f\". Layer will be saved as '{layer_name}'.\"\n\n        # \u2705 Print message before checking for missing values\n        print(msg)\n\n        # Check for bad rows (too many missing values)\n        missing_fraction = np.isnan(normalize_data).sum(axis=1) / normalize_data.shape[1]\n        max_missing_fraction = kwargs.pop(\"max_missing_fraction\", 0.5)\n        bad_rows_mask = missing_fraction &gt; max_missing_fraction\n\n        if np.any(bad_rows_mask):\n            n_bad = np.sum(bad_rows_mask)\n            print(f\"{format_log_prefix('warn',2)} {n_bad} sample(s) have &gt;{int(max_missing_fraction*100)}% missing values.\")\n            print(\"     Try running `.impute()` before normalization. Suggest to use the flag `use_nonmissing=True` to normalize using only consistently observed proteins.\")\n            if not force:\n                print(\"     \u27a1\ufe0f Use `force=True` to proceed anyway.\")\n                return\n            print(f\"{format_log_prefix('warn',2)} Proceeding with normalization despite bad rows (force=True).\")\n\n        if classes is None:\n            normalize_data = self._normalize_helper(normalize_data, method, use_nonmissing=use_nonmissing, **kwargs)\n        else:\n            # Group-wise normalization\n            sample_names = utils.get_samplenames(adata, classes)\n            sample_names = np.array(sample_names)\n            unique_groups = np.unique(sample_names)\n\n            for group in unique_groups:\n                idx = np.where(sample_names == group)[0]\n                group_data = normalize_data[idx, :]\n\n                normalized_group = self._normalize_helper(group_data, method=method, use_nonmissing=use_nonmissing, **kwargs)\n                normalize_data[idx, :] = normalized_group\n\n        # summary printout\n        summary_lines = []\n        if classes is None:\n            summary_lines.append(f\"{format_log_prefix('result_only', indent=2)} Normalized all {normalize_data.shape[0]} samples.\")\n        else:\n            for group in unique_groups:\n                count = np.sum(sample_names == group)\n                summary_lines.append(f\"   - {group}: {count} samples normalized\")\n            summary_lines.insert(0, f\"{format_log_prefix('result_only', indent=2)} Normalized {normalize_data.shape[0]} samples total.\")\n        print(\"\\n\".join(summary_lines))\n\n        adata.layers[layer_name] = sparse.csr_matrix(normalize_data) if was_sparse else normalize_data\n\n        if set_X:\n            self.set_X(layer = layer_name, on = on) # type: ignore[attr-defined], EditingMixin\n\n        # Determine if use_nonmissing note should be added\n        note = \"\"\n        if use_nonmissing and method in {'sum', 'mean', 'median', 'max'}:\n            note = \" (using only fully observed columns)\"\n\n        self._history.append( # type: ignore[attr-defined], HistoryMixin\n            f\"{on}: Normalized layer {layer} using {method}{note} (grouped by {classes}). Stored in `{layer_name}`.\"\n            )\n\n    def _normalize_helper(self, data, method, use_nonmissing, **kwargs):\n        \"\"\"\n        Perform row-wise normalization using a selected method.\n\n        Used internally by `normalize()` to compute per-sample scaling.\n        Supports reference feature scaling, robust methods, and quantile normalization.\n\n        Args:\n            data (np.ndarray): Sample \u00d7 feature data matrix.\n            method (str): Normalization strategy. Options:\n                - 'sum'\n                - 'mean'\n                - 'median'\n                - 'max'\n                - 'reference_feature'\n                - 'robust_scale'\n                - 'quantile_transform'\n            use_nonmissing (bool): If True, computes scaling using only columns with no NaNs.\n\n        Returns:\n            np.ndarray: Normalized data matrix.\n        \"\"\"\n\n        if method in {'sum', 'mean', 'median', 'max'}:\n            reducer = {\n                    'sum': np.nansum,\n                    'mean': np.nanmean,\n                    'median': np.nanmedian,\n                    'max': np.nanmax\n                }[method]\n\n            if use_nonmissing:\n                fully_observed_cols = ~np.isnan(data).any(axis=0)\n                if not np.any(fully_observed_cols):\n                    raise ValueError(\"No fully observed columns available for normalization with `use_nonmissing=True`.\")\n                used_cols = np.where(fully_observed_cols)[0]\n                print(f\"{format_log_prefix('info_only',2)} Normalizing using only fully observed columns: {len(used_cols)}\")\n                row_vals = reducer(data[:, fully_observed_cols], axis=1)\n            else:\n                row_vals = reducer(data, axis=1)\n\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale = np.nanmax(row_vals) / row_vals\n            scale = np.where(np.isnan(scale), 1.0, scale) # metaboanalyst: scale = 1.0 / row_vals\n            data_norm = data * scale[:, None]\n\n        elif method == 'reference_feature':\n            # norm by reference feature: scale each row s.t. the reference column is the same across all rows (scale to max value of reference column)\n            reference_columns = kwargs.get('reference_columns', [2])\n            reference_method = kwargs.get('reference_method', 'median')  # default to median\n\n            reducer_map = {\n                'mean': np.nanmean,\n                'median': np.nanmedian,\n                'sum': np.nansum\n            }\n\n            if reference_method not in reducer_map:\n                raise ValueError(f\"Unsupported reference method: {reference_method}. Supported methods are: {list(reducer_map.keys())}\")\n            reducer = reducer_map[reference_method]\n\n            # resolve reference column names if needed\n            if isinstance(reference_columns[0], str):\n                gene_to_acc, _ = self.get_gene_maps(on='protein') # type: ignore[attr-defined], IdentifierMixin\n                resolved = utils.resolve_accessions(self.prot, reference_columns, gene_map=gene_to_acc)\n                reference_acc = [ref for ref in resolved if ref in self.prot.var.index]\n                reference_columns = [self.prot.var.index.get_loc(ref) for ref in reference_acc]\n                print(f\"{format_log_prefix('info')} Normalizing using found reference columns: {reference_acc}\")\n                self._history.append(f\"Used reference_feature normalization with resolved accessions: {resolved}\") # type: ignore[attr-defined]\n            else:\n                reference_columns = [int(ref) for ref in reference_columns]\n                reference_acc = [self.prot.var.index[ref] for ref in reference_columns if ref &lt; self.prot.shape[1]]\n                print(f\"{format_log_prefix('info')} Normalizing using reference columns: {reference_acc}\")\n                self._history.append(f\"Used reference_feature normalization with resolved accessions: {reference_acc}\") # type: ignore[attr-defined]\n\n            scaling_factors = np.nanmean(np.nanmax(data[:, reference_columns], axis=0) / (data[:, reference_columns]), axis=1)\n\n            nan_rows = np.where(np.isnan(scaling_factors))[0]\n            if nan_rows.size &gt; 0:\n                print(f\"{format_log_prefix('warn')} Rows {list(nan_rows)} have all missing reference values.\")\n                print(f\"{format_log_prefix('info')} Falling back to row median normalization for these rows.\")\n\n                fallback = np.nanmedian(data[nan_rows, :], axis=1)\n                fallback[fallback == 0] = np.nan  # avoid division by 0\n                fallback_scale = np.nanmax(fallback) / fallback\n                fallback_scale = np.where(np.isnan(fallback_scale), 1.0, fallback_scale)  # default to 1.0 if all else fails\n\n                scaling_factors[nan_rows] = fallback_scale\n\n            scaling_factors = np.where(np.isnan(scaling_factors), np.nanmean(scaling_factors), scaling_factors)\n            data_norm = data * scaling_factors[:, None]\n\n        elif method == 'robust_scale':\n            # norm by robust_scale: Center to the median and component wise scale according to the interquartile range. See sklearn.preprocessing.robust_scale for more information.\n            from sklearn.preprocessing import robust_scale\n            data_norm = robust_scale(data, axis=1)\n\n        elif method == 'quantile_transform':\n            # norm by quantile_transform: Transform features using quantiles information. See sklearn.preprocessing.quantile_transform for more information.\n            from sklearn.preprocessing import quantile_transform\n            import warnings\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", UserWarning)\n                data_norm = quantile_transform(data, axis=1)\n\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        return data_norm\n\n    def _normalize_helper_directlfq(self, input_type_to_use=\"pAnnData\", path=None, **kwargs):\n        \"\"\"\n        Run directlfq normalization and return normalized protein-level intensities.\n\n        Args:\n            input_type_to_use (str): Either 'pAnnData' (default) or \n                'diann_precursor_ms1_and_ms2'.\n            path (str, optional): Path to DIA-NN report file (required if \n                input_type_to_use='diann_precursor_ms1_and_ms2').\n            **kwargs: Passed to directlfq.lfq_manager.run_lfq().\n\n        Returns:\n            np.ndarray: Normalized data (samples \u00d7 proteins).\n        \"\"\"\n        import directlfq.lfq_manager as lfq_manager\n        import os\n\n        if input_type_to_use == \"diann_precursor_ms1_and_ms2\":\n            if path is None:\n                raise ValueError(\"For input_type_to_use='diann_precursor_ms1_and_ms2', please provide the DIA-NN report path via `path`.\")\n            lfq_manager.run_lfq(path, input_type_to_use=input_type_to_use, **kwargs)\n\n        else:\n            # check if pep exists\n            if self.pep is None:\n                raise ValueError(\"Peptide-level data not found. Please load peptide data before running directlfq normalization.\")\n\n            # Build peptide-level input table from .pep\n            X = self.pep.layers.get(\"X_precursor\", self.pep.X)\n            if not isinstance(X, pd.DataFrame):\n                X = X.toarray() if hasattr(X, \"toarray\") else X\n            X_df = pd.DataFrame(\n                X.T,\n                index=self.pep.var_names,\n                columns=self.pep.obs_names\n            )\n            prot_col = \"Protein.Group\" if \"Protein.Group\" in self.pep.var.columns else \"Master Protein Accessions\"\n            X_df.insert(0, \"protein\", self.pep.var[prot_col].to_list())\n            X_df.insert(1, \"ion\", X_df.index.to_list())\n            X_df.reset_index(drop=True, inplace=True)\n            tmp_file = \"peptide_matrix.aq_reformat.tsv\"\n            X_df.to_csv(tmp_file, sep=\"\\t\", index=False)\n            lfq_manager.run_lfq(tmp_file, **kwargs)\n\n        # Load directlfq output (look for protein_intensities file)\n        out_file = None\n        for f in os.listdir(\".\"):\n            if f.endswith(\"protein_intensities.tsv\"):\n                out_file = f\n        if out_file is None:\n            raise FileNotFoundError(\"directlfq did not produce a '*protein_intensities.tsv' file in current directory.\")\n\n        norm_prot = pd.read_csv(out_file, sep=\"\\t\").set_index(\"protein\")\n        aligned = norm_prot.reindex(\n            index=self.prot.var_names,\n            columns=self.prot.obs_names\n        ).fillna(0)\n\n        return aligned.T.to_numpy()\n\n    def clean_X(self, on='prot', inplace=True, set_to=0, layer=None, to_sparse=False, backup_layer=\"X_preclean\", verbose=True):\n        \"\"\"\n        Replace NaNs in `.X` or a specified layer with a given value (default: 0).\n\n        Optionally backs up the original data to a layer (default: `'X_preclean'`) before overwriting.\n        Typically used to prepare data for scanpy or sklearn functions that cannot handle missing values.\n\n        Args:\n            on (str): Target data to clean, either `'protein'` or `'peptide'`.\n            inplace (bool): If True, update `.X` or `.layers[layer]` in place. If False, return cleaned matrix.\n            set_to (float): Value to replace NaNs with (default: 0.0).\n            layer (str or None): If specified, applies to `.layers[layer]`; otherwise uses `.X`.\n            to_sparse (bool): If True, returns a sparse matrix.\n            backup_layer (str or None): If `inplace=True` and `layer=None`, saves the original `.X` to this layer.\n            verbose (bool): Whether to print summary messages.\n\n        Returns:\n            np.ndarray: Cleaned matrix if `inplace=False`, otherwise `None`.\n        \"\"\"\n        if not self._check_data(on):\n            return\n        if on == 'prot' or on == 'protein':\n            adata = self.prot\n        elif on == 'pep' or on == 'peptide': \n            adata = self.pep\n\n        print(f'{format_log_prefix(\"user\")} Cleaning {on} data: making scanpy compatible, replacing NaNs with {set_to} in {\"layer \" + layer if layer else \".X\"}.')\n\n        # Choose source matrix\n        X = adata.layers[layer] if layer else adata.X\n        is_sparse = sparse.issparse(X)\n\n        # Copy for manipulation\n        X_clean = X.copy()\n        nan_count = 0\n\n        if is_sparse:\n            nan_mask = np.isnan(X_clean.data)\n            nan_count = np.sum(nan_mask)\n            if nan_count &gt; 0:\n                X_clean.data[nan_mask] = set_to\n        else:\n            nan_mask = np.isnan(X_clean)\n            nan_count = np.sum(nan_mask)\n            X_clean[nan_mask] = set_to\n\n        if to_sparse and not is_sparse:\n            X_clean = sparse.csr_matrix(X_clean)\n\n        # Apply result\n        if inplace:\n            if layer:\n                self.prot.layers[layer] = X_clean\n            else:\n                # Save original .X if requested and not already backed up\n                if backup_layer and backup_layer not in self.prot.layers:\n                    self.prot.layers[backup_layer] = self.prot.X.copy()\n                    if verbose:\n                        print(f\"{format_log_prefix('info')} Backed up .X to .layers['{backup_layer}']\")\n                self.prot.X = X_clean\n            if verbose:\n                print(f\"{format_log_prefix('result')} Cleaned {'layer ' + layer if layer else '.X'}: replaced {nan_count} NaNs with {set_to}.\")\n        else:\n            if verbose:\n                print(f\"{format_log_prefix('result')} Returning cleaned matrix: {nan_count} NaNs replaced with {set_to}.\")\n            return X_clean \n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.clean_X","title":"clean_X","text":"<pre><code>clean_X(on='prot', inplace=True, set_to=0, layer=None, to_sparse=False, backup_layer='X_preclean', verbose=True)\n</code></pre> <p>Replace NaNs in <code>.X</code> or a specified layer with a given value (default: 0).</p> <p>Optionally backs up the original data to a layer (default: <code>'X_preclean'</code>) before overwriting. Typically used to prepare data for scanpy or sklearn functions that cannot handle missing values.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Target data to clean, either <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'prot'</code> <code>inplace</code> <code>bool</code> <p>If True, update <code>.X</code> or <code>.layers[layer]</code> in place. If False, return cleaned matrix.</p> <code>True</code> <code>set_to</code> <code>float</code> <p>Value to replace NaNs with (default: 0.0).</p> <code>0</code> <code>layer</code> <code>str or None</code> <p>If specified, applies to <code>.layers[layer]</code>; otherwise uses <code>.X</code>.</p> <code>None</code> <code>to_sparse</code> <code>bool</code> <p>If True, returns a sparse matrix.</p> <code>False</code> <code>backup_layer</code> <code>str or None</code> <p>If <code>inplace=True</code> and <code>layer=None</code>, saves the original <code>.X</code> to this layer.</p> <code>'X_preclean'</code> <code>verbose</code> <code>bool</code> <p>Whether to print summary messages.</p> <code>True</code> <p>Returns:</p> Type Description <p>np.ndarray: Cleaned matrix if <code>inplace=False</code>, otherwise <code>None</code>.</p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def clean_X(self, on='prot', inplace=True, set_to=0, layer=None, to_sparse=False, backup_layer=\"X_preclean\", verbose=True):\n    \"\"\"\n    Replace NaNs in `.X` or a specified layer with a given value (default: 0).\n\n    Optionally backs up the original data to a layer (default: `'X_preclean'`) before overwriting.\n    Typically used to prepare data for scanpy or sklearn functions that cannot handle missing values.\n\n    Args:\n        on (str): Target data to clean, either `'protein'` or `'peptide'`.\n        inplace (bool): If True, update `.X` or `.layers[layer]` in place. If False, return cleaned matrix.\n        set_to (float): Value to replace NaNs with (default: 0.0).\n        layer (str or None): If specified, applies to `.layers[layer]`; otherwise uses `.X`.\n        to_sparse (bool): If True, returns a sparse matrix.\n        backup_layer (str or None): If `inplace=True` and `layer=None`, saves the original `.X` to this layer.\n        verbose (bool): Whether to print summary messages.\n\n    Returns:\n        np.ndarray: Cleaned matrix if `inplace=False`, otherwise `None`.\n    \"\"\"\n    if not self._check_data(on):\n        return\n    if on == 'prot' or on == 'protein':\n        adata = self.prot\n    elif on == 'pep' or on == 'peptide': \n        adata = self.pep\n\n    print(f'{format_log_prefix(\"user\")} Cleaning {on} data: making scanpy compatible, replacing NaNs with {set_to} in {\"layer \" + layer if layer else \".X\"}.')\n\n    # Choose source matrix\n    X = adata.layers[layer] if layer else adata.X\n    is_sparse = sparse.issparse(X)\n\n    # Copy for manipulation\n    X_clean = X.copy()\n    nan_count = 0\n\n    if is_sparse:\n        nan_mask = np.isnan(X_clean.data)\n        nan_count = np.sum(nan_mask)\n        if nan_count &gt; 0:\n            X_clean.data[nan_mask] = set_to\n    else:\n        nan_mask = np.isnan(X_clean)\n        nan_count = np.sum(nan_mask)\n        X_clean[nan_mask] = set_to\n\n    if to_sparse and not is_sparse:\n        X_clean = sparse.csr_matrix(X_clean)\n\n    # Apply result\n    if inplace:\n        if layer:\n            self.prot.layers[layer] = X_clean\n        else:\n            # Save original .X if requested and not already backed up\n            if backup_layer and backup_layer not in self.prot.layers:\n                self.prot.layers[backup_layer] = self.prot.X.copy()\n                if verbose:\n                    print(f\"{format_log_prefix('info')} Backed up .X to .layers['{backup_layer}']\")\n            self.prot.X = X_clean\n        if verbose:\n            print(f\"{format_log_prefix('result')} Cleaned {'layer ' + layer if layer else '.X'}: replaced {nan_count} NaNs with {set_to}.\")\n    else:\n        if verbose:\n            print(f\"{format_log_prefix('result')} Returning cleaned matrix: {nan_count} NaNs replaced with {set_to}.\")\n        return X_clean \n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.cv","title":"cv","text":"<pre><code>cv(classes=None, on='protein', layer='X', debug=False)\n</code></pre> <p>Compute the coefficient of variation (CV) for each feature across sample groups.</p> <p>This method calculates CV for each protein or peptide across all samples in each group, storing the result as new columns in <code>.var</code>, one per group.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>str or list of str</code> <p>Sample-level class or list of classes used to define groups.</p> <code>None</code> <code>on</code> <code>str</code> <p>Whether to compute CV on \"protein\" or \"peptide\" data.</p> <code>'protein'</code> <code>layer</code> <code>str</code> <p>Data layer to use for computation (default is \"X\").</p> <code>'X'</code> <code>debug</code> <code>bool</code> <p>If True, prints debug information while filtering groups.</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Compute per-group CV for proteins using a custom normalization layer:     <pre><code>pdata.cv(classes=[\"group\", \"condition\"], on=\"protein\", layer=\"X_norm\")\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def cv(self, classes = None, on = 'protein', layer = \"X\", debug = False):\n    \"\"\"\n    Compute the coefficient of variation (CV) for each feature across sample groups.\n\n    This method calculates CV for each protein or peptide across all samples in each group,\n    storing the result as new columns in `.var`, one per group.\n\n    Args:\n        classes (str or list of str, optional): Sample-level class or list of classes used to define groups.\n        on (str): Whether to compute CV on \"protein\" or \"peptide\" data.\n        layer (str): Data layer to use for computation (default is \"X\").\n        debug (bool): If True, prints debug information while filtering groups.\n\n    Returns:\n        None\n\n    Example:\n        Compute per-group CV for proteins using a custom normalization layer:\n            ```python\n            pdata.cv(classes=[\"group\", \"condition\"], on=\"protein\", layer=\"X_norm\")\n            ```\n    \"\"\"\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        pass\n\n    adata = self.prot if on == 'protein' else self.pep\n    classes_list = utils.get_classlist(adata, classes)\n\n    for j, class_value in enumerate(classes_list):\n        data_filtered = utils.resolve_class_filter(adata, classes, class_value)\n\n        cv_data = data_filtered.X.toarray() if layer == \"X\" else data_filtered.layers[layer].toarray() if layer in data_filtered.layers else None\n        if cv_data is None:\n            raise ValueError(f\"Layer '{layer}' not found in adata.layers.\")\n\n        adata.var['CV: '+ class_value] = variation(cv_data, axis=0)\n\n    self._history.append(f\"{on}: Coefficient of Variation (CV) calculated for {layer} data by {classes}. CV stored in var['CV: {class_value}'].\") # type: ignore[attr-defined]\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.de","title":"de","text":"<pre><code>de(values=None, class_type=None, method='ttest', layer='X', pval=0.05, log2fc=1.0, fold_change_mode='mean')\n</code></pre> <p>Perform differential expression (DE) analysis on proteins across sample groups.</p> <p>This method compares protein abundance between two sample groups using a specified statistical test and fold change method. Input groups can be defined using either legacy-style (<code>class_type</code> + <code>values</code>) or dictionary-style filters.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list of dict or list of list</code> <p>Sample group filters to compare.</p> <ul> <li>Dictionary-style (recommended): [{'cellline': 'HCT116', 'treatment': 'DMSO'}, {...}]</li> <li>Legacy-style (if <code>class_type</code> is provided): [['HCT116', 'DMSO'], ['HCT116', 'DrugX']]</li> </ul> <code>None</code> <code>class_type</code> <code>str or list of str</code> <p>Legacy-style class label(s) to interpret <code>values</code>.</p> <code>None</code> <code>method</code> <code>str</code> <p>Statistical test to use. Options: \"ttest\", \"mannwhitneyu\", \"wilcoxon\".</p> <code>'ttest'</code> <code>layer</code> <code>str</code> <p>Name of the data layer to use (default is \"X\").</p> <code>'X'</code> <code>pval</code> <code>float</code> <p>P-value cutoff used for labeling significance.</p> <code>0.05</code> <code>log2fc</code> <code>float</code> <p>Minimum log2 fold change threshold for significance labeling.</p> <code>1.0</code> <code>fold_change_mode</code> <code>str</code> <p>Strategy for computing fold change. Options:</p> <ul> <li>\"mean\": log2(mean(group1) / mean(group2))</li> <li>\"pairwise_median\": median of all pairwise log2 ratios</li> <li>\"pep_pairwise_median\": median of peptide-level pairwise log2 ratios, aggregated per protein</li> </ul> <code>'mean'</code> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame with DE statistics including log2 fold change, p-values, and significance labels.</p> Example <p>Legacy-style DE comparison using class types and value combinations:     <pre><code>pdata.de(\n    class_type=[\"cellline\", \"treatment\"],\n    values=[[\"HCT116\", \"DMSO\"], [\"HCT116\", \"DrugX\"]]\n)\n</code></pre></p> <p>Dictionary-style (recommended) DE comparison:     <pre><code>pdata.de(\n    values=[\n        {\"cellline\": \"HCT116\", \"treatment\": \"DMSO\"},\n        {\"cellline\": \"HCT116\", \"treatment\": \"DrugX\"}\n    ]\n)\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def de(self, values=None, class_type=None, method='ttest', layer='X', pval=0.05, log2fc=1.0, fold_change_mode='mean'):\n    \"\"\"\n    Perform differential expression (DE) analysis on proteins across sample groups.\n\n    This method compares protein abundance between two sample groups using a specified\n    statistical test and fold change method. Input groups can be defined using either\n    legacy-style (`class_type` + `values`) or dictionary-style filters.\n\n    Args:\n        values (list of dict or list of list): Sample group filters to compare.\n\n            - Dictionary-style (recommended): [{'cellline': 'HCT116', 'treatment': 'DMSO'}, {...}]\n            - Legacy-style (if `class_type` is provided): [['HCT116', 'DMSO'], ['HCT116', 'DrugX']]\n\n        class_type (str or list of str, optional): Legacy-style class label(s) to interpret `values`.\n\n        method (str): Statistical test to use. Options: \"ttest\", \"mannwhitneyu\", \"wilcoxon\".\n\n        layer (str): Name of the data layer to use (default is \"X\").\n\n        pval (float): P-value cutoff used for labeling significance.\n\n        log2fc (float): Minimum log2 fold change threshold for significance labeling.\n\n        fold_change_mode (str): Strategy for computing fold change. Options:\n\n            - \"mean\": log2(mean(group1) / mean(group2))\n            - \"pairwise_median\": median of all pairwise log2 ratios\n            - \"pep_pairwise_median\": median of peptide-level pairwise log2 ratios, aggregated per protein\n\n    Returns:\n        pd.DataFrame: DataFrame with DE statistics including log2 fold change, p-values, and significance labels.\n\n    Example:\n        Legacy-style DE comparison using class types and value combinations:\n            ```python\n            pdata.de(\n                class_type=[\"cellline\", \"treatment\"],\n                values=[[\"HCT116\", \"DMSO\"], [\"HCT116\", \"DrugX\"]]\n            )\n            ```\n\n        Dictionary-style (recommended) DE comparison:\n            ```python\n            pdata.de(\n                values=[\n                    {\"cellline\": \"HCT116\", \"treatment\": \"DMSO\"},\n                    {\"cellline\": \"HCT116\", \"treatment\": \"DrugX\"}\n                ]\n            )\n            ```\n    \"\"\"\n\n    # --- Handle legacy input ---\n    if values is None:\n        raise ValueError(\"Please provide `values` (new format) or both `class_type` and `values` (legacy format).\")\n\n    if class_type is not None:\n        values = utils.format_class_filter(class_type, values, exact_cases=True)\n\n    if not isinstance(values, list) or len(values) != 2:\n        raise ValueError(\"`values` must be a list of two group dictionaries (or legacy value pairs).\")\n\n    if values[0] == values[1]:\n        raise ValueError(\"Both groups in `values` refer to the same condition. Please provide two distinct groups.\")\n\n    group1_dict, group2_dict = (\n        [values[0]] if not isinstance(values[0], list) else values[0],\n        [values[1]] if not isinstance(values[1], list) else values[1]\n    )\n\n\n    # --- Sample filtering ---\n    pdata_case1 = self._filter_sample_values(values=group1_dict, exact_cases=True, return_copy=True, verbose=False, cleanup=False) # type: ignore[attr-defined], FilteringMixin\n    pdata_case2 = self._filter_sample_values(values=group2_dict, exact_cases=True, return_copy=True, verbose=False, cleanup=False) # type: ignore[attr-defined], FilteringMixin\n\n    def _label(d):\n        if isinstance(d, dict):\n            return '_'.join(str(v) for v in d.values())\n        return str(d)\n\n    group1_string = _label(group1_dict)\n    group2_string = _label(group2_dict)\n    comparison_string = f'{group1_string} vs {group2_string}'\n\n    log_prefix = format_log_prefix(\"user\")\n    n1, n2 = len(pdata_case1.prot), len(pdata_case2.prot)\n    print(f\"{log_prefix} Running differential expression [protein]\")\n    print(f\"   \ud83d\udd38 Comparing groups: {comparison_string}\")\n    print(f\"   \ud83d\udd38 Group sizes: {n1} vs {n2} samples\")\n    print(f\"   \ud83d\udd38 Method: {method} | Fold Change: {fold_change_mode} | Layer: {layer}\")\n    print(f\"   \ud83d\udd38 P-value threshold: {pval} | Log2FC threshold: {log2fc}\")\n\n    # --- Get layer data ---\n    data1 = utils.get_adata_layer(pdata_case1.prot, layer)\n    data2 = utils.get_adata_layer(pdata_case2.prot, layer)\n\n    # Shape: (samples, features)\n    data1 = np.asarray(data1)\n    data2 = np.asarray(data2)\n\n    # --- Compute fold change ---\n    if fold_change_mode == 'mean':\n        with np.errstate(all='ignore'):\n            group1_mean = np.nanmean(data1, axis=0)\n            group2_mean = np.nanmean(data2, axis=0)\n\n            # Identify zeros or NaNs in either group\n            mask_invalid = (group1_mean == 0) | (group2_mean == 0) | np.isnan(group1_mean) | np.isnan(group2_mean)\n            log2fc_vals = np.log2(group1_mean / group2_mean)\n            log2fc_vals[mask_invalid] = np.nan\n\n            n_invalid = np.sum(mask_invalid)\n            if n_invalid &gt; 0:\n                print(f\"{format_log_prefix('info',2)} {n_invalid} proteins were not comparable (zero or NaN mean in one group).\")\n\n    elif fold_change_mode == 'pairwise_median':\n        mask_invalid = ( # Detect invalid features (any 0 or NaN in either group)\n            np.any((data1 == 0) | np.isnan(data1), axis=0) |\n            np.any((data2 == 0) | np.isnan(data2), axis=0)\n        )\n        # Compute median pairwise log2FC\n        log2fc_vals = utils.pairwise_log2fc(data1, data2)\n        log2fc_vals[mask_invalid] = np.nan # Mark invalid features as NaN\n        n_invalid = np.sum(mask_invalid)\n        if n_invalid &gt; 0:\n            print(f\"{format_log_prefix('info',2)} {n_invalid} proteins were not comparable (zero or NaN mean in one group).\")\n\n    elif fold_change_mode == 'pep_pairwise_median':\n        # --- Validate .pep presence ---\n        if self.pep is None:\n            raise ValueError(\"Peptide-level data (.pep) is required for fold_change_mode='pep_pairwise_median', but self.pep is None.\")\n\n        # --- Handle peptide layer fallback ---\n        actual_layer = layer\n        if layer != 'X' and not (hasattr(self.pep, \"layers\") and layer in self.pep.layers):\n            warnings.warn(\n                f\"Layer '{layer}' not found in .pep.layers. Falling back to 'X'.\",\n                UserWarning\n            )\n            actual_layer = 'X'\n\n        # Get peptide data\n        pep_data1 = np.asarray(utils.get_adata_layer(pdata_case1.pep, actual_layer))\n        pep_data2 = np.asarray(utils.get_adata_layer(pdata_case2.pep, actual_layer))\n\n        # Detect invalid peptides (any 0 or NaN in either group)\n        mask_invalid_pep = (\n            np.any((pep_data1 == 0) | np.isnan(pep_data1), axis=0) |\n            np.any((pep_data2 == 0) | np.isnan(pep_data2), axis=0)\n        )\n\n        # Compute per-peptide pairwise log2FCs\n        pep_log2fc = utils.pairwise_log2fc(pep_data1, pep_data2)\n        pep_log2fc[mask_invalid_pep] = np.nan  # mark invalids\n\n        n_invalid_pep = np.sum(mask_invalid_pep)\n        if n_invalid_pep &gt; 0:\n            print(f\"{format_log_prefix('info',2)} {n_invalid_pep} peptides were not comparable (zero or NaN mean in one group).\")\n\n        # Map peptides to proteins\n        pep_to_prot = utils.get_pep_prot_mapping(self, return_series=True)\n\n        # Aggregate peptide log2FCs into protein-level log2FCs\n        prot_log2fc = pd.Series(index=self.prot.var_names, dtype=float)\n        not_comparable_prot = []\n\n        for prot in self.prot.var_names:\n            matching_peptides = pep_to_prot[pep_to_prot == prot].index\n            if len(matching_peptides) == 0:\n                continue\n\n            idxs = self.pep.var_names.get_indexer(matching_peptides)\n            valid_idxs = idxs[idxs &gt;= 0]\n            if len(valid_idxs) == 0:\n                continue\n\n            valid_log2fc = pep_log2fc[valid_idxs]\n\n            if np.all(np.isnan(valid_log2fc)):\n                prot_log2fc[prot] = np.nan\n                not_comparable_prot.append(prot)\n            else:\n                prot_log2fc[prot] = np.nanmedian(pep_log2fc[valid_idxs])\n\n        log2fc_vals = prot_log2fc.values\n        if len(not_comparable_prot) &gt; 0:\n            print(f\"{format_log_prefix('info',2)} {len(not_comparable_prot)} proteins were not comparable (all peptides invalid or missing).\")\n\n    else:\n        raise ValueError(f\"Unsupported fold_change_mode: {fold_change_mode}\")\n\n    # --- Statistical test ---\n    pvals = []\n    stats = []\n    for i in range(data1.shape[1]):\n        x1, x2 = data1[:, i], data2[:, i]\n        try:\n            if method == 'ttest':\n                res = ttest_ind(x1, x2, nan_policy='omit')\n            elif method == 'mannwhitneyu':\n                res = mannwhitneyu(x1, x2, alternative='two-sided')\n            elif method == 'wilcoxon':\n                res = wilcoxon(x1, x2)\n            else:\n                raise ValueError(f\"Unsupported test method: {method}\")\n            pvals.append(res.pvalue)\n            stats.append(res.statistic)\n        except Exception as e:\n            pvals.append(np.nan)\n            stats.append(np.nan)\n\n    # --- Compile results ---\n    var = self.prot.var.copy()\n    df_stats = pd.DataFrame(index=self.prot.var_names)\n    df_stats['Genes'] = var['Genes'] if 'Genes' in var.columns else var.index\n    df_stats[group1_string] = np.nanmean(data1, axis=0)\n    df_stats[group2_string] = np.nanmean(data2, axis=0)\n    df_stats['log2fc'] = log2fc_vals\n    df_stats['p_value'] = pvals\n    df_stats['test_statistic'] = stats\n\n    df_stats['-log10(p_value)'] = -np.log10(df_stats['p_value'].replace(0, np.nan).astype(float))\n    df_stats['significance_score'] = df_stats['-log10(p_value)'] * df_stats['log2fc']\n    df_stats['significance'] = 'not significant'\n    mask_not_comparable = df_stats['log2fc'].isna()\n    df_stats.loc[mask_not_comparable, 'significance'] = 'not comparable'\n    df_stats.loc[(df_stats['p_value'] &lt; pval) &amp; (df_stats['log2fc'] &gt; log2fc), 'significance'] = 'upregulated'\n    df_stats.loc[(df_stats['p_value'] &lt; pval) &amp; (df_stats['log2fc'] &lt; -log2fc), 'significance'] = 'downregulated'\n    df_stats['significance'] = pd.Categorical(df_stats['significance'], categories=['upregulated', 'downregulated', 'not significant', 'not comparable'], ordered=True)\n\n    df_stats = df_stats.sort_values(by='significance')\n\n    # --- Store and return ---\n    self._stats[comparison_string] = df_stats # type: ignore[attr-defined]\n    self._append_history(f\"prot: DE for {class_type} {values} using {method} and fold_change_mode='{fold_change_mode}'. Stored in .stats['{comparison_string}'].\") # type: ignore[attr-defined], HistoryMixin\n\n    sig_counts = df_stats['significance'].value_counts().to_dict()\n    n_up = sig_counts.get('upregulated', 0)\n    n_down = sig_counts.get('downregulated', 0)\n    n_ns = sig_counts.get('not significant', 0)\n\n    print(f\"{format_log_prefix('result_only', indent=2)} DE complete. Results stored in:\")\n    print(f'       \u2022 .stats[\"{comparison_string}\"]')\n    print(f\"       \u2022 Columns: log2fc, p_value, significance, etc.\")\n    print(f\"       \u2022 Upregulated: {n_up} | Downregulated: {n_down} | Not significant: {n_ns}\")\n\n    return df_stats\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.harmony","title":"harmony","text":"<pre><code>harmony(key, on='protein')\n</code></pre> <p>Perform batch correction using Harmony integration.</p> <p>This method applies Harmony-based batch correction (via <code>scanpy.external.pp.harmony_integrate</code>) on PCA-reduced protein or peptide data to mitigate batch effects across samples.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Column name in <code>.obs</code> representing the batch variable to correct.</p> required <code>on</code> <code>str</code> <p>Whether to use \"protein\" or \"peptide\" data. Accepts \"prot\"/\"protein\" or \"pep\"/\"peptide\" (default: \"protein\").</p> <code>'protein'</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Perform Harmony integration on protein-level PCA embeddings:     <pre><code>pdata.harmony(key=\"batch\", on=\"protein\")\n</code></pre></p> <p>Apply Harmony on peptide-level data instead:     <pre><code>pdata.harmony(key=\"run_id\", on=\"peptide\")\n</code></pre></p> Note <ul> <li>Harmony requires prior PCA computation. If PCA is missing, it will be computed automatically.</li> <li>The Harmony-corrected coordinates are stored in <code>.obsm[\"X_pca_harmony\"]</code>.</li> <li>Updates the processing history via <code>.history</code>.</li> </ul> Todo <p>Add optional arguments for controlling Harmony parameters (e.g., <code>max_iter_harmony</code>, <code>theta</code>, <code>lambda</code>).</p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def harmony(self, key, on = 'protein'):\n    \"\"\"\n    Perform batch correction using Harmony integration.\n\n    This method applies Harmony-based batch correction (via `scanpy.external.pp.harmony_integrate`)\n    on PCA-reduced protein or peptide data to mitigate batch effects across samples.\n\n    Args:\n        key (str): Column name in `.obs` representing the batch variable to correct.\n        on (str): Whether to use \"protein\" or \"peptide\" data. Accepts \"prot\"/\"protein\" or \"pep\"/\"peptide\" (default: \"protein\").\n\n    Returns:\n        None\n\n    Example:\n        Perform Harmony integration on protein-level PCA embeddings:\n            ```python\n            pdata.harmony(key=\"batch\", on=\"protein\")\n            ```\n\n        Apply Harmony on peptide-level data instead:\n            ```python\n            pdata.harmony(key=\"run_id\", on=\"peptide\")\n            ```\n\n    Note:\n        - Harmony requires prior PCA computation. If PCA is missing, it will be computed automatically.\n        - The Harmony-corrected coordinates are stored in `.obsm[\"X_pca_harmony\"]`.\n        - Updates the processing history via `.history`.\n\n    Todo:\n        Add optional arguments for controlling Harmony parameters (e.g., `max_iter_harmony`, `theta`, `lambda`).\n    \"\"\"\n\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        pass\n\n    if on == 'protein' or on == 'prot':\n        adata = self.prot\n    elif on == 'peptide' or on == 'pep':\n        adata = self.pep\n\n    log_prefix = format_log_prefix(\"user\")\n    print(f\"{log_prefix} Performing Harmony batch correction on [{on}] PCA.\")\n\n    # check if pca has been run before, look for distances and connectivities in obsp\n    if 'pca' not in adata.uns:\n        print(f\"{format_log_prefix('info_only', indent=2)} PCA not found in AnnData object. Running PCA with default settings.\")\n        self.pca(on = on, layer = \"X\")\n\n    # check that key is valid column in adata.obs\n    if key not in adata.obs.columns:\n        raise ValueError(f\"Batch key '{key}' not found in adata.obs.\")\n\n    sc.external.pp.harmony_integrate(adata, key)\n\n    self._append_history(f'{on}: Harmony batch correction applied on key {key}, stored in obsm[\"X_pca_harmony\"] and uns[\"umap\"]') # type: ignore[attr-defined], HistoryMixin\n    print(f\"{format_log_prefix('result_only', indent=2)} Harmony batch correction complete. Results stored in:\")\n    print(f\"       \u2022 obsm['X_pca_harmony'] (PCA coordinates)\")\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.impute","title":"impute","text":"<pre><code>impute(classes=None, layer='X', method='mean', on='protein', min_scale=1, set_X=True, **kwargs)\n</code></pre> <p>Impute missing values across samples globally or within groups.</p> <p>This method imputes missing values in the specified data layer using one of several strategies. It supports both global (across all samples) and group-wise imputation based on sample classes.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>str or list of str</code> <p>Sample-level class/grouping column(s). If None, imputation is global.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to impute from (default is \"X\").</p> <code>'X'</code> <code>method</code> <code>str</code> <p>Imputation strategy to use. Options include:</p> <ul> <li>\"mean\": Fill missing values with the mean of each feature.</li> <li>\"median\": Fill missing values with the median of each feature.</li> <li>\"min\": Fill with the minimum observed value (0 if all missing).</li> <li>\"knn\": Use K-nearest neighbors (only supported for global imputation).</li> </ul> <code>'mean'</code> <code>on</code> <code>str</code> <p>Whether to impute \"protein\" or \"peptide\" data.</p> <code>'protein'</code> <code>min_scale</code> <code>float</code> <p>Scaled multiplication of minimum value for imputation, i.e. 0.2 would be 20% of minimum value (default is 1).</p> <code>1</code> <code>set_X</code> <code>bool</code> <p>If True, updates <code>.X</code> to use the imputed result.</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments passed to the imputer (e.g., <code>n_neighbors</code> for KNN).</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Globally impute missing values using the median strategy:     <pre><code>pdata.impute(method=\"median\", on=\"protein\")\n</code></pre></p> <p>Group-wise imputation based on treatment:     <pre><code>pdata.impute(classes=\"treatment\", method=\"mean\", on=\"protein\")\n</code></pre></p> Note <ul> <li>KNN imputation is only supported for global (non-grouped) mode.</li> <li>Features that are entirely missing within a group or across all samples are skipped and preserved as NaN.</li> <li>Imputed results are stored in a new layer named <code>\"X_impute_&lt;method&gt;\"</code>.</li> <li>Imputation summaries are printed to the console by group or overall.</li> </ul> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def impute(self, classes=None, layer=\"X\", method='mean', on='protein', min_scale=1, set_X=True, **kwargs):\n    \"\"\"\n    Impute missing values across samples globally or within groups.\n\n    This method imputes missing values in the specified data layer using one of several strategies.\n    It supports both global (across all samples) and group-wise imputation based on sample classes.\n\n    Args:\n        classes (str or list of str, optional): Sample-level class/grouping column(s). If None, imputation is global.\n        layer (str): Data layer to impute from (default is \"X\").\n        method (str): Imputation strategy to use. Options include:\n\n            - \"mean\": Fill missing values with the mean of each feature.\n            - \"median\": Fill missing values with the median of each feature.\n            - \"min\": Fill with the minimum observed value (0 if all missing).\n            - \"knn\": Use K-nearest neighbors (only supported for global imputation).\n\n        on (str): Whether to impute \"protein\" or \"peptide\" data.\n        min_scale (float): Scaled multiplication of minimum value for imputation, i.e. 0.2 would be 20% of minimum value (default is 1).\n        set_X (bool): If True, updates `.X` to use the imputed result.\n        **kwargs: Additional arguments passed to the imputer (e.g., `n_neighbors` for KNN).\n\n    Returns:\n        None\n\n    Example:\n        Globally impute missing values using the median strategy:\n            ```python\n            pdata.impute(method=\"median\", on=\"protein\")\n            ```\n\n        Group-wise imputation based on treatment:\n            ```python\n            pdata.impute(classes=\"treatment\", method=\"mean\", on=\"protein\")\n            ```\n\n    Note:\n        - KNN imputation is only supported for global (non-grouped) mode.\n        - Features that are entirely missing within a group or across all samples are skipped and preserved as NaN.\n        - Imputed results are stored in a new layer named `\"X_impute_&lt;method&gt;\"`.\n        - Imputation summaries are printed to the console by group or overall.\n    \"\"\"\n    from sklearn.impute import SimpleImputer, KNNImputer\n    from scipy import sparse\n    from scpviz import utils\n\n\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        return\n\n    adata = self.prot if on == 'protein' else self.pep\n    if layer != \"X\" and layer not in adata.layers:\n        raise ValueError(f\"Layer '{layer}' not found in .{on}.\")\n\n    impute_data = adata.layers[layer] if layer != \"X\" else adata.X\n    was_sparse = sparse.issparse(impute_data)\n    impute_data = impute_data.toarray() if was_sparse else impute_data.copy()\n    original_data = impute_data.copy()\n\n    layer_name = f\"X_impute_{method}\"\n\n    if method not in {\"mean\", \"median\", \"min\",\"knn\"}:\n        raise ValueError(f\"Unsupported method: {method}\")\n\n    if classes is None:\n        # Global imputation\n        if method == 'min':\n            min_vals = np.nanmin(impute_data, axis=0)\n            min_vals = np.where(np.isnan(min_vals), 0, min_vals)\n            min_vals = min_vals * min_scale\n            mask = np.isnan(impute_data)\n            impute_data[mask] = np.take(min_vals, np.where(mask)[1])\n        elif method == 'knn':\n            n_neighbors = kwargs.get('n_neighbors', 3)\n            imputer = KNNImputer(n_neighbors=n_neighbors)\n            impute_data = imputer.fit_transform(impute_data)\n        else:\n            imputer = SimpleImputer(strategy=method, keep_empty_features=True)\n            nan_columns = np.isnan(impute_data).all(axis=0)  # features fully missing in this group\n            impute_data = imputer.fit_transform(impute_data)\n            impute_data[:, nan_columns] = np.nan\n\n        min_message = \"\" if method != 'min' else f\"Minimum scaled by {min_scale}.\"\n        print(f\"{format_log_prefix('user')} Global imputation using '{method}'. Layer saved as '{layer_name}'. {min_message}\")\n        skipped_features = np.sum(np.isnan(impute_data).all(axis=0))\n\n    else:\n        # Group-wise imputation\n        if method == 'knn':\n            raise ValueError(\"KNN imputation is not supported for group-wise imputation.\")\n\n        sample_names = utils.get_samplenames(adata, classes)\n        sample_names = np.array(sample_names)\n        unique_groups = np.unique(sample_names)\n\n        for group in unique_groups:\n            idx = np.where(sample_names == group)[0]\n            group_data = impute_data[idx, :]\n\n            if method == 'min':\n                min_vals = np.nanmin(group_data, axis=0)\n                min_vals = np.where(np.isnan(min_vals), 0, min_vals)\n                min_vals = min_vals * min_scale\n                mask = np.isnan(group_data)\n                group_data[mask] = np.take(min_vals, np.where(mask)[1])\n                imputed_group = group_data\n            else:\n                imputer = SimpleImputer(strategy=method, keep_empty_features=True)\n                nan_columns = np.isnan(group_data).all(axis=0)  # features fully missing in this group\n                imputed_group = imputer.fit_transform(group_data)\n                imputed_group[:, nan_columns] = np.nan # restore fully missing features\n\n            impute_data[idx, :] = imputed_group\n\n        min_message = \"\" if method != 'min' else f\"Minimum scaled by {min_scale}.\"\n        print(f\"{format_log_prefix('user')} Group-wise imputation using '{method}' on class(es): {classes}. Layer saved as '{layer_name}'. {min_message}\")\n\n    summary_lines = []\n    if classes is None:\n        num_imputed = np.sum(np.isnan(original_data) &amp; ~np.isnan(impute_data))\n        # Row-wise missingness\n        was_missing = np.isnan(original_data).any(axis=1)\n        now_complete = ~np.isnan(impute_data).any(axis=1)\n        now_incomplete = np.isnan(impute_data).any(axis=1)\n\n        fully_imputed_samples = np.sum(was_missing &amp; now_complete)\n        partially_imputed_samples = np.sum(was_missing &amp; now_incomplete)\n        skipped_features = np.sum(np.isnan(impute_data).all(axis=0))\n\n        summary_lines.append(\n            f\"{format_log_prefix('result_only', indent=2)} {num_imputed} values imputed.\"\n        )\n        summary_lines.append(\n            f\"{format_log_prefix('info_only', indent=2)} {fully_imputed_samples} samples fully imputed, {partially_imputed_samples} samples partially imputed, {skipped_features} skipped feature(s) with all missing values.\"\n        )\n\n    else:\n        sample_names = utils.get_samplenames(adata, classes)\n        sample_names = np.array(sample_names)\n        unique_groups = np.unique(sample_names)\n\n        counts_by_group = {}\n        fully_by_group = {}\n        partial_by_group = {}\n        missing_features_by_group = {}\n        total_samples_by_group = {}\n\n        for group in unique_groups:\n            idx = np.where(sample_names == group)[0]\n            before = original_data[idx, :]\n            after = impute_data[idx, :]\n\n            # count imputed values\n            mask = np.isnan(before) &amp; ~np.isnan(after)\n            counts_by_group[group] = np.sum(mask)\n\n            # count fully and partially imputed samples\n            was_missing = np.isnan(before).any(axis=1)\n            now_complete = ~np.isnan(after).any(axis=1)\n            now_incomplete = np.isnan(after).any(axis=1)\n            now_missing = np.sum(np.isnan(before).all(axis=0))\n\n            fully_by_group[group] = np.sum(was_missing &amp; now_complete)\n            partial_by_group[group] = np.sum(was_missing &amp; now_incomplete)\n            missing_features_by_group[group] = now_missing\n            total_samples_by_group[group] = len(idx)\n\n        # Compute dynamic width based on longest group name\n        group_width = max(max(len(str(g)) for g in unique_groups), 20)\n\n        # Summary totals\n        total = sum(counts_by_group.values())\n        summary_lines.append(f\"{format_log_prefix('result_only', indent=2)} {total} values imputed total.\")\n        summary_lines.append(f\"{format_log_prefix('info_only', indent=2)} Group-wise summary:\")\n\n        # Header row (aligned with computed width)\n        header = (f\"{'Group':&lt;{group_width}} | Values Imputed | Skipped Features | Samples Imputed (Partial,Fully)/Total\")\n        divider = \"-\" * len(header)\n        summary_lines.append(f\"{' ' * 5}{header}\")\n        summary_lines.append(f\"{' ' * 5}{divider}\")\n\n        # Data rows\n        for group in unique_groups:\n            count = counts_by_group[group]\n            fully = fully_by_group[group]\n            partial = partial_by_group[group]\n            skipped = missing_features_by_group[group]\n            total_samples = total_samples_by_group[group]\n            summary_lines.append(\n                f\"{' ' * 5}{group:&lt;{group_width}} | {count:&gt;14} | {skipped:&gt;16} | {partial:&gt;7}, {fully:&gt;5} / {total_samples:&lt;3}\"\n            )\n\n    print(\"\\n\".join(summary_lines))\n\n    adata.layers[layer_name] = sparse.csr_matrix(impute_data) if was_sparse else impute_data\n\n    if set_X:\n        self.set_X(layer=layer_name, on=on) # type: ignore[attr-defined], EditingMixin\n\n    self._history.append( # type: ignore[attr-defined]\n        f\"{on}: Imputed layer '{layer}' using '{method}' (grouped by {classes if classes else 'ALL'}). Stored in '{layer_name}'.\"\n    )\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.leiden","title":"leiden","text":"<pre><code>leiden(on='protein', layer='X', **kwargs)\n</code></pre> <p>Perform Leiden clustering on protein or peptide data.</p> <p>This method runs community detection using the Leiden algorithm based on a precomputed neighbor graph using <code>scanpy.tl.leiden()</code>. If neighbors are not already computed, they will be generated automatically.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Whether to use \"protein\" or \"peptide\" data.</p> <code>'protein'</code> <code>layer</code> <code>str</code> <p>Data layer to use for clustering (default is \"X\").</p> <code>'X'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>scanpy.tl.leiden()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Perform Leiden clustering using the default PCA-based neighbors:     <pre><code>pdata.leiden(on=\"protein\", layer=\"X\", resolution=0.25)\n</code></pre></p> Note <ul> <li>Cluster labels are stored in <code>.obs[\"leiden\"]</code>.</li> <li>Neighbor graphs are automatically computed if not present in <code>.uns[\"neighbors\"]</code>.</li> <li>Automatically sets <code>.X</code> to the specified layer if it is not already active.</li> </ul> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def leiden(self, on = 'protein', layer = \"X\", **kwargs):\n    \"\"\"\n    Perform Leiden clustering on protein or peptide data.\n\n    This method runs community detection using the Leiden algorithm based on a precomputed\n    neighbor graph using `scanpy.tl.leiden()`. If neighbors are not already computed, they will be generated automatically.\n\n    Args:\n        on (str): Whether to use \"protein\" or \"peptide\" data.\n        layer (str): Data layer to use for clustering (default is \"X\").\n        **kwargs: Additional keyword arguments passed to `scanpy.tl.leiden()`.\n\n    Returns:\n        None\n\n    Example:\n        Perform Leiden clustering using the default PCA-based neighbors:\n            ```python\n            pdata.leiden(on=\"protein\", layer=\"X\", resolution=0.25)\n            ```\n\n    Note:\n        - Cluster labels are stored in `.obs[\"leiden\"]`.\n        - Neighbor graphs are automatically computed if not present in `.uns[\"neighbors\"]`.\n        - Automatically sets `.X` to the specified layer if it is not already active.\n    \"\"\"\n    # uses sc.tl.leiden with default resolution of 0.25\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        pass\n\n    if on == 'protein':\n        adata = self.prot\n    elif on == 'peptide':\n        adata = self.pep\n\n    log_prefix = format_log_prefix(\"user\")\n    print(f\"{log_prefix} Performing Leiden clustering [{on}] using layer: {layer}\")\n\n    if 'resolution' in kwargs:\n        resolution = kwargs.pop(\"resolution\", 0.25)\n\n    if 'neighbors' not in adata.uns:\n        print(f\"{format_log_prefix('info_only', indent=2)} Neighbors not found in AnnData object. Running neighbors with default settings.\")\n        self.neighbor(on = on, layer = layer, **kwargs)\n\n    if layer == \"X\":\n        # do nothing\n        pass\n    elif layer in adata.layers.keys():\n        self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n    sc.tl.leiden(adata, resolution)\n\n    self._append_history(f'{on}: Leiden clustering fitted on {layer}, stored in obs[\"leiden\"]') # type: ignore[attr-defined], HistoryMixin\n    print(f\"{format_log_prefix('result_only', indent=2)} Leiden clustering complete. Results stored in:\")\n    print(f\"       \u2022 obs['leiden'] (cluster labels)\")\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.nanmissingvalues","title":"nanmissingvalues","text":"<pre><code>nanmissingvalues(on='protein', limit=0.5)\n</code></pre> <p>Set columns (proteins or peptides) with excessive missing values to NaN.</p> <p>This method scans all features and replaces their corresponding columns with NaN if the fraction of missing values exceeds the given threshold. It helps ensure downstream normalization and imputation steps are applied to meaningful features only.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Whether to use \"protein\" or \"peptide\" data. Accepts \"prot\"/\"protein\" or \"pep\"/\"peptide\" (default: \"protein\").</p> <code>'protein'</code> <code>limit</code> <code>float</code> <p>Proportion threshold for missing values (default: 0.5).  Features with more than <code>limit \u00d7 100%</code> missing values are set entirely to NaN.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>None</p> <p>Deprecation Notice</p> <p>This function may be deprecated in future releases. Use <code>annotate_found</code> and <code>filter_prot_found</code> for more robust and configurable detection-based filtering.</p> Example <p>Mask proteins with more than 50% missing values:     <pre><code>pdata.nanmissingvalues(on=\"protein\", limit=0.5)\n</code></pre></p> <p>Apply the same filter for peptide-level data:     <pre><code>pdata.nanmissingvalues(on=\"peptide\", limit=0.3)\n</code></pre></p> Note <ul> <li>The missing-value fraction is computed per feature across all samples.</li> <li>This operation modifies the <code>.X</code> matrix in-place.</li> <li>The updated data are stored back into <code>.prot</code> or <code>.pep</code>.</li> </ul> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def nanmissingvalues(self, on = 'protein', limit = 0.5):\n    \"\"\"\n    Set columns (proteins or peptides) with excessive missing values to NaN.\n\n    This method scans all features and replaces their corresponding columns with NaN\n    if the fraction of missing values exceeds the given threshold. It helps ensure\n    downstream normalization and imputation steps are applied to meaningful features only.\n\n    Args:\n        on (str): Whether to use \"protein\" or \"peptide\" data. Accepts \"prot\"/\"protein\" or \"pep\"/\"peptide\" (default: \"protein\").\n        limit (float): Proportion threshold for missing values (default: 0.5). \n            Features with more than `limit \u00d7 100%` missing values are set entirely to NaN.\n\n    Returns:\n        None\n\n    !!! warning \"Deprecation Notice\"\n        This function may be deprecated in future releases.  \n        Use [`annotate_found`](reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing_mixins.annotate_found)  \n        and [`filter_prot_found`](reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing_mixins.filter_prot_found)  \n        for more robust and configurable detection-based filtering.\n\n    Example:\n        Mask proteins with more than 50% missing values:\n            ```python\n            pdata.nanmissingvalues(on=\"protein\", limit=0.5)\n            ```\n\n        Apply the same filter for peptide-level data:\n            ```python\n            pdata.nanmissingvalues(on=\"peptide\", limit=0.3)\n            ```\n\n    Note:\n        - The missing-value fraction is computed per feature across all samples.\n        - This operation modifies the `.X` matrix in-place.\n        - The updated data are stored back into `.prot` or `.pep`.\n    \"\"\"\n    import scipy.sparse\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        pass\n\n    if on == 'protein':\n        adata = self.prot\n\n    elif on == 'peptide':\n        adata = self.pep\n\n    if scipy.sparse.issparse(adata.X):\n        X = adata.X.toarray()\n    else:\n        X = adata.X\n    missing_proportion = np.isnan(X).mean(axis=0)\n    columns_to_nan = missing_proportion &gt; limit\n    X[:, columns_to_nan] = np.nan\n    adata.X = scipy.sparse.csr_matrix(X) if scipy.sparse.issparse(adata.X) else X\n\n    if on == 'protein':\n        self.prot = adata\n    elif on == 'peptide':\n        self.pep = adata\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.neighbor","title":"neighbor","text":"<pre><code>neighbor(on='protein', layer='X', use_rep='X_pca', user_indent=0, **kwargs)\n</code></pre> <p>Compute a neighbor graph based on protein or peptide data.</p> <p>This method builds a nearest-neighbors graph for downstream analysis using  <code>scanpy.pp.neighbors</code>. It optionally performs PCA before constructing the graph  if a valid representation is not already available.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Whether to use \"protein\" or \"peptide\" data.</p> <code>'protein'</code> <code>layer</code> <code>str</code> <p>Data layer to use (default is \"X\").</p> <code>'X'</code> <code>use_rep</code> <code>str</code> <p>Key in <code>.obsm</code> to use for computing neighbors. Default is <code>\"X_pca\"</code>. If <code>\"X_pca\"</code> is requested but not found, PCA will be run automatically.</p> <code>'X_pca'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>scanpy.pp.neighbors()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Compute neighbors using default PCA representation:     <pre><code>pdata.neighbor(on=\"protein\", layer=\"X\")\n</code></pre></p> <p>Use a custom representation stored in <code>.obsm[\"X_umap\"]</code>:     <pre><code>pdata.neighbor(on=\"protein\", use_rep=\"X_umap\", n_neighbors=15)\n</code></pre></p> Note <ul> <li>The neighbor graph is stored in <code>.obs[\"distances\"]</code> and <code>.obs[\"connectivities\"]</code>.</li> <li>Neighbor metadata is stored in <code>.uns[\"neighbors\"]</code>.</li> <li>Automatically calls <code>self.set_X()</code> if a non-default layer is specified.</li> <li>PCA is computed automatically if <code>use_rep='X_pca'</code> and not already present.</li> </ul> Todo <p>Allow users to supply a custom <code>KNeighborsTransformer</code> or precomputed neighbor graph.     <pre><code>from sklearn.neighbors import KNeighborsTransformer\ntransformer = KNeighborsTransformer(n_neighbors=10, metric='manhattan', algorithm='kd_tree')\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def neighbor(self, on = 'protein', layer = \"X\", use_rep='X_pca', user_indent=0,**kwargs):\n    \"\"\"\n    Compute a neighbor graph based on protein or peptide data.\n\n    This method builds a nearest-neighbors graph for downstream analysis using \n    `scanpy.pp.neighbors`. It optionally performs PCA before constructing the graph \n    if a valid representation is not already available.\n\n    Args:\n        on (str): Whether to use \"protein\" or \"peptide\" data.\n        layer (str): Data layer to use (default is \"X\").\n        use_rep (str): Key in `.obsm` to use for computing neighbors. Default is `\"X_pca\"`.\n            If `\"X_pca\"` is requested but not found, PCA will be run automatically.\n        **kwargs: Additional keyword arguments passed to `scanpy.pp.neighbors()`.\n\n    Returns:\n        None\n\n    Example:\n        Compute neighbors using default PCA representation:\n            ```python\n            pdata.neighbor(on=\"protein\", layer=\"X\")\n            ```\n\n        Use a custom representation stored in `.obsm[\"X_umap\"]`:\n            ```python\n            pdata.neighbor(on=\"protein\", use_rep=\"X_umap\", n_neighbors=15)\n            ```\n\n    Note:\n        - The neighbor graph is stored in `.obs[\"distances\"]` and `.obs[\"connectivities\"]`.\n        - Neighbor metadata is stored in `.uns[\"neighbors\"]`.\n        - Automatically calls `self.set_X()` if a non-default layer is specified.\n        - PCA is computed automatically if `use_rep='X_pca'` and not already present.\n\n    Todo:\n        Allow users to supply a custom `KNeighborsTransformer` or precomputed neighbor graph.\n            ```python\n            from sklearn.neighbors import KNeighborsTransformer\n            transformer = KNeighborsTransformer(n_neighbors=10, metric='manhattan', algorithm='kd_tree')\n            ```\n    \"\"\"\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        pass\n\n    if on.lower() in [\"prot\", \"protein\"]:\n        adata = self.prot\n    elif on.lower() in [\"pep\", \"peptide\"]:\n        adata = self.pep\n\n    if layer == \"X\":\n        # do nothing\n        pass\n    elif layer in adata.layers.keys():\n        self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n    log_prefix = format_log_prefix(\"user\") if user_indent == 0 else format_log_prefix(\"user_only\",2)\n    print(f\"{log_prefix} Computing neighbors [{on}] using layer: {layer}\")\n\n    if use_rep == 'X_pca':\n        if 'pca' not in adata.uns:\n            print(f\"{format_log_prefix('info_only',indent=2)} PCA not found in AnnData object. Running PCA with default settings.\")\n            self.pca(on = on, layer = layer)\n    else:\n        if use_rep not in adata.obsm:\n            raise ValueError(f\"PCA key '{use_rep}' not found in obsm. Please run PCA first and specify a valid key.\")\n        print(f\"{format_log_prefix('info_only',indent=2)} Using '{use_rep}' found in obsm for neighbor graph.\")\n\n    if use_rep == 'X_pca':\n        sc.pp.neighbors(adata, **kwargs)\n    else:\n        sc.pp.neighbors(adata, use_rep=use_rep, **kwargs)\n\n    self._append_history(f'{on}: Neighbors fitted on {layer}, using {use_rep}, stored in obs[\"distances\"] and obs[\"connectivities\"]') # type: ignore[attr-defined], HistoryMixin\n    print(f\"{format_log_prefix('result_only',indent=2)} Neighbors computed on {layer}, using {use_rep}. Results stored in:\")\n    print(f\"       \u2022 obs['distances'] (pairwise distances)\")\n    print(f\"       \u2022 obs['connectivities'] (connectivity graph)\")\n    print(f\"       \u2022 uns['neighbors'] (neighbor graph metadata)\")\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.normalize","title":"normalize","text":"<pre><code>normalize(classes=None, layer='X', method='sum', on='protein', set_X=True, force=False, use_nonmissing=False, **kwargs)\n</code></pre> <p>Normalize sample intensities across protein or peptide data.</p> <p>This method performs global or group-wise normalization of the selected data layer. It supports multiple normalization strategies ranging from simple scaling (e.g., sum, median) to advanced approaches such as <code>reference_feature</code> and <code>directlfq</code>.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>str or list</code> <p>Sample-level grouping column(s) in <code>.obs</code> to perform group-wise normalization. If None, normalization is applied globally.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to normalize from (default: <code>\"X\"</code>).</p> <code>'X'</code> <code>method</code> <code>str</code> <p>Normalization strategy to apply. Options include: <code>'sum'</code>, <code>'median'</code>, <code>'mean'</code>, <code>'max'</code>, <code>'reference_feature'</code>, <code>'robust_scale'</code>, <code>'quantile_transform'</code>, <code>'directlfq'</code>.</p> <code>'sum'</code> <code>on</code> <code>str</code> <p>Whether to use <code>\"protein\"</code> or <code>\"peptide\"</code> data.</p> <code>'protein'</code> <code>set_X</code> <code>bool</code> <p>Whether to set <code>.X</code> to the normalized result (default: True).</p> <code>True</code> <code>force</code> <code>bool</code> <p>Proceed with normalization even if samples exceed the allowed fraction of missing values (default: False).</p> <code>False</code> <code>use_nonmissing</code> <code>bool</code> <p>If True, only use columns with no missing values across all samples when computing scaling factors (default: False).</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for normalization methods. - <code>reference_columns</code> (list): For <code>'reference_feature'</code>, specify columns or gene names to normalize against. - <code>max_missing_fraction</code> (float): Maximum allowed fraction of missing values per sample (default: 0.5). - <code>n_neighbors</code> (int): For methods requiring neighbor-based computations. - <code>input_type_to_use</code> (str): For <code>'directlfq'</code>, specify <code>'pAnnData'</code>, <code>'diann_precursor_ms1'</code>, or <code>'diann_precursor_ms1_and_ms2'</code>. - <code>path</code> (str): For <code>'directlfq'</code>, path to the <code>report.tsv</code> or <code>report.parquet</code> file from DIA-NN output.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Perform global normalization using the median intensity:     <pre><code>pdata.normalize(on=\"protein\", method=\"median\")\n</code></pre></p> <p>Apply group-wise normalization by treatment class using sum-scaling:     <pre><code>pdata.normalize(classes=\"treatment\", method=\"sum\", on=\"protein\")\n</code></pre></p> <p>Run reference-feature normalization using specific genes:     <pre><code>pdata.normalize(\n    on=\"protein\",\n    method=\"reference_feature\",\n    reference_columns=[\"ACTB\", \"GAPDH\"]\n)\n</code></pre></p> <p>About <code>directlfq</code> normalization</p> <ul> <li>The <code>directlfq</code> method aggregates peptide-level data to protein-level intensities and stores results in a new protein-layer (e.g. <code>'X_norm_directlfq'</code>).</li> <li>It does not support group-wise normalization.</li> <li>Processing time may scale with dataset size.</li> <li>For algorithmic and benchmarking details, see: Ammar, Constantin et al. (2023) Accurate Label-Free Quantification by directLFQ to Compare Unlimited Numbers of Proteomes. Molecular &amp; Cellular Proteomics, 22(7):100581. https://doi.org/10.1016/j.mcpro.2023.100581</li> </ul> Note <ul> <li>Results are stored in a new layer named <code>'X_norm_&lt;method&gt;'</code>.</li> <li>The normalized layer replaces <code>.X</code> if <code>set_X=True</code>.</li> <li>Normalization operations are recorded in <code>.history</code>.</li> <li>For consistency across runs, consider running <code>.impute()</code> before normalization.</li> </ul> Todo <ul> <li>Add optional z-score and percentile normalization modes.</li> <li>Add support for specifying external scaling factors.</li> </ul> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def normalize(self, classes = None, layer = \"X\", method = 'sum', on = 'protein', set_X = True, force = False, use_nonmissing = False, **kwargs):  \n    \"\"\"\n    Normalize sample intensities across protein or peptide data.\n\n    This method performs global or group-wise normalization of the selected data layer.\n    It supports multiple normalization strategies ranging from simple scaling\n    (e.g., sum, median) to advanced approaches such as `reference_feature` and\n    [`directlfq`]((https://doi.org/10.1016/j.mcpro.2023.100581)).\n\n    Args:\n        classes (str or list, optional): Sample-level grouping column(s) in `.obs` to\n            perform group-wise normalization. If None, normalization is applied globally.\n        layer (str, optional): Data layer to normalize from (default: `\"X\"`).\n        method (str, optional): Normalization strategy to apply. Options include:\n            `'sum'`, `'median'`, `'mean'`, `'max'`, `'reference_feature'`,\n            `'robust_scale'`, `'quantile_transform'`, `'directlfq'`.\n        on (str, optional): Whether to use `\"protein\"` or `\"peptide\"` data.\n        set_X (bool, optional): Whether to set `.X` to the normalized result (default: True).\n        force (bool, optional): Proceed with normalization even if samples exceed the\n            allowed fraction of missing values (default: False).\n        use_nonmissing (bool, optional): If True, only use columns with no missing values\n            across all samples when computing scaling factors (default: False).\n        **kwargs: Additional keyword arguments for normalization methods.\n            - `reference_columns` (list): For `'reference_feature'`, specify columns or\n            gene names to normalize against.\n            - `max_missing_fraction` (float): Maximum allowed fraction of missing values\n            per sample (default: 0.5).\n            - `n_neighbors` (int): For methods requiring neighbor-based computations.\n            - `input_type_to_use` (str): For `'directlfq'`, specify `'pAnnData'`,\n            `'diann_precursor_ms1'`, or `'diann_precursor_ms1_and_ms2'`.\n            - `path` (str): For `'directlfq'`, path to the `report.tsv` or `report.parquet`\n            file from DIA-NN output.\n\n    Returns:\n        None\n\n    Example:\n        Perform global normalization using the median intensity:\n            ```python\n            pdata.normalize(on=\"protein\", method=\"median\")\n            ```\n\n        Apply group-wise normalization by treatment class using sum-scaling:\n            ```python\n            pdata.normalize(classes=\"treatment\", method=\"sum\", on=\"protein\")\n            ```\n\n        Run reference-feature normalization using specific genes:\n            ```python\n            pdata.normalize(\n                on=\"protein\",\n                method=\"reference_feature\",\n                reference_columns=[\"ACTB\", \"GAPDH\"]\n            )\n            ```\n\n    !!! tip \"About `directlfq` normalization\"\n        - The `directlfq` method aggregates peptide-level data to protein-level intensities\n        and stores results in a new protein-layer (e.g. `'X_norm_directlfq'`).\n        - It does not support group-wise normalization.\n        - Processing time may scale with dataset size.\n        - For algorithmic and benchmarking details, see:  \n        **Ammar, Constantin et al. (2023)**  \n        *Accurate Label-Free Quantification by directLFQ to Compare Unlimited Numbers of Proteomes.*  \n        *Molecular &amp; Cellular Proteomics*, 22(7):100581.  \n        [https://doi.org/10.1016/j.mcpro.2023.100581](https://doi.org/10.1016/j.mcpro.2023.100581)\n\n\n\n    Note:\n        - Results are stored in a new layer named `'X_norm_&lt;method&gt;'`.\n        - The normalized layer replaces `.X` if `set_X=True`.\n        - Normalization operations are recorded in `.history`.\n        - For consistency across runs, consider running `.impute()` before normalization.\n\n    Todo:\n        - Add optional z-score and percentile normalization modes.\n        - Add support for specifying external scaling factors.\n    \"\"\"\n\n\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        return\n\n    adata = self.prot if on == 'protein' else self.pep\n    if layer != \"X\" and layer not in adata.layers:\n        raise ValueError(f\"Layer {layer} not found in .{on}.\")\n\n    normalize_data = adata.layers[layer] if layer != \"X\" else adata.X\n    was_sparse = sparse.issparse(normalize_data)\n    normalize_data = normalize_data.toarray() if was_sparse else normalize_data.copy()\n    original_data = normalize_data.copy()\n\n    layer_name = 'X_norm_' + method\n    normalize_funcs = ['sum', 'median', 'mean', 'max', 'reference_feature', 'robust_scale', 'quantile_transform','directlfq']\n\n    if method not in normalize_funcs:\n        raise ValueError(f\"Unsupported normalization method: {method}\")\n\n    # Special handling for directlfq\n    if method == \"directlfq\":\n        if classes is not None:\n            print(f\"{format_log_prefix('warn')} 'directlfq' does not support group-wise normalization. Proceeding with global normalization.\")\n            classes = None\n\n        print(f\"{format_log_prefix('user')} Running directlfq normalization on peptide-level data.\")\n        print(f\"{format_log_prefix('info_only', indent=2)} Note: please be patient, directlfq can take a minute to run depending on data size. Output files will be produced.\")\n        normalize_data = self._normalize_helper_directlfq(**kwargs)\n\n        adata = self.prot  # directlfq always outputs protein-level intensities\n        adata.layers[layer_name] = sparse.csr_matrix(normalize_data) if was_sparse else normalize_data\n\n        if set_X:\n            self.set_X(layer=layer_name, on=\"protein\")  # type: ignore[attr-defined]\n\n        self._history.append(  # type: ignore[attr-defined]\n            f\"protein: Normalized layer using directlfq (input_type={kwargs.get('input_type_to_use', 'default')}). Stored in `{layer_name}`.\"\n        )\n        print(f\"{format_log_prefix('result_only', indent=2)} directlfq normalization complete. Results are stored in layer '{layer_name}'.\")\n        return\n\n    # --- standard normalization ---\n    # Build the header message early\n    if classes is None:\n        msg = f\"{format_log_prefix('user')} Global normalization using '{method}'\"\n    else:\n        msg = f\"{format_log_prefix('info_only')} Group-wise normalization using '{method}' on class(es): {classes}\"\n\n    if use_nonmissing and method in {'sum', 'mean', 'median', 'max'}:\n        msg += \" (using only fully observed columns)\"\n    msg += f\". Layer will be saved as '{layer_name}'.\"\n\n    # \u2705 Print message before checking for missing values\n    print(msg)\n\n    # Check for bad rows (too many missing values)\n    missing_fraction = np.isnan(normalize_data).sum(axis=1) / normalize_data.shape[1]\n    max_missing_fraction = kwargs.pop(\"max_missing_fraction\", 0.5)\n    bad_rows_mask = missing_fraction &gt; max_missing_fraction\n\n    if np.any(bad_rows_mask):\n        n_bad = np.sum(bad_rows_mask)\n        print(f\"{format_log_prefix('warn',2)} {n_bad} sample(s) have &gt;{int(max_missing_fraction*100)}% missing values.\")\n        print(\"     Try running `.impute()` before normalization. Suggest to use the flag `use_nonmissing=True` to normalize using only consistently observed proteins.\")\n        if not force:\n            print(\"     \u27a1\ufe0f Use `force=True` to proceed anyway.\")\n            return\n        print(f\"{format_log_prefix('warn',2)} Proceeding with normalization despite bad rows (force=True).\")\n\n    if classes is None:\n        normalize_data = self._normalize_helper(normalize_data, method, use_nonmissing=use_nonmissing, **kwargs)\n    else:\n        # Group-wise normalization\n        sample_names = utils.get_samplenames(adata, classes)\n        sample_names = np.array(sample_names)\n        unique_groups = np.unique(sample_names)\n\n        for group in unique_groups:\n            idx = np.where(sample_names == group)[0]\n            group_data = normalize_data[idx, :]\n\n            normalized_group = self._normalize_helper(group_data, method=method, use_nonmissing=use_nonmissing, **kwargs)\n            normalize_data[idx, :] = normalized_group\n\n    # summary printout\n    summary_lines = []\n    if classes is None:\n        summary_lines.append(f\"{format_log_prefix('result_only', indent=2)} Normalized all {normalize_data.shape[0]} samples.\")\n    else:\n        for group in unique_groups:\n            count = np.sum(sample_names == group)\n            summary_lines.append(f\"   - {group}: {count} samples normalized\")\n        summary_lines.insert(0, f\"{format_log_prefix('result_only', indent=2)} Normalized {normalize_data.shape[0]} samples total.\")\n    print(\"\\n\".join(summary_lines))\n\n    adata.layers[layer_name] = sparse.csr_matrix(normalize_data) if was_sparse else normalize_data\n\n    if set_X:\n        self.set_X(layer = layer_name, on = on) # type: ignore[attr-defined], EditingMixin\n\n    # Determine if use_nonmissing note should be added\n    note = \"\"\n    if use_nonmissing and method in {'sum', 'mean', 'median', 'max'}:\n        note = \" (using only fully observed columns)\"\n\n    self._history.append( # type: ignore[attr-defined], HistoryMixin\n        f\"{on}: Normalized layer {layer} using {method}{note} (grouped by {classes}). Stored in `{layer_name}`.\"\n        )\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.pca","title":"pca","text":"<pre><code>pca(on='protein', layer='X', **kwargs)\n</code></pre> <p>Perform PCA (Principal Component Analysis) on protein or peptide data.</p> <p>This method performs PCA on the selected data layer, after z-score normalization and removal of NaN-containing features. The results are stored in <code>.obsm[\"X_pca\"]</code> and <code>.uns[\"pca\"]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Whether to use \"protein\" or \"peptide\" data.</p> <code>'protein'</code> <code>layer</code> <code>str</code> <p>Data layer to use for PCA (default is \"X\").</p> <code>'X'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>scanpy.tl.pca()</code>. For example, <code>key_added</code> to store PCA in a different key.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Note <ul> <li>Features (columns) with NaN values are excluded before PCA and then padded with zeros.</li> <li>PCA scores are stored in <code>.obsm['X_pca']</code>.</li> <li>Principal component loadings, variance ratios, and total variances are stored in <code>.uns['pca']</code>.</li> <li>If you store PCs under a custom key using <code>key_added</code>, remember to set <code>use_rep</code> when calling <code>.neighbor()</code> or <code>.umap()</code>.</li> </ul> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def pca(self, on = 'protein', layer = \"X\", **kwargs):\n    \"\"\"\n    Perform PCA (Principal Component Analysis) on protein or peptide data.\n\n    This method performs PCA on the selected data layer, after z-score normalization and removal of\n    NaN-containing features. The results are stored in `.obsm[\"X_pca\"]` and `.uns[\"pca\"]`.\n\n    Args:\n        on (str): Whether to use \"protein\" or \"peptide\" data.\n        layer (str): Data layer to use for PCA (default is \"X\").\n        **kwargs: Additional keyword arguments passed to `scanpy.tl.pca()`. For example,\n            `key_added` to store PCA in a different key.\n\n    Returns:\n        None\n\n    Note:\n        - Features (columns) with NaN values are excluded before PCA and then padded with zeros.\n        - PCA scores are stored in `.obsm['X_pca']`.\n        - Principal component loadings, variance ratios, and total variances are stored in `.uns['pca']`.\n        - If you store PCs under a custom key using `key_added`, remember to set `use_rep` when calling `.neighbor()` or `.umap()`.\n    \"\"\"\n\n    # uses sc.tl.pca\n    # for kwargs can use key_added to store PCA in a different key - then for neighbors need to specify key by use_rep\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        pass\n\n    if on == 'protein':\n        adata = self.prot\n    elif on == 'peptide':\n        adata = self.pep\n\n    # make sample array\n    if layer == \"X\":\n        X = adata.X.toarray()\n    elif layer in adata.layers.keys():\n        X = adata.layers[layer].toarray()\n\n    log_prefix = format_log_prefix(\"user\")\n    print(f\"{log_prefix} Performing PCA [{on}] using layer: {layer}, removing NaN features.\")\n    print(f\"   \ud83d\udd38 BEFORE (samples \u00d7 proteins): {X.shape}\")\n    Xnorm = (X - X.mean(axis=0)) / X.std(axis=0)\n    nan_cols = np.isnan(Xnorm).any(axis=0)\n    Xnorm = Xnorm[:, ~nan_cols]\n    print(f\"   \ud83d\udd38 AFTER  (samples \u00d7 proteins): {Xnorm.shape}\")\n\n    # TODO: fix bug here (ValueError: n_components=59 must be between 1 and min(n_samples, n_features)=31 with svd_solver='arpack')\n    pca_data = sc.tl.pca(Xnorm, return_info=True, **kwargs)\n    adata.obsm['X_pca'] = pca_data[0]\n    PCs = np.zeros((pca_data[1].shape[0], nan_cols.shape[0]))\n\n    # fill back the 0s where column was NaN in the original data, and thus not used in PCA\n    counter = 0\n    for i in range(PCs.shape[1]):\n        if not nan_cols[i]:\n            PCs[:, i] = pca_data[1][:, counter]\n            counter += 1\n\n    adata.uns['pca'] = {'PCs': PCs, 'variance_ratio': pca_data[2], 'variance': pca_data[3]}\n\n    subpdata = \"prot\" if on == 'protein' else \"pep\"\n\n    self._append_history(f'{on}: PCA fitted on {layer}, stored in obsm[\"X_pca\"] and varm[\"PCs\"]') # type: ignore[attr-defined], HistoryMixin\n    print(f\"{format_log_prefix('result_only',indent=2)} PCA complete, fitted on {layer}. Results stored in:\")\n    print(f\"       \u2022 .{subpdata}.obsm['X_pca']\")\n    print(f\"       \u2022 .{subpdata}.uns['pca'] (includes PCs, variance, variance ratio)\")\n    var_pc1, var_pc2 = pca_data[2][:2]\n    print(f\"       \u2022 Variance explained by PC1/PC2: {var_pc1*100:.2f}% , {var_pc2*100:.2f}%\") \n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.rank","title":"rank","text":"<pre><code>rank(classes=None, on='protein', layer='X')\n</code></pre> <p>Rank proteins or peptides by average abundance across sample groups.</p> <p>This method computes the average and standard deviation for each feature within  each group and assigns a rank (highest to lowest) based on the group-level mean. The results are stored in <code>.var</code> with one set of columns per group.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>str or list of str</code> <p>Sample-level class/grouping column(s) in <code>.obs</code>.</p> <code>None</code> <code>on</code> <code>str</code> <p>Whether to compute ranks on \"protein\" or \"peptide\" data.</p> <code>'protein'</code> <code>layer</code> <code>str</code> <p>Name of the data layer to use (default is \"X\").</p> <code>'X'</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Rank proteins by average abundance across treatment groups:     <pre><code>pdata.rank(classes=\"treatment\", on=\"protein\", layer=\"X_norm\")\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def rank(self, classes = None, on = 'protein', layer = \"X\"):\n    \"\"\"\n    Rank proteins or peptides by average abundance across sample groups.\n\n    This method computes the average and standard deviation for each feature within \n    each group and assigns a rank (highest to lowest) based on the group-level mean.\n    The results are stored in `.var` with one set of columns per group.\n\n    Args:\n        classes (str or list of str, optional): Sample-level class/grouping column(s) in `.obs`.\n        on (str): Whether to compute ranks on \"protein\" or \"peptide\" data.\n        layer (str): Name of the data layer to use (default is \"X\").\n\n    Returns:\n        None\n\n    Example:\n        Rank proteins by average abundance across treatment groups:\n            ```python\n            pdata.rank(classes=\"treatment\", on=\"protein\", layer=\"X_norm\")\n            ```\n    \"\"\"\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        pass\n\n    adata = self.prot if on == 'protein' else self.pep\n    classes_list = utils.get_classlist(adata, classes)\n\n    for class_value in classes_list:\n        rank_data = utils.resolve_class_filter(adata, classes, class_value)\n        if layer == \"X\":\n            layer_data = rank_data.X.toarray()\n        elif layer in rank_data.layers:\n            layer_data = rank_data.layers[layer].toarray()\n        else:\n            raise ValueError(f\"Layer '{layer}' not found in layers.\")\n\n        # Convert sparse to dense if needed\n        if hasattr(layer_data, 'toarray'):\n            layer_data = layer_data.toarray()\n\n        # Transpose to get DataFrame of shape (features, samples)\n        rank_df = pd.DataFrame(layer_data.T, index=rank_data.var.index, columns=rank_data.obs_names)\n\n        # Compute stats\n        avg_col = f\"Average: {class_value}\"\n        std_col = f\"Stdev: {class_value}\"\n        rank_col = f\"Rank: {class_value}\"\n\n        with np.errstate(invalid='ignore', divide='ignore'):\n            rank_df[avg_col] = np.nanmean(layer_data, axis=0)\n            rank_df[std_col] = np.nanstd(layer_data, axis=0)\n\n        # Sort by average (descending), assign rank\n        rank_df.sort_values(by=avg_col, ascending=False, inplace=True)\n        rank_df[rank_col] = np.where(rank_df[avg_col].isna(), np.nan, np.arange(1, len(rank_df) + 1))\n\n        # Reindex back to original order in adata.var\n        rank_df = rank_df.reindex(adata.var.index)\n\n        adata.var[avg_col] = rank_df[avg_col]\n        adata.var[std_col] = rank_df[std_col]\n        adata.var[rank_col] = rank_df[rank_col]\n\n    self._history.append(f\"{on}: Ranked {layer} data. Ranking, average and stdev stored in var.\") # type: ignore[attr-defined], HistoryMixin\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.analysis.AnalysisMixin.umap","title":"umap","text":"<pre><code>umap(on='protein', layer='X', **kwargs)\n</code></pre> <p>Compute UMAP dimensionality reduction on protein or peptide data.</p> <p>This method runs UMAP (Uniform Manifold Approximation and Projection) on the selected data layer using <code>scanpy.tl.umap()</code>. If neighbor graphs are not already computed, they will be generated automatically.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Whether to use \"protein\" or \"peptide\" data.</p> <code>'protein'</code> <code>layer</code> <code>str</code> <p>Data layer to use for UMAP (default is \"X\").</p> <code>'X'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>scanpy.tl.umap()</code>, <code>scanpy.tl.neighbor()</code> or the scpviz <code>pca</code> function. Example:     \"n_neighbors\": neighbor argument     \"min_dist\": umap argument     \"metric\": neighbor argument     \"spread\": umap argument     \"random_state\": umap argument     \"n_pcs\": neighbor argument</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Run UMAP using default settings:     <pre><code>pdata.umap(on=\"protein\", layer=\"X\")\n</code></pre></p> <p>Note:     - UMAP coordinates are stored in <code>.obsm[\"X_umap\"]</code>.     - UMAP settings are stored in <code>.uns[\"umap\"]</code>.     - Automatically computes neighbor graphs if not already available.     - Will call <code>.set_X()</code> if a non-default layer is used.</p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def umap(self, on = 'protein', layer = \"X\", **kwargs):\n    \"\"\"\n    Compute UMAP dimensionality reduction on protein or peptide data.\n\n    This method runs UMAP (Uniform Manifold Approximation and Projection) on the selected data layer using `scanpy.tl.umap()`.\n    If neighbor graphs are not already computed, they will be generated automatically.\n\n    Args:\n        on (str): Whether to use \"protein\" or \"peptide\" data.\n        layer (str): Data layer to use for UMAP (default is \"X\").\n        **kwargs: Additional keyword arguments passed to `scanpy.tl.umap()`, `scanpy.tl.neighbor()` or the scpviz `pca` function.\n            Example:\n                \"n_neighbors\": neighbor argument\n                \"min_dist\": umap argument\n                \"metric\": neighbor argument\n                \"spread\": umap argument\n                \"random_state\": umap argument\n                \"n_pcs\": neighbor argument\n\n    Returns:\n        None\n\n    Example:\n        Run UMAP using default settings:\n            ```python\n            pdata.umap(on=\"protein\", layer=\"X\")\n            ```\n    Note:\n        - UMAP coordinates are stored in `.obsm[\"X_umap\"]`.\n        - UMAP settings are stored in `.uns[\"umap\"]`.\n        - Automatically computes neighbor graphs if not already available.\n        - Will call `.set_X()` if a non-default layer is used.\n    \"\"\"\n    # uses sc.tl.umap\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        pass\n\n    if on == 'protein':\n        adata = self.prot\n    elif on == 'peptide':\n        adata = self.pep\n\n    log_prefix = format_log_prefix(\"user\")\n    print(f\"{log_prefix} Computing UMAP [{on}] using layer: {layer}\")\n\n    if \"n_neighbors\" in kwargs or \"metric\" in kwargs or \"n_pcs\" in kwargs:\n                n_neighbors = kwargs.pop(\"n_neighbors\", None)\n                metric = kwargs.pop(\"metric\", None)\n                n_pcs = kwargs.pop(\"n_pcs\", None)\n\n                # Prepare a readable message\n                neighbor_args = []\n                if n_neighbors is not None:\n                    neighbor_args.append(f\"n_neighbors={n_neighbors}\")\n                else:\n                    n_neighbors = 15  # default value\n                if metric is not None:\n                    neighbor_args.append(f\"metric='{metric}'\")\n                else:\n                    metric = \"euclidean\"  # default value\n                if n_pcs is not None:\n                    neighbor_args.append(f\"n_pcs={n_pcs}\")\n                else:\n                    n_pcs = 50\n                arg_str = \", \".join(neighbor_args)\n\n                print(f\"{format_log_prefix('info_only', indent=2)} {arg_str} provided. \"\n                    f\"Re-running neighbors with these settings before UMAP.\")\n\n                self.neighbor(on=on, layer=layer, n_neighbors=n_neighbors, metric=metric, user_indent=2)\n                self._append_history(f\"{on}: Neighbors re-computed with {arg_str} before UMAP\")  # type: ignore[attr-defined], HistoryMixin\n    else:\n        # check if neighbor has been run before, look for distances and connectivities in obsp\n        if 'neighbors' not in adata.uns:\n            print(f\"{format_log_prefix('info_only', indent=2)} Neighbors not found in AnnData object. Running neighbors with default settings.\")\n            self.neighbor(on = on, layer = layer)\n            self._append_history(f\"{on}: Neighbors computed with default settings before UMAP\")  # type: ignore[attr-defined], HistoryMixin\n        else:\n            print(f\"{format_log_prefix('info_only', indent=2)} Using existing neighbors found in AnnData object.\")\n\n    if layer == \"X\":\n        # do nothing\n        pass\n    elif layer in adata.layers.keys():\n        self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n    sc.tl.umap(adata, **kwargs)\n\n    self._append_history(f'{on}: UMAP fitted on {layer}, stored in obsm[\"X_umap\"] and uns[\"umap\"]') # type: ignore[attr-defined], HistoryMixin\n    print(f\"{format_log_prefix('result_only', indent=2)} UMAP complete. Results stored in:\")\n    print(f\"       \u2022 obsm['X_umap'] (UMAP coordinates)\")\n    print(f\"       \u2022 uns['umap'] (UMAP settings)\")\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.enrichment.EnrichmentMixin","title":"EnrichmentMixin","text":"<p>Provides methods for STRING-based functional and protein\u2013protein interaction (PPI) enrichment.</p> <p>This mixin includes utilities for:</p> <ul> <li>Running functional enrichment on differentially expressed or user-supplied gene lists.</li> <li>Performing STRING PPI enrichment to identify interaction networks.</li> <li>Generating STRING network visualization links and embedded SVGs.</li> <li>Listing and accessing enrichment results stored in <code>.stats</code>.</li> </ul> <p>Methods:</p> Name Description <code>enrichment_functional</code> <p>Runs STRING functional enrichment on DE results or a custom gene list.</p> <code>enrichment_ppi</code> <p>Runs STRING PPI enrichment on a user-supplied gene or accession list.</p> <code>list_enrichments</code> <p>Lists available enrichment results and DE comparisons.</p> <code>plot_enrichment_svg</code> <p>Displays a STRING enrichment SVG inline or saves it to file.</p> <code>get_string_mappings</code> <p>Maps UniProt accessions to STRING IDs using the STRING API.</p> <code>resolve_to_accessions</code> <p>Resolves gene names or mixed inputs to accessions using internal mappings.</p> <code>get_string_network_link</code> <p>Generates a direct STRING network URL for visualization.</p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>class EnrichmentMixin:\n    \"\"\"\n    Provides methods for STRING-based functional and protein\u2013protein interaction (PPI) enrichment.\n\n    This mixin includes utilities for:\n\n    - Running functional enrichment on differentially expressed or user-supplied gene lists.\n    - Performing STRING PPI enrichment to identify interaction networks.\n    - Generating STRING network visualization links and embedded SVGs.\n    - Listing and accessing enrichment results stored in `.stats`.\n\n    Functions:\n        enrichment_functional: Runs STRING functional enrichment on DE results or a custom gene list.\n        enrichment_ppi: Runs STRING PPI enrichment on a user-supplied gene or accession list.\n        list_enrichments: Lists available enrichment results and DE comparisons.\n        plot_enrichment_svg: Displays a STRING enrichment SVG inline or saves it to file.\n        get_string_mappings: Maps UniProt accessions to STRING IDs using the STRING API.\n        resolve_to_accessions: Resolves gene names or mixed inputs to accessions using internal mappings.\n        get_string_network_link: Generates a direct STRING network URL for visualization.\n    \"\"\"\n\n    def get_string_mappings(self, identifiers, overwrite=False, cache_col=\"STRING\", batch_size=100, debug=False):\n        \"\"\"\n        Resolve STRING IDs for UniProt accessions with a 2-step strategy:\n        1) Use UniProt stream (fields: xref_string) to fill cache quickly.\n        2) For any still-missing rows, query STRING get_string_ids, batched by organism_id.\n\n        This method retrieves corresponding STRING identifiers for a list of UniProt accessions\n        and stores the result in `self.prot.var[\"STRING_id\"]` for downstream use.\n\n        Args:\n            identifiers (list of str): List of UniProt accession IDs to map.\n            batch_size (int): Number of accessions to include in each API query (default is 300).\n            debug (bool): If True, prints progress and response info.\n\n        Returns:\n            pd.DataFrame: Mapping table with columns: `input_identifier`, `string_identifier`, and `ncbi_taxon_id`.\n\n        Note:\n            This is a helper method used primarily by `enrichment_functional()` and `enrichment_ppi()`.\n        \"\"\"\n        print(f\"[INFO] Resolving STRING IDs for {len(identifiers)} identifiers...\") if debug else None\n\n        prot_var = self.prot.var\n        if cache_col not in prot_var.columns:\n            prot_var[cache_col] = pd.NA\n\n        # Use cached STRING IDs if available\n        valid_ids = [i for i in identifiers if i in prot_var.index]\n        existing = prot_var.loc[valid_ids, cache_col]\n        found_ids = {i: sid for i, sid in existing.items() if pd.notna(sid)}\n        missing = [i for i in identifiers if i not in found_ids]\n\n        if overwrite:\n            # If overwriting, treat all valid IDs as missing for fresh pull\n            print(f\"{format_log_prefix('info_only',2)} Overwriting cached STRING IDs.\")\n            missing = valid_ids\n            found_ids = {}\n\n        print(f\"{format_log_prefix('info_only',2)} Found {len(found_ids)} cached STRING IDs. {len(missing)} need lookup.\")\n        print(missing) if debug else None\n\n        # -----------------------------\n        # Step 1: UniProt stream (fast)         # Use UniProt xref_string field to fill cache quickly\n        # -----------------------------\n\n        uni_results = [] \n        species_map = {}\n\n        if missing:\n            try:\n                dfu = get_uniprot_fields(missing, search_fields=['xref_string', 'organism_id'], batch_size=100, standardize=True, verbose=debug)\n                print(dfu) if debug else None\n\n                if dfu is not None and not dfu.empty:\n                    entry_col = \"accession\" if \"accession\" in dfu.columns else None\n                    xref_col  = \"xref_string\" if \"xref_string\" in dfu.columns else None\n                    org_col   = \"organism_id\" if \"organism_id\" in dfu.columns else None\n\n                    if entry_col and xref_col:\n                        # Parse first STRING ID if multiple are returned\n                        def _first_string(s):\n                            if pd.isna(s):\n                                return np.nan\n                            s = str(s).strip()\n                            if not s:\n                                return np.nan\n                            return s.split(';')[0].strip()\n\n                        dfu['__STRING__'] = dfu[xref_col].apply(_first_string)\n\n                        for _, row in dfu.iterrows():\n                            acc = row[entry_col]\n                            sid = row['__STRING__']\n\n                            # capture organism id if present\n                            if org_col in dfu.columns:\n                                org_val = row[org_col]\n                                if pd.notna(org_val) and str(org_val).strip():\n                                    try:\n                                        species_map[acc] = int(org_val)\n                                    except Exception:\n                                        # keep as raw if not int-castable\n                                        species_map[acc] = org_val\n\n                            if acc in prot_var.index and pd.notna(sid) and str(sid).strip():\n                                if overwrite or pd.isna(prot_var.at[acc, cache_col]) or not str(prot_var.at[acc, cache_col]).strip():\n                                    prot_var.at[acc, cache_col] = sid\n                                    prot_var.at[acc, \"ncbi_taxon_id\"] = str(species_map.get(acc, np.nan)) if pd.notna(species_map.get(acc, np.nan)) else np.nan\n                                    found_ids[acc] = sid\n                                    uni_results.append({\"input_identifier\": acc, \"string_identifier\": sid})\n\n                print(f\"{format_log_prefix('info_only',3)} Cached {len(uni_results)} STRING IDs from UniProt API xref_string.\")\n            except Exception as e:\n                print(f\"[WARN] UniProt stream step failed: {e}\")\n\n        # Recompute missing after UniProt step\n        missing = [i for i in identifiers if i not in found_ids]\n\n        # -----------------------------------------\n        # STEP 2: STRING API for still-missing ones\n        # -----------------------------------------\n\n        if not missing:\n            # nothing left to resolve via STRING\n            if debug:\n                print(f\"[INFO] All identifiers resolved via UniProt: {found_ids}\")\n            all_rows=[]\n\n        else:\n            all_rows = []\n\n            for i in range(0, len(missing), batch_size):\n                batch = missing[i:i + batch_size]\n                print(f\"{format_log_prefix('info')} Querying STRING for batch {i // batch_size + 1} ({len(batch)} identifiers)...\") if debug else None\n\n                url = \"https://string-db.org/api/tsv-no-header/get_string_ids\"\n                params = {\n                    \"identifiers\": \"\\r\".join(batch),\n                    \"limit\": 1,\n                    \"echo_query\": 1,\n                    \"caller_identity\": \"scpviz\"\n                }\n\n                try:\n                    t0 = time.time()\n                    response = requests.post(url, data=params)\n                    response.raise_for_status()\n                    df = pd.read_csv(StringIO(response.text), sep=\"\\t\", header=None)\n                    df.columns = [\n                        \"input_identifier\", \"input_alias\", \"string_identifier\", \"ncbi_taxon_id\",\n                        \"preferred_name\", \"annotation\", \"score\"\n                    ]\n                    print(f\"[INFO] Batch completed in {time.time() - t0:.2f}s\") if debug else None\n                    all_rows.append(df)\n                except Exception as e:\n                    print(f\"[ERROR] Failed on batch {i // batch_size + 1}: {e}\") if debug else None\n\n        # Combine all new mappings\n        if all_rows:\n            new_df = pd.concat(all_rows, ignore_index=True)\n            updated_ids = []\n\n            for _, row in new_df.iterrows():\n                acc = row[\"input_identifier\"]\n                sid = row[\"string_identifier\"]\n                if acc in self.prot.var.index:\n                    self.prot.var.at[acc, cache_col] = sid\n                    found_ids[acc] = sid\n                    updated_ids.append(acc)\n                else:\n                    print(f\"[DEBUG] Skipping unknown accession '{acc}'\")\n\n            print(f\"{format_log_prefix('info_only',3)} Cached {len(updated_ids)} new STRING ID mappings from STRING API.\")\n        elif missing:\n            print(f\"{format_log_prefix('warn_only',3)} No STRING mappings returned from STRING API.\")\n\n\n        # ------------------------------------\n        # Build and MERGE UniProt results into out_df\n        # ------------------------------------\n        out_df = pd.DataFrame.from_dict(found_ids, orient=\"index\", columns=[\"string_identifier\"])\n        out_df.index.name = \"input_identifier\"\n        out_df = out_df.reset_index()\n\n        if uni_results:\n            uni_df = pd.DataFrame(uni_results).dropna().drop_duplicates(subset=[\"input_identifier\"])\n            out_df = out_df.merge(uni_df, on=\"input_identifier\", how=\"left\", suffixes=(\"\", \"_uni\"))\n            out_df[\"string_identifier\"] = out_df[\"string_identifier\"].combine_first(out_df[\"string_identifier_uni\"])\n            out_df = out_df.drop(columns=[\"string_identifier_uni\"])\n\n        # Use species_map (from UniProt and/or STRING) for ncbi_taxon_id\n        from_map = out_df[\"input_identifier\"].map(lambda acc: species_map.get(acc, np.nan))\n        from_cache = out_df[\"input_identifier\"].map(lambda acc: prot_var.at[acc, \"ncbi_taxon_id\"] if acc in prot_var.index else np.nan)\n        out_df[\"ncbi_taxon_id\"] = from_map.combine_first(from_cache)\n\n        return out_df\n\n\n    def resolve_to_accessions(self, mixed_list):\n        \"\"\"\n        Convert gene names or accessions into standardized UniProt accession IDs.\n\n        This method resolves input items using the internal gene-to-accession map,\n        ensuring all returned entries are accessions present in the `.prot` object.\n\n        Args:\n            mixed_list (list of str): A list containing gene names and/or UniProt accessions.\n\n        Returns:\n            list of str: List of resolved UniProt accession IDs.\n\n        Note:\n            This function is similar to `utils.resolve_accessions()` but operates in the context \n            of the current `pAnnData` object and its internal gene mappings.\n\n        Todo:\n            Add example comparing results from `resolve_to_accessions()` and `utils.resolve_accessions()`.\n        \"\"\"\n        gene_to_acc, _ = self.get_gene_maps(on='protein') \n        accs = []\n        unresolved_accs = []\n        for item in mixed_list:\n            if item in self.prot.var.index:\n                accs.append(item)  # already an accession\n            elif item in gene_to_acc:\n                accs.append(gene_to_acc[item])\n            else:\n                unresolved_accs.append(item)\n                # print(f\"{format_log_prefix('warn_only',2)} Could not resolve '{item}' to an accession \u2014 skipping.\")\n        return accs, unresolved_accs\n\n    def enrichment_functional(\n        self,\n        genes=None,\n        from_de=True,\n        top_n=150,\n        score_col=\"significance_score\",\n        gene_col=\"Genes\",\n        de_key=\"de_results\",\n        store_key=None,\n        species=None,\n        background=None,\n        debug=False,\n        **kwargs\n    ):\n        \"\"\"\n        Run functional enrichment analysis using STRING on a gene list.\n\n        This method performs ranked or unranked enrichment analysis using STRING's API.\n        It supports both differential expression-based analysis (up- and down-regulated genes)\n        and custom gene lists provided by the user. Enrichment results are stored in\n        `.stats[\"functional\"]` for later access and plotting.\n\n        Args:\n            genes (list of str, optional): List of gene symbols to analyze. Ignored if `from_de=True`.\n            from_de (bool): If True (default), selects genes from stored differential expression results.\n            top_n (int): Number of top-ranked genes to use when `from_de=True` (default is 150).\n            score_col (str): Column name in the DE table to rank genes by (default is `\"significance_score\"`).\n            gene_col (str): Column name in `.prot.var` or DE results that contains gene names.\n            de_key (str): Key to retrieve stored DE results from `.stats[\"de_results\"]`.\n            store_key (str, optional): Custom key to store enrichment results. Ignored when `from_de=True`.\n            species (str, optional): Organism name or NCBI taxonomy ID. If None, inferred from STRING response.\n            background (str or list of str, optional): Background gene list to use for enrichment.\n\n                - If `\"all_quantified\"`, uses non-significant proteins from DE or all other quantified proteins.\n                - If a list, must contain valid gene names or accessions.\n            debug (bool): If True, prints API request info and diagnostic messages.\n            **kwargs: Additional keyword arguments passed to the STRING enrichment API.\n\n        Returns:\n            dict or pd.DataFrame:\n\n                - If `from_de=True`, returns a dictionary of enrichment DataFrames for \"up\" and \"down\" gene sets.\n                - If `genes` is provided, returns a single enrichment DataFrame.\n\n        Example:\n            Run differential expression, then perform STRING enrichment on top-ranked genes:\n                ```python\n                case1 = {'cellline': 'AS', 'treatment': 'sc'} # legacy style: class_type = [\"group\", \"condition\"]\n                case2 = {'cellline': 'BE', 'treatment': 'sc'} # legacy style: values = [[\"GroupA\", \"Treatment1\"], [\"GroupA\", \"Control\"]]\n                pdata_nb.de(values = case_values) # or legacy style: pdata.de(classes=class_type, values=values)\n                pdata.list_enrichments()  # list available DE result keys\n                pdata.enrichment_functional(from_de=True, de_key=\"GroupA_Treatment1 vs GroupA_Control\")\n                ```\n\n            Perform enrichment on a custom list of genes:\n                ```python\n                genelist = [\"P55072\", \"NPLOC4\", \"UFD1\", \"STX5A\", \"NSFL1C\", \"UBXN2A\",\n                            \"UBXN4\", \"UBE4B\", \"YOD1\", \"WASHC5\", \"PLAA\", \"UBXN10\"]\n                pdata.enrichment_functional(genes=genelist, from_de=False)\n                ```\n\n        Note:\n            Internally uses `resolve_to_accessions()` and `get_string_mappings()`, and stores results \n            in `.stats[\"functional\"]`. Results can be accessed or visualized via `plot_enrichment_svg()`\n            or by visiting the linked STRING URLs.\n        \"\"\"\n        def query_functional_enrichment(query_ids, species_id, background_ids=None, debug=False):\n            print(f\"{format_log_prefix('info_only',2)} Running enrichment on {len(query_ids)} STRING IDs (species {species_id})...\") if debug else None\n            url = \"https://string-db.org/api/json/enrichment\"\n            payload = {\n                \"identifiers\": \"%0d\".join(query_ids),\n                \"species\": species_id,\n                \"caller_identity\": \"scpviz\"\n            }\n            if background_ids is not None:\n                print(f\"{format_log_prefix('info_only')} Using background of {len(background_ids)} STRING IDs.\")\n                payload[\"background_string_identifiers\"] = \"%0d\".join(background_ids)\n\n            print(payload) if debug else None\n            response = requests.post(url, data=payload)\n            response.raise_for_status()\n            return pd.DataFrame(response.json())\n\n        # Ensure string metadata section exists\n        if \"functional\" not in self.stats:\n            self.stats[\"functional\"] = {}\n\n        if genes is None and from_de:\n            resolved_key = _resolve_de_key(self.stats, de_key)\n            de_df = self.stats[resolved_key]\n            sig_df = de_df[de_df[\"significance\"] != \"not significant\"].copy()\n            print(f\"{format_log_prefix('user')} Running STRING enrichment [DE-based: {resolved_key}]\")\n\n            up_genes = sig_df[sig_df[score_col] &gt; 0][gene_col].dropna().head(top_n).tolist()\n            down_genes = sig_df[sig_df[score_col] &lt; 0][gene_col].dropna().head(top_n).tolist()\n\n            up_accs, up_unresolved = self.resolve_to_accessions(up_genes)\n            down_accs, down_unresolved = self.resolve_to_accessions(down_genes)\n\n            background_accs = None\n            background_string_ids = None\n            if background == \"all_quantified\":\n                print(f\"{format_log_prefix('warn')} Mapping background proteins may take a long time due to batching.\")\n                background_accs = de_df[de_df[\"significance\"] == \"not significant\"].index.tolist()\n\n            if background_accs:\n                bg_map = self.get_string_mappings(background_accs,debug=debug)\n                bg_map = bg_map[bg_map[\"string_identifier\"].notna()]\n                background_string_ids = bg_map[\"string_identifier\"].tolist()\n\n            if store_key is not None:\n                print(f\"{format_log_prefix('warn')} Ignoring `store_key` for DE-based enrichment. Using auto-generated pretty keys.\")\n\n            results = {}\n            for label, accs in zip([\"up\", \"down\"], [up_accs, down_accs]):\n                print(f\"\\n\ud83d\udd39 {label.capitalize()}-regulated proteins\")\n                t0 = time.time()\n\n                if not accs:\n                    print(f\"{format_log_prefix('warn')} No {label}-regulated proteins to analyze.\")\n                    continue\n\n                mapping_df = self.get_string_mappings(accs, debug=debug)\n                mapping_df = mapping_df[mapping_df[\"string_identifier\"].notna()]\n                if mapping_df.empty:\n                    print(f\"{format_log_prefix('warn')} No valid STRING mappings found for {label}-regulated proteins.\")\n                    continue\n\n                string_ids = mapping_df[\"string_identifier\"].tolist()\n                inferred_species = mapping_df[\"ncbi_taxon_id\"].mode().iloc[0]\n                if species is not None:\n                    # check if user species is same as inferred\n                    if inferred_species != species:\n                        print(f\"{format_log_prefix('warn',2)} Inferred species ({inferred_species}) does not match user-specified ({species}). Using user-specified species.\")\n                    species_id = species\n                else:\n                    species_id = inferred_species\n\n                print(f\"   \ud83d\udd38 Proteins: {len(accs)} \u2192 STRING IDs: {len(string_ids)}\")\n                print(f\"   \ud83d\udd38 Species: {species_id} | Background: {'None' if background_string_ids is None else 'custom'}\")\n                if label == \"up\":\n                    if up_unresolved:\n                        print(f\"{format_log_prefix('warn',2)} Some accessions unresolved for {label}-regulated proteins: {', '.join(up_unresolved)}\")\n                else:\n                    if down_unresolved:\n                        print(f\"{format_log_prefix('warn',2)} Some accessions unresolved for {label}-regulated proteins: {', '.join(down_unresolved)}\")\n\n                enrichment_df = query_functional_enrichment(string_ids, species_id, background_string_ids, debug=debug)\n                enrich_key = f\"{resolved_key}_{label}\"\n                pretty_base = _pretty_vs_key(resolved_key)\n                pretty_key = f\"{pretty_base}_{label}\"\n                string_url = self.get_string_network_link(string_ids=string_ids, species=species_id)\n\n                self.stats[\"functional\"][pretty_key] = {\n                    \"string_ids\": string_ids,\n                    \"background_string_ids\": background_string_ids,\n                    \"species\": species_id,\n                    \"input_key\": resolved_key if from_de else None,\n                    \"string_url\": string_url,\n                    \"result\": enrichment_df\n                }\n\n                print(f\"{format_log_prefix('result')} Enrichment complete ({time.time() - t0:.2f}s)\")\n                print(f\"   \u2022 Access result: pdata.stats['functional'][\\\"{pretty_key}\\\"][\\\"result\\\"]\")\n                print(f\"   \u2022 Plot command : pdata.plot_enrichment_svg(\\\"{pretty_base}\\\", direction=\\\"{label}\\\")\")\n                print(f\"   \u2022 View online  : {string_url}\\n\")\n\n                results[label] = enrichment_df\n\n        elif genes is not None:\n            t0 = time.time()\n            print(f\"{format_log_prefix('user')} Running STRING enrichment [user-supplied]\")\n\n            if store_key is None:\n                prefix = \"UserSearch\"\n                existing = self.stats[\"functional\"].keys() if \"functional\" in self.stats else []\n                existing_ids = [k for k in existing if k.startswith(prefix)]\n                next_id = len(existing_ids) + 1\n                store_key = f\"{prefix}{next_id}\"\n\n            input_accs, unresolved_accs = self.resolve_to_accessions(genes)\n            mapping_df = self.get_string_mappings(input_accs, debug=debug)\n            mapping_df = mapping_df[mapping_df[\"string_identifier\"].notna()]\n            if mapping_df.empty:\n                raise ValueError(\"No valid STRING mappings found for the provided identifiers.\")\n\n            string_ids = mapping_df[\"string_identifier\"].tolist()\n            inferred_species = mapping_df[\"ncbi_taxon_id\"].mode().iloc[0]\n            if species is not None:\n                # check if user species is same as inferred\n                if inferred_species != species:\n                    print(f\"{format_log_prefix('warn',2)} Inferred species ({inferred_species}) does not match user-specified ({species}). Using user-specified species.\")\n                species_id = species\n            else:\n                species_id = inferred_species\n\n            background_string_ids = None\n            if background == \"all_quantified\":\n                print(f\"{format_log_prefix('warn')} Mapping background proteins may take a long time due to batching.\")\n                all_accs = list(self.prot.var_names)\n                background_accs = list(set(all_accs) - set(input_accs))\n                bg_map = self.get_string_mappings(background_accs, debug=debug)\n                bg_map = bg_map[bg_map[\"string_identifier\"].notna()]\n                background_string_ids = bg_map[\"string_identifier\"].tolist()\n\n            print(f\"   \ud83d\udd38 Input genes: {len(genes)} \u2192 Resolved STRING IDs: {len(string_ids)}\")\n            print(f\"   \ud83d\udd38 Species: {species_id} | Background: {'None' if background_string_ids is None else 'custom'}\")\n            if unresolved_accs:\n                print(f\"{format_log_prefix('warn',2)} Some accessions unresolved: {', '.join(unresolved_accs)}\")\n\n            enrichment_df = query_functional_enrichment(string_ids, species_id, background_string_ids, debug=debug)\n            string_url = self.get_string_network_link(string_ids=string_ids, species=species_id)\n\n            self.stats[\"functional\"][store_key] = {\n                \"string_ids\": string_ids,\n                \"background_string_ids\": background_string_ids,\n                \"species\": species_id,\n                \"input_key\": None,\n                \"string_url\": string_url,\n                \"result\": enrichment_df\n            }\n\n            print(f\"{format_log_prefix('result')} Enrichment complete ({time.time() - t0:.2f}s)\")\n            print(f\"   \u2022 Access result: pdata.stats['functional'][\\\"{store_key}\\\"][\\\"result\\\"]\")\n            print(f\"   \u2022 Plot command : pdata.plot_enrichment_svg(\\\"{store_key}\\\")\")\n            print(f\"   \u2022 View online  : {string_url}\\n\")\n\n            return enrichment_df\n\n        else:\n            raise ValueError(\"Must provide 'genes' or set from_de=True to use DE results.\") \n\n    def enrichment_ppi(self, genes, species=None, store_key=None, debug=False):\n        \"\"\"\n        Run STRING PPI (protein\u2013protein interaction) enrichment on a user-supplied gene or accession list.\n\n        This method maps the input gene names or UniProt accessions to STRING IDs, infers the species \n        if not provided, and submits the list to STRING's PPI enrichment endpoint. Results are stored \n        in `.stats[\"ppi\"]` for later retrieval or visualization.\n\n        Args:\n            genes (list of str): A list of gene names or UniProt accessions to analyze.\n            species (int or str, optional): NCBI taxonomy ID (e.g., 9606 for human). If None, inferred from STRING mappings.\n            store_key (str, optional): Key to store the enrichment result under `.stats[\"ppi\"]`.\n                If None, a unique key is auto-generated.\n\n        Returns:\n            pd.DataFrame: DataFrame of STRING PPI enrichment results.\n\n        Example:\n            Run differential expression, then perform STRING PPI enrichment on significant genes:\n                ```python\n                class_type = [\"group\", \"condition\"]\n                values = [[\"GroupA\", \"Treatment1\"], [\"GroupA\", \"Control\"]]\n\n                pdata.de(classes=class_type, values=values)\n                pdata.list_enrichments()\n                sig_genes = pdata.stats[\"de_results\"][\"GroupA_Treatment1 vs GroupA_Control\"]\n                sig_genes = sig_genes[sig_genes[\"significance\"] != \"not significant\"][\"Genes\"].dropna().tolist()\n\n                pdata.enrichment_ppi(genes=sig_genes)\n                ```\n        \"\"\"\n        def query_ppi_enrichment(string_ids, species):\n            # print(f\"[INFO] Running PPI enrichment for {len(string_ids)} STRING IDs (species {species})...\")\n            url = \"https://string-db.org/api/json/ppi_enrichment\"\n            payload = {\n                \"identifiers\": \"%0d\".join(string_ids),\n                \"species\": species,\n                \"caller_identity\": \"scpviz\"\n            }\n\n            response = requests.post(url, data=payload)\n            response.raise_for_status()\n\n            result = response.json()\n            print(\"[DEBUG] PPI enrichment result:\", result)\n            return result[0] if isinstance(result, list) else result\n\n        print(f\"{format_log_prefix('user')} Running STRING PPI enrichment\")\n        t0 = time.time()\n        input_accs, unresolved_accs = self.resolve_to_accessions(genes)\n        mapping_df = self.get_string_mappings(input_accs, debug=debug)\n        mapping_df = mapping_df[mapping_df[\"string_identifier\"].notna()]\n        if mapping_df.empty:\n            raise ValueError(\"No valid STRING mappings found for the provided genes/accessions.\")\n\n        string_ids = mapping_df[\"string_identifier\"].tolist()\n        inferred_species = mapping_df[\"ncbi_taxon_id\"].mode().iloc[0]\n        species_id = species if species is not None else inferred_species\n\n        print(f\"   \ud83d\udd38 Input genes: {len(genes)} \u2192 Resolved STRING IDs: {len(mapping_df)}\")\n        print(f\"   \ud83d\udd38 Species: {species_id}\")\n        if unresolved_accs:\n            print(f\"{format_log_prefix('warn', 2)} Some accessions unresolved: {', '.join(unresolved_accs)}\")\n\n        result = query_ppi_enrichment(string_ids, species_id)\n\n        # Store results\n        if \"ppi\" not in self.stats:\n            self.stats[\"ppi\"] = {}\n\n        if store_key is None:\n            base = \"UserPPI\"\n            counter = 1\n            while f\"{base}{counter}\" in self.stats[\"ppi\"]:\n                counter += 1\n            store_key = f\"{base}{counter}\"\n\n        self.stats[\"ppi\"][store_key] = {\n            \"result\": result,\n            \"string_ids\": string_ids,\n            \"species\": species_id\n        }\n\n        print(f\"{format_log_prefix('result')} PPI enrichment complete ({time.time() - t0:.2f}s)\")\n        print(f\"   \u2022 STRING IDs   : {len(string_ids)}\")\n        print(f\"   \u2022 Edges found  : {result['number_of_edges']} vs {result['expected_number_of_edges']} expected\")\n        print(f\"   \u2022 p-value      : {result['p_value']:.2e}\")\n        print(f\"   \u2022 Access result: pdata.stats['ppi']['{store_key}']['result']\\n\")\n\n        return result\n\n    def list_enrichments(self):\n        \"\"\"\n        List available STRING enrichment results and unprocessed DE contrasts.\n\n        This method prints available functional and PPI enrichment entries stored in\n        `.stats[\"functional\"]` and `.stats[\"ppi\"]`, as well as DE comparisons in \n        `.stats[\"de_results\"]` that have not yet been analyzed.\n\n        Returns:\n            None\n\n        Example:\n            List enrichment results stored after running functional or PPI enrichment:\n                ```python\n                pdata.list_enrichments()\n                ```\n        \"\"\"\n\n        functional = self.stats.get(\"functional\", {})\n        ppi_keys = self.stats.get(\"ppi\", {}).keys()\n        de_keys = {k for k in self.stats if \"vs\" in k and not k.endswith((\"_up\", \"_down\"))}\n\n        # Collect enriched DE keys based on input_key metadata\n        enriched_de = set()\n        enriched_results = []\n\n        for k, meta in functional.items():\n            input_key = meta.get(\"input_key\", None)\n            is_de = \"vs\" in k\n\n            if input_key and input_key in de_keys:\n                base = input_key\n                suffix = k.rsplit(\"_\", 1)[-1]\n                pretty = f\"{_pretty_vs_key(base)}_{suffix}\"\n                enriched_de.add(base)\n                enriched_results.append((pretty, k, \"DE-based\"))\n            else:\n                enriched_results.append((k, k, \"User\"))\n\n        de_unenriched = sorted(_pretty_vs_key(k) for k in (de_keys - enriched_de))\n\n        print(f\"{format_log_prefix('user')} Listing STRING enrichment status\\n\")\n\n        print(f\"{format_log_prefix('info_only',2)} Available DE comparisons (not yet enriched):\")\n        if de_unenriched:\n            for pk in de_unenriched:\n                print(f\"        - {pk}\")\n        else:\n            print(\"  (none)\\n\")\n\n        print(\"\\n  \ud83d\udd39 To run enrichment:\")\n        print(\"      pdata.enrichment_functional(from_de=True, de_key=\\\"...\\\")\")\n\n        print(f\"\\n{format_log_prefix('result_only')} Completed STRING enrichment results:\")\n        if not enriched_results:\n            print(\"    (none)\")\n        for pretty, raw_key, kind in enriched_results:\n            if kind == \"DE-based\":\n                base, suffix = pretty.rsplit(\"_\", 1)\n                print(f\"  - {pretty} ({kind})\")\n                print(f\"    \u2022 Table: pdata.stats['functional'][\\\"{raw_key}\\\"]['result']\")\n                print(f\"    \u2022 Plot : pdata.plot_enrichment_svg(\\\"{base}\\\", direction=\\\"{suffix}\\\")\")\n                url = self.stats[\"functional\"].get(raw_key, {}).get(\"string_url\")\n                if url:\n                    print(f\"    \u2022 Link  : {url}\")\n            else:\n                print(f\"  - {pretty} ({kind})\")\n                print(f\"    \u2022 Table: pdata.stats['functional'][\\\"{raw_key}\\\"]['result']\")\n                print(f\"    \u2022 Plot : pdata.plot_enrichment_svg(\\\"{pretty}\\\")\")\n                url = self.stats[\"functional\"].get(raw_key, {}).get(\"string_url\")\n                if url:\n                    print(f\"    \u2022 Link  : {url}\")\n\n        if ppi_keys:\n            print(f\"\\n{format_log_prefix('result_only')} Completed STRING enrichment results:\")\n            for key in sorted(ppi_keys):\n                print(f\"  - {key} (User)\")\n                print(f\"    \u2022 Table: pdata.stats['ppi']['{key}']['result']\")\n        else:\n            print(f\"\\n{format_log_prefix('result_only')} Completed STRING PPI results:\")\n            print(\"    (none)\")\n\n    def plot_enrichment_svg(self, key, direction=None, category=None, save_as=None):\n        \"\"\"\n        Display STRING enrichment SVG inline in a Jupyter notebook.\n\n        This method fetches and renders a STRING-generated SVG for a previously completed\n        functional enrichment result. Optionally, the SVG can also be saved to disk.\n\n        Args:\n            key (str): Enrichment result key from `.stats[\"functional\"]`. For DE-based comparisons, this \n                includes both contrast and direction (e.g., `\"GroupA_Treatment1_vs_Control_up\"`).\n            direction (str, optional): Direction of DE result, either `\"up\"` or `\"down\"`. Use `None` for \n                user-defined gene lists.\n            category (str, optional): STRING enrichment category to filter by (e.g., `\"GO\"`, `\"KEGG\"`).\n            save_as (str, optional): If provided, saves the retrieved SVG to the given file path.\n\n        Returns:\n            None\n\n        Example:\n            Display a STRING enrichment network for a user-supplied gene list:\n                ```python\n                pdata.plot_enrichment_svg(\"UserSearch1\")\n                ```\n\n        Note:\n            The `key` must correspond to an existing entry in `.stats[\"functional\"]`, created via \n            `enrichment_functional()`.\n        \"\"\"\n        if \"functional\" not in self.stats:\n            raise ValueError(\"No STRING enrichment results found in .stats['functional'].\")\n\n        all_keys = list(self.stats[\"functional\"].keys())\n\n        # Handle DE-type key\n        if \"vs\" in key:\n            if direction not in {\"up\", \"down\"}:\n                raise ValueError(\"You must specify direction='up' or 'down' for DE-based enrichment keys.\")\n            lookup_key = _resolve_de_key(self.stats[\"functional\"], f\"{key}_{direction}\")\n        else:\n            # Handle user-supplied key (e.g. \"userSearch1\")\n            if direction is not None:\n                print(f\"[WARNING] Ignoring direction='{direction}' for user-supplied key: '{key}'\")\n            lookup_key = key\n\n        if lookup_key not in self.stats[\"functional\"]:\n            available = \"\\n\".join(f\"  - {k}\" for k in self.stats[\"functional\"].keys())\n            raise ValueError(f\"Could not find enrichment results for '{lookup_key}'. Available keys:\\n{available}\")\n\n        meta = self.stats[\"functional\"][lookup_key]\n        string_ids = meta[\"string_ids\"]\n        species_id = meta[\"species\"]\n\n        url = \"https://string-db.org/api/svg/enrichmentfigure\"\n        params = {\n            \"identifiers\": \"%0d\".join(string_ids),\n            \"species\": species_id\n        }\n        if category:\n            params[\"category\"] = category\n\n        print(f\"{format_log_prefix('user')} Fetching STRING SVG for key '{lookup_key}' (n={len(string_ids)})...\")\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n\n        if save_as:\n            with open(save_as, \"wb\") as f:\n                f.write(response.content)\n            print(f\"{format_log_prefix('info_only')} Saved SVG to: {save_as}\")\n\n        with tempfile.NamedTemporaryFile(\"wb\", suffix=\".svg\", delete=False) as tmp:\n            tmp.write(response.content)\n            tmp_path = tmp.name\n\n        try:\n            display(SVG(filename=tmp_path))\n        finally:\n            os.remove(tmp_path)\n\n    def get_string_network_link(self, key=None, string_ids=None, species=None, show_labels=True):\n        \"\"\"\n        Generate a direct STRING network URL to visualize protein interactions online.\n\n        This method constructs a STRING website link to view a network of proteins,\n        using either a list of STRING IDs or a key from previously stored enrichment results.\n\n        Args:\n            key (str, optional): Key from `.stats[\"functional\"]` to extract STRING IDs and species info.\n            string_ids (list of str, optional): List of STRING identifiers to include in the network.\n            species (int or str, optional): NCBI taxonomy ID (e.g., 9606 for human). Required if not using a stored key.\n            show_labels (bool): If True (default), node labels will be shown in the network view.\n\n        Returns:\n            str: URL to open the network in the STRING web interface.\n\n        Example:\n            Get a STRING network link for a stored enrichment result:\n                ```python\n                url = pdata.get_string_network_link(key=\"UserSearch1\")\n                print(url)\n                ```\n        \"\"\"\n        if string_ids is None:\n            if key is None:\n                raise ValueError(\"Must provide either a list of STRING IDs or a key.\")\n            metadata = self.stats.get(\"functional\", {}).get(key)\n            if metadata is None:\n                raise ValueError(f\"Key '{key}' not found in self.stats['functional'].\")\n            string_ids = metadata.get(\"string_ids\")\n            species = species or metadata.get(\"species\")\n\n        if not string_ids:\n            raise ValueError(\"No STRING IDs found or provided.\")\n\n        base_url = \"https://string-db.org/cgi/network\"\n        params = [\n            f\"identifiers={'%0d'.join(string_ids)}\",\n            f\"caller_identity=scpviz\"\n        ]\n        if species:\n            params.append(f\"species={species}\")\n        if show_labels:\n            params.append(\"show_query_node_labels=1\")\n\n        return f\"{base_url}?{'&amp;'.join(params)}\"\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.enrichment.EnrichmentMixin.enrichment_functional","title":"enrichment_functional","text":"<pre><code>enrichment_functional(genes=None, from_de=True, top_n=150, score_col='significance_score', gene_col='Genes', de_key='de_results', store_key=None, species=None, background=None, debug=False, **kwargs)\n</code></pre> <p>Run functional enrichment analysis using STRING on a gene list.</p> <p>This method performs ranked or unranked enrichment analysis using STRING's API. It supports both differential expression-based analysis (up- and down-regulated genes) and custom gene lists provided by the user. Enrichment results are stored in <code>.stats[\"functional\"]</code> for later access and plotting.</p> <p>Parameters:</p> Name Type Description Default <code>genes</code> <code>list of str</code> <p>List of gene symbols to analyze. Ignored if <code>from_de=True</code>.</p> <code>None</code> <code>from_de</code> <code>bool</code> <p>If True (default), selects genes from stored differential expression results.</p> <code>True</code> <code>top_n</code> <code>int</code> <p>Number of top-ranked genes to use when <code>from_de=True</code> (default is 150).</p> <code>150</code> <code>score_col</code> <code>str</code> <p>Column name in the DE table to rank genes by (default is <code>\"significance_score\"</code>).</p> <code>'significance_score'</code> <code>gene_col</code> <code>str</code> <p>Column name in <code>.prot.var</code> or DE results that contains gene names.</p> <code>'Genes'</code> <code>de_key</code> <code>str</code> <p>Key to retrieve stored DE results from <code>.stats[\"de_results\"]</code>.</p> <code>'de_results'</code> <code>store_key</code> <code>str</code> <p>Custom key to store enrichment results. Ignored when <code>from_de=True</code>.</p> <code>None</code> <code>species</code> <code>str</code> <p>Organism name or NCBI taxonomy ID. If None, inferred from STRING response.</p> <code>None</code> <code>background</code> <code>str or list of str</code> <p>Background gene list to use for enrichment.</p> <ul> <li>If <code>\"all_quantified\"</code>, uses non-significant proteins from DE or all other quantified proteins.</li> <li>If a list, must contain valid gene names or accessions.</li> </ul> <code>None</code> <code>debug</code> <code>bool</code> <p>If True, prints API request info and diagnostic messages.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the STRING enrichment API.</p> <code>{}</code> <p>Returns:</p> Type Description <p>dict or pd.DataFrame:</p> <ul> <li>If <code>from_de=True</code>, returns a dictionary of enrichment DataFrames for \"up\" and \"down\" gene sets.</li> <li>If <code>genes</code> is provided, returns a single enrichment DataFrame.</li> </ul> Example <p>Run differential expression, then perform STRING enrichment on top-ranked genes:     <pre><code>case1 = {'cellline': 'AS', 'treatment': 'sc'} # legacy style: class_type = [\"group\", \"condition\"]\ncase2 = {'cellline': 'BE', 'treatment': 'sc'} # legacy style: values = [[\"GroupA\", \"Treatment1\"], [\"GroupA\", \"Control\"]]\npdata_nb.de(values = case_values) # or legacy style: pdata.de(classes=class_type, values=values)\npdata.list_enrichments()  # list available DE result keys\npdata.enrichment_functional(from_de=True, de_key=\"GroupA_Treatment1 vs GroupA_Control\")\n</code></pre></p> <p>Perform enrichment on a custom list of genes:     <pre><code>genelist = [\"P55072\", \"NPLOC4\", \"UFD1\", \"STX5A\", \"NSFL1C\", \"UBXN2A\",\n            \"UBXN4\", \"UBE4B\", \"YOD1\", \"WASHC5\", \"PLAA\", \"UBXN10\"]\npdata.enrichment_functional(genes=genelist, from_de=False)\n</code></pre></p> Note <p>Internally uses <code>resolve_to_accessions()</code> and <code>get_string_mappings()</code>, and stores results  in <code>.stats[\"functional\"]</code>. Results can be accessed or visualized via <code>plot_enrichment_svg()</code> or by visiting the linked STRING URLs.</p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def enrichment_functional(\n    self,\n    genes=None,\n    from_de=True,\n    top_n=150,\n    score_col=\"significance_score\",\n    gene_col=\"Genes\",\n    de_key=\"de_results\",\n    store_key=None,\n    species=None,\n    background=None,\n    debug=False,\n    **kwargs\n):\n    \"\"\"\n    Run functional enrichment analysis using STRING on a gene list.\n\n    This method performs ranked or unranked enrichment analysis using STRING's API.\n    It supports both differential expression-based analysis (up- and down-regulated genes)\n    and custom gene lists provided by the user. Enrichment results are stored in\n    `.stats[\"functional\"]` for later access and plotting.\n\n    Args:\n        genes (list of str, optional): List of gene symbols to analyze. Ignored if `from_de=True`.\n        from_de (bool): If True (default), selects genes from stored differential expression results.\n        top_n (int): Number of top-ranked genes to use when `from_de=True` (default is 150).\n        score_col (str): Column name in the DE table to rank genes by (default is `\"significance_score\"`).\n        gene_col (str): Column name in `.prot.var` or DE results that contains gene names.\n        de_key (str): Key to retrieve stored DE results from `.stats[\"de_results\"]`.\n        store_key (str, optional): Custom key to store enrichment results. Ignored when `from_de=True`.\n        species (str, optional): Organism name or NCBI taxonomy ID. If None, inferred from STRING response.\n        background (str or list of str, optional): Background gene list to use for enrichment.\n\n            - If `\"all_quantified\"`, uses non-significant proteins from DE or all other quantified proteins.\n            - If a list, must contain valid gene names or accessions.\n        debug (bool): If True, prints API request info and diagnostic messages.\n        **kwargs: Additional keyword arguments passed to the STRING enrichment API.\n\n    Returns:\n        dict or pd.DataFrame:\n\n            - If `from_de=True`, returns a dictionary of enrichment DataFrames for \"up\" and \"down\" gene sets.\n            - If `genes` is provided, returns a single enrichment DataFrame.\n\n    Example:\n        Run differential expression, then perform STRING enrichment on top-ranked genes:\n            ```python\n            case1 = {'cellline': 'AS', 'treatment': 'sc'} # legacy style: class_type = [\"group\", \"condition\"]\n            case2 = {'cellline': 'BE', 'treatment': 'sc'} # legacy style: values = [[\"GroupA\", \"Treatment1\"], [\"GroupA\", \"Control\"]]\n            pdata_nb.de(values = case_values) # or legacy style: pdata.de(classes=class_type, values=values)\n            pdata.list_enrichments()  # list available DE result keys\n            pdata.enrichment_functional(from_de=True, de_key=\"GroupA_Treatment1 vs GroupA_Control\")\n            ```\n\n        Perform enrichment on a custom list of genes:\n            ```python\n            genelist = [\"P55072\", \"NPLOC4\", \"UFD1\", \"STX5A\", \"NSFL1C\", \"UBXN2A\",\n                        \"UBXN4\", \"UBE4B\", \"YOD1\", \"WASHC5\", \"PLAA\", \"UBXN10\"]\n            pdata.enrichment_functional(genes=genelist, from_de=False)\n            ```\n\n    Note:\n        Internally uses `resolve_to_accessions()` and `get_string_mappings()`, and stores results \n        in `.stats[\"functional\"]`. Results can be accessed or visualized via `plot_enrichment_svg()`\n        or by visiting the linked STRING URLs.\n    \"\"\"\n    def query_functional_enrichment(query_ids, species_id, background_ids=None, debug=False):\n        print(f\"{format_log_prefix('info_only',2)} Running enrichment on {len(query_ids)} STRING IDs (species {species_id})...\") if debug else None\n        url = \"https://string-db.org/api/json/enrichment\"\n        payload = {\n            \"identifiers\": \"%0d\".join(query_ids),\n            \"species\": species_id,\n            \"caller_identity\": \"scpviz\"\n        }\n        if background_ids is not None:\n            print(f\"{format_log_prefix('info_only')} Using background of {len(background_ids)} STRING IDs.\")\n            payload[\"background_string_identifiers\"] = \"%0d\".join(background_ids)\n\n        print(payload) if debug else None\n        response = requests.post(url, data=payload)\n        response.raise_for_status()\n        return pd.DataFrame(response.json())\n\n    # Ensure string metadata section exists\n    if \"functional\" not in self.stats:\n        self.stats[\"functional\"] = {}\n\n    if genes is None and from_de:\n        resolved_key = _resolve_de_key(self.stats, de_key)\n        de_df = self.stats[resolved_key]\n        sig_df = de_df[de_df[\"significance\"] != \"not significant\"].copy()\n        print(f\"{format_log_prefix('user')} Running STRING enrichment [DE-based: {resolved_key}]\")\n\n        up_genes = sig_df[sig_df[score_col] &gt; 0][gene_col].dropna().head(top_n).tolist()\n        down_genes = sig_df[sig_df[score_col] &lt; 0][gene_col].dropna().head(top_n).tolist()\n\n        up_accs, up_unresolved = self.resolve_to_accessions(up_genes)\n        down_accs, down_unresolved = self.resolve_to_accessions(down_genes)\n\n        background_accs = None\n        background_string_ids = None\n        if background == \"all_quantified\":\n            print(f\"{format_log_prefix('warn')} Mapping background proteins may take a long time due to batching.\")\n            background_accs = de_df[de_df[\"significance\"] == \"not significant\"].index.tolist()\n\n        if background_accs:\n            bg_map = self.get_string_mappings(background_accs,debug=debug)\n            bg_map = bg_map[bg_map[\"string_identifier\"].notna()]\n            background_string_ids = bg_map[\"string_identifier\"].tolist()\n\n        if store_key is not None:\n            print(f\"{format_log_prefix('warn')} Ignoring `store_key` for DE-based enrichment. Using auto-generated pretty keys.\")\n\n        results = {}\n        for label, accs in zip([\"up\", \"down\"], [up_accs, down_accs]):\n            print(f\"\\n\ud83d\udd39 {label.capitalize()}-regulated proteins\")\n            t0 = time.time()\n\n            if not accs:\n                print(f\"{format_log_prefix('warn')} No {label}-regulated proteins to analyze.\")\n                continue\n\n            mapping_df = self.get_string_mappings(accs, debug=debug)\n            mapping_df = mapping_df[mapping_df[\"string_identifier\"].notna()]\n            if mapping_df.empty:\n                print(f\"{format_log_prefix('warn')} No valid STRING mappings found for {label}-regulated proteins.\")\n                continue\n\n            string_ids = mapping_df[\"string_identifier\"].tolist()\n            inferred_species = mapping_df[\"ncbi_taxon_id\"].mode().iloc[0]\n            if species is not None:\n                # check if user species is same as inferred\n                if inferred_species != species:\n                    print(f\"{format_log_prefix('warn',2)} Inferred species ({inferred_species}) does not match user-specified ({species}). Using user-specified species.\")\n                species_id = species\n            else:\n                species_id = inferred_species\n\n            print(f\"   \ud83d\udd38 Proteins: {len(accs)} \u2192 STRING IDs: {len(string_ids)}\")\n            print(f\"   \ud83d\udd38 Species: {species_id} | Background: {'None' if background_string_ids is None else 'custom'}\")\n            if label == \"up\":\n                if up_unresolved:\n                    print(f\"{format_log_prefix('warn',2)} Some accessions unresolved for {label}-regulated proteins: {', '.join(up_unresolved)}\")\n            else:\n                if down_unresolved:\n                    print(f\"{format_log_prefix('warn',2)} Some accessions unresolved for {label}-regulated proteins: {', '.join(down_unresolved)}\")\n\n            enrichment_df = query_functional_enrichment(string_ids, species_id, background_string_ids, debug=debug)\n            enrich_key = f\"{resolved_key}_{label}\"\n            pretty_base = _pretty_vs_key(resolved_key)\n            pretty_key = f\"{pretty_base}_{label}\"\n            string_url = self.get_string_network_link(string_ids=string_ids, species=species_id)\n\n            self.stats[\"functional\"][pretty_key] = {\n                \"string_ids\": string_ids,\n                \"background_string_ids\": background_string_ids,\n                \"species\": species_id,\n                \"input_key\": resolved_key if from_de else None,\n                \"string_url\": string_url,\n                \"result\": enrichment_df\n            }\n\n            print(f\"{format_log_prefix('result')} Enrichment complete ({time.time() - t0:.2f}s)\")\n            print(f\"   \u2022 Access result: pdata.stats['functional'][\\\"{pretty_key}\\\"][\\\"result\\\"]\")\n            print(f\"   \u2022 Plot command : pdata.plot_enrichment_svg(\\\"{pretty_base}\\\", direction=\\\"{label}\\\")\")\n            print(f\"   \u2022 View online  : {string_url}\\n\")\n\n            results[label] = enrichment_df\n\n    elif genes is not None:\n        t0 = time.time()\n        print(f\"{format_log_prefix('user')} Running STRING enrichment [user-supplied]\")\n\n        if store_key is None:\n            prefix = \"UserSearch\"\n            existing = self.stats[\"functional\"].keys() if \"functional\" in self.stats else []\n            existing_ids = [k for k in existing if k.startswith(prefix)]\n            next_id = len(existing_ids) + 1\n            store_key = f\"{prefix}{next_id}\"\n\n        input_accs, unresolved_accs = self.resolve_to_accessions(genes)\n        mapping_df = self.get_string_mappings(input_accs, debug=debug)\n        mapping_df = mapping_df[mapping_df[\"string_identifier\"].notna()]\n        if mapping_df.empty:\n            raise ValueError(\"No valid STRING mappings found for the provided identifiers.\")\n\n        string_ids = mapping_df[\"string_identifier\"].tolist()\n        inferred_species = mapping_df[\"ncbi_taxon_id\"].mode().iloc[0]\n        if species is not None:\n            # check if user species is same as inferred\n            if inferred_species != species:\n                print(f\"{format_log_prefix('warn',2)} Inferred species ({inferred_species}) does not match user-specified ({species}). Using user-specified species.\")\n            species_id = species\n        else:\n            species_id = inferred_species\n\n        background_string_ids = None\n        if background == \"all_quantified\":\n            print(f\"{format_log_prefix('warn')} Mapping background proteins may take a long time due to batching.\")\n            all_accs = list(self.prot.var_names)\n            background_accs = list(set(all_accs) - set(input_accs))\n            bg_map = self.get_string_mappings(background_accs, debug=debug)\n            bg_map = bg_map[bg_map[\"string_identifier\"].notna()]\n            background_string_ids = bg_map[\"string_identifier\"].tolist()\n\n        print(f\"   \ud83d\udd38 Input genes: {len(genes)} \u2192 Resolved STRING IDs: {len(string_ids)}\")\n        print(f\"   \ud83d\udd38 Species: {species_id} | Background: {'None' if background_string_ids is None else 'custom'}\")\n        if unresolved_accs:\n            print(f\"{format_log_prefix('warn',2)} Some accessions unresolved: {', '.join(unresolved_accs)}\")\n\n        enrichment_df = query_functional_enrichment(string_ids, species_id, background_string_ids, debug=debug)\n        string_url = self.get_string_network_link(string_ids=string_ids, species=species_id)\n\n        self.stats[\"functional\"][store_key] = {\n            \"string_ids\": string_ids,\n            \"background_string_ids\": background_string_ids,\n            \"species\": species_id,\n            \"input_key\": None,\n            \"string_url\": string_url,\n            \"result\": enrichment_df\n        }\n\n        print(f\"{format_log_prefix('result')} Enrichment complete ({time.time() - t0:.2f}s)\")\n        print(f\"   \u2022 Access result: pdata.stats['functional'][\\\"{store_key}\\\"][\\\"result\\\"]\")\n        print(f\"   \u2022 Plot command : pdata.plot_enrichment_svg(\\\"{store_key}\\\")\")\n        print(f\"   \u2022 View online  : {string_url}\\n\")\n\n        return enrichment_df\n\n    else:\n        raise ValueError(\"Must provide 'genes' or set from_de=True to use DE results.\") \n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.enrichment.EnrichmentMixin.enrichment_ppi","title":"enrichment_ppi","text":"<pre><code>enrichment_ppi(genes, species=None, store_key=None, debug=False)\n</code></pre> <p>Run STRING PPI (protein\u2013protein interaction) enrichment on a user-supplied gene or accession list.</p> <p>This method maps the input gene names or UniProt accessions to STRING IDs, infers the species  if not provided, and submits the list to STRING's PPI enrichment endpoint. Results are stored  in <code>.stats[\"ppi\"]</code> for later retrieval or visualization.</p> <p>Parameters:</p> Name Type Description Default <code>genes</code> <code>list of str</code> <p>A list of gene names or UniProt accessions to analyze.</p> required <code>species</code> <code>int or str</code> <p>NCBI taxonomy ID (e.g., 9606 for human). If None, inferred from STRING mappings.</p> <code>None</code> <code>store_key</code> <code>str</code> <p>Key to store the enrichment result under <code>.stats[\"ppi\"]</code>. If None, a unique key is auto-generated.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame of STRING PPI enrichment results.</p> Example <p>Run differential expression, then perform STRING PPI enrichment on significant genes:     <pre><code>class_type = [\"group\", \"condition\"]\nvalues = [[\"GroupA\", \"Treatment1\"], [\"GroupA\", \"Control\"]]\n\npdata.de(classes=class_type, values=values)\npdata.list_enrichments()\nsig_genes = pdata.stats[\"de_results\"][\"GroupA_Treatment1 vs GroupA_Control\"]\nsig_genes = sig_genes[sig_genes[\"significance\"] != \"not significant\"][\"Genes\"].dropna().tolist()\n\npdata.enrichment_ppi(genes=sig_genes)\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def enrichment_ppi(self, genes, species=None, store_key=None, debug=False):\n    \"\"\"\n    Run STRING PPI (protein\u2013protein interaction) enrichment on a user-supplied gene or accession list.\n\n    This method maps the input gene names or UniProt accessions to STRING IDs, infers the species \n    if not provided, and submits the list to STRING's PPI enrichment endpoint. Results are stored \n    in `.stats[\"ppi\"]` for later retrieval or visualization.\n\n    Args:\n        genes (list of str): A list of gene names or UniProt accessions to analyze.\n        species (int or str, optional): NCBI taxonomy ID (e.g., 9606 for human). If None, inferred from STRING mappings.\n        store_key (str, optional): Key to store the enrichment result under `.stats[\"ppi\"]`.\n            If None, a unique key is auto-generated.\n\n    Returns:\n        pd.DataFrame: DataFrame of STRING PPI enrichment results.\n\n    Example:\n        Run differential expression, then perform STRING PPI enrichment on significant genes:\n            ```python\n            class_type = [\"group\", \"condition\"]\n            values = [[\"GroupA\", \"Treatment1\"], [\"GroupA\", \"Control\"]]\n\n            pdata.de(classes=class_type, values=values)\n            pdata.list_enrichments()\n            sig_genes = pdata.stats[\"de_results\"][\"GroupA_Treatment1 vs GroupA_Control\"]\n            sig_genes = sig_genes[sig_genes[\"significance\"] != \"not significant\"][\"Genes\"].dropna().tolist()\n\n            pdata.enrichment_ppi(genes=sig_genes)\n            ```\n    \"\"\"\n    def query_ppi_enrichment(string_ids, species):\n        # print(f\"[INFO] Running PPI enrichment for {len(string_ids)} STRING IDs (species {species})...\")\n        url = \"https://string-db.org/api/json/ppi_enrichment\"\n        payload = {\n            \"identifiers\": \"%0d\".join(string_ids),\n            \"species\": species,\n            \"caller_identity\": \"scpviz\"\n        }\n\n        response = requests.post(url, data=payload)\n        response.raise_for_status()\n\n        result = response.json()\n        print(\"[DEBUG] PPI enrichment result:\", result)\n        return result[0] if isinstance(result, list) else result\n\n    print(f\"{format_log_prefix('user')} Running STRING PPI enrichment\")\n    t0 = time.time()\n    input_accs, unresolved_accs = self.resolve_to_accessions(genes)\n    mapping_df = self.get_string_mappings(input_accs, debug=debug)\n    mapping_df = mapping_df[mapping_df[\"string_identifier\"].notna()]\n    if mapping_df.empty:\n        raise ValueError(\"No valid STRING mappings found for the provided genes/accessions.\")\n\n    string_ids = mapping_df[\"string_identifier\"].tolist()\n    inferred_species = mapping_df[\"ncbi_taxon_id\"].mode().iloc[0]\n    species_id = species if species is not None else inferred_species\n\n    print(f\"   \ud83d\udd38 Input genes: {len(genes)} \u2192 Resolved STRING IDs: {len(mapping_df)}\")\n    print(f\"   \ud83d\udd38 Species: {species_id}\")\n    if unresolved_accs:\n        print(f\"{format_log_prefix('warn', 2)} Some accessions unresolved: {', '.join(unresolved_accs)}\")\n\n    result = query_ppi_enrichment(string_ids, species_id)\n\n    # Store results\n    if \"ppi\" not in self.stats:\n        self.stats[\"ppi\"] = {}\n\n    if store_key is None:\n        base = \"UserPPI\"\n        counter = 1\n        while f\"{base}{counter}\" in self.stats[\"ppi\"]:\n            counter += 1\n        store_key = f\"{base}{counter}\"\n\n    self.stats[\"ppi\"][store_key] = {\n        \"result\": result,\n        \"string_ids\": string_ids,\n        \"species\": species_id\n    }\n\n    print(f\"{format_log_prefix('result')} PPI enrichment complete ({time.time() - t0:.2f}s)\")\n    print(f\"   \u2022 STRING IDs   : {len(string_ids)}\")\n    print(f\"   \u2022 Edges found  : {result['number_of_edges']} vs {result['expected_number_of_edges']} expected\")\n    print(f\"   \u2022 p-value      : {result['p_value']:.2e}\")\n    print(f\"   \u2022 Access result: pdata.stats['ppi']['{store_key}']['result']\\n\")\n\n    return result\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.enrichment.EnrichmentMixin.get_string_mappings","title":"get_string_mappings","text":"<pre><code>get_string_mappings(identifiers, overwrite=False, cache_col='STRING', batch_size=100, debug=False)\n</code></pre> <p>Resolve STRING IDs for UniProt accessions with a 2-step strategy: 1) Use UniProt stream (fields: xref_string) to fill cache quickly. 2) For any still-missing rows, query STRING get_string_ids, batched by organism_id.</p> <p>This method retrieves corresponding STRING identifiers for a list of UniProt accessions and stores the result in <code>self.prot.var[\"STRING_id\"]</code> for downstream use.</p> <p>Parameters:</p> Name Type Description Default <code>identifiers</code> <code>list of str</code> <p>List of UniProt accession IDs to map.</p> required <code>batch_size</code> <code>int</code> <p>Number of accessions to include in each API query (default is 300).</p> <code>100</code> <code>debug</code> <code>bool</code> <p>If True, prints progress and response info.</p> <code>False</code> <p>Returns:</p> Type Description <p>pd.DataFrame: Mapping table with columns: <code>input_identifier</code>, <code>string_identifier</code>, and <code>ncbi_taxon_id</code>.</p> Note <p>This is a helper method used primarily by <code>enrichment_functional()</code> and <code>enrichment_ppi()</code>.</p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def get_string_mappings(self, identifiers, overwrite=False, cache_col=\"STRING\", batch_size=100, debug=False):\n    \"\"\"\n    Resolve STRING IDs for UniProt accessions with a 2-step strategy:\n    1) Use UniProt stream (fields: xref_string) to fill cache quickly.\n    2) For any still-missing rows, query STRING get_string_ids, batched by organism_id.\n\n    This method retrieves corresponding STRING identifiers for a list of UniProt accessions\n    and stores the result in `self.prot.var[\"STRING_id\"]` for downstream use.\n\n    Args:\n        identifiers (list of str): List of UniProt accession IDs to map.\n        batch_size (int): Number of accessions to include in each API query (default is 300).\n        debug (bool): If True, prints progress and response info.\n\n    Returns:\n        pd.DataFrame: Mapping table with columns: `input_identifier`, `string_identifier`, and `ncbi_taxon_id`.\n\n    Note:\n        This is a helper method used primarily by `enrichment_functional()` and `enrichment_ppi()`.\n    \"\"\"\n    print(f\"[INFO] Resolving STRING IDs for {len(identifiers)} identifiers...\") if debug else None\n\n    prot_var = self.prot.var\n    if cache_col not in prot_var.columns:\n        prot_var[cache_col] = pd.NA\n\n    # Use cached STRING IDs if available\n    valid_ids = [i for i in identifiers if i in prot_var.index]\n    existing = prot_var.loc[valid_ids, cache_col]\n    found_ids = {i: sid for i, sid in existing.items() if pd.notna(sid)}\n    missing = [i for i in identifiers if i not in found_ids]\n\n    if overwrite:\n        # If overwriting, treat all valid IDs as missing for fresh pull\n        print(f\"{format_log_prefix('info_only',2)} Overwriting cached STRING IDs.\")\n        missing = valid_ids\n        found_ids = {}\n\n    print(f\"{format_log_prefix('info_only',2)} Found {len(found_ids)} cached STRING IDs. {len(missing)} need lookup.\")\n    print(missing) if debug else None\n\n    # -----------------------------\n    # Step 1: UniProt stream (fast)         # Use UniProt xref_string field to fill cache quickly\n    # -----------------------------\n\n    uni_results = [] \n    species_map = {}\n\n    if missing:\n        try:\n            dfu = get_uniprot_fields(missing, search_fields=['xref_string', 'organism_id'], batch_size=100, standardize=True, verbose=debug)\n            print(dfu) if debug else None\n\n            if dfu is not None and not dfu.empty:\n                entry_col = \"accession\" if \"accession\" in dfu.columns else None\n                xref_col  = \"xref_string\" if \"xref_string\" in dfu.columns else None\n                org_col   = \"organism_id\" if \"organism_id\" in dfu.columns else None\n\n                if entry_col and xref_col:\n                    # Parse first STRING ID if multiple are returned\n                    def _first_string(s):\n                        if pd.isna(s):\n                            return np.nan\n                        s = str(s).strip()\n                        if not s:\n                            return np.nan\n                        return s.split(';')[0].strip()\n\n                    dfu['__STRING__'] = dfu[xref_col].apply(_first_string)\n\n                    for _, row in dfu.iterrows():\n                        acc = row[entry_col]\n                        sid = row['__STRING__']\n\n                        # capture organism id if present\n                        if org_col in dfu.columns:\n                            org_val = row[org_col]\n                            if pd.notna(org_val) and str(org_val).strip():\n                                try:\n                                    species_map[acc] = int(org_val)\n                                except Exception:\n                                    # keep as raw if not int-castable\n                                    species_map[acc] = org_val\n\n                        if acc in prot_var.index and pd.notna(sid) and str(sid).strip():\n                            if overwrite or pd.isna(prot_var.at[acc, cache_col]) or not str(prot_var.at[acc, cache_col]).strip():\n                                prot_var.at[acc, cache_col] = sid\n                                prot_var.at[acc, \"ncbi_taxon_id\"] = str(species_map.get(acc, np.nan)) if pd.notna(species_map.get(acc, np.nan)) else np.nan\n                                found_ids[acc] = sid\n                                uni_results.append({\"input_identifier\": acc, \"string_identifier\": sid})\n\n            print(f\"{format_log_prefix('info_only',3)} Cached {len(uni_results)} STRING IDs from UniProt API xref_string.\")\n        except Exception as e:\n            print(f\"[WARN] UniProt stream step failed: {e}\")\n\n    # Recompute missing after UniProt step\n    missing = [i for i in identifiers if i not in found_ids]\n\n    # -----------------------------------------\n    # STEP 2: STRING API for still-missing ones\n    # -----------------------------------------\n\n    if not missing:\n        # nothing left to resolve via STRING\n        if debug:\n            print(f\"[INFO] All identifiers resolved via UniProt: {found_ids}\")\n        all_rows=[]\n\n    else:\n        all_rows = []\n\n        for i in range(0, len(missing), batch_size):\n            batch = missing[i:i + batch_size]\n            print(f\"{format_log_prefix('info')} Querying STRING for batch {i // batch_size + 1} ({len(batch)} identifiers)...\") if debug else None\n\n            url = \"https://string-db.org/api/tsv-no-header/get_string_ids\"\n            params = {\n                \"identifiers\": \"\\r\".join(batch),\n                \"limit\": 1,\n                \"echo_query\": 1,\n                \"caller_identity\": \"scpviz\"\n            }\n\n            try:\n                t0 = time.time()\n                response = requests.post(url, data=params)\n                response.raise_for_status()\n                df = pd.read_csv(StringIO(response.text), sep=\"\\t\", header=None)\n                df.columns = [\n                    \"input_identifier\", \"input_alias\", \"string_identifier\", \"ncbi_taxon_id\",\n                    \"preferred_name\", \"annotation\", \"score\"\n                ]\n                print(f\"[INFO] Batch completed in {time.time() - t0:.2f}s\") if debug else None\n                all_rows.append(df)\n            except Exception as e:\n                print(f\"[ERROR] Failed on batch {i // batch_size + 1}: {e}\") if debug else None\n\n    # Combine all new mappings\n    if all_rows:\n        new_df = pd.concat(all_rows, ignore_index=True)\n        updated_ids = []\n\n        for _, row in new_df.iterrows():\n            acc = row[\"input_identifier\"]\n            sid = row[\"string_identifier\"]\n            if acc in self.prot.var.index:\n                self.prot.var.at[acc, cache_col] = sid\n                found_ids[acc] = sid\n                updated_ids.append(acc)\n            else:\n                print(f\"[DEBUG] Skipping unknown accession '{acc}'\")\n\n        print(f\"{format_log_prefix('info_only',3)} Cached {len(updated_ids)} new STRING ID mappings from STRING API.\")\n    elif missing:\n        print(f\"{format_log_prefix('warn_only',3)} No STRING mappings returned from STRING API.\")\n\n\n    # ------------------------------------\n    # Build and MERGE UniProt results into out_df\n    # ------------------------------------\n    out_df = pd.DataFrame.from_dict(found_ids, orient=\"index\", columns=[\"string_identifier\"])\n    out_df.index.name = \"input_identifier\"\n    out_df = out_df.reset_index()\n\n    if uni_results:\n        uni_df = pd.DataFrame(uni_results).dropna().drop_duplicates(subset=[\"input_identifier\"])\n        out_df = out_df.merge(uni_df, on=\"input_identifier\", how=\"left\", suffixes=(\"\", \"_uni\"))\n        out_df[\"string_identifier\"] = out_df[\"string_identifier\"].combine_first(out_df[\"string_identifier_uni\"])\n        out_df = out_df.drop(columns=[\"string_identifier_uni\"])\n\n    # Use species_map (from UniProt and/or STRING) for ncbi_taxon_id\n    from_map = out_df[\"input_identifier\"].map(lambda acc: species_map.get(acc, np.nan))\n    from_cache = out_df[\"input_identifier\"].map(lambda acc: prot_var.at[acc, \"ncbi_taxon_id\"] if acc in prot_var.index else np.nan)\n    out_df[\"ncbi_taxon_id\"] = from_map.combine_first(from_cache)\n\n    return out_df\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.enrichment.EnrichmentMixin.get_string_network_link","title":"get_string_network_link","text":"<pre><code>get_string_network_link(key=None, string_ids=None, species=None, show_labels=True)\n</code></pre> <p>Generate a direct STRING network URL to visualize protein interactions online.</p> <p>This method constructs a STRING website link to view a network of proteins, using either a list of STRING IDs or a key from previously stored enrichment results.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key from <code>.stats[\"functional\"]</code> to extract STRING IDs and species info.</p> <code>None</code> <code>string_ids</code> <code>list of str</code> <p>List of STRING identifiers to include in the network.</p> <code>None</code> <code>species</code> <code>int or str</code> <p>NCBI taxonomy ID (e.g., 9606 for human). Required if not using a stored key.</p> <code>None</code> <code>show_labels</code> <code>bool</code> <p>If True (default), node labels will be shown in the network view.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <p>URL to open the network in the STRING web interface.</p> Example <p>Get a STRING network link for a stored enrichment result:     <pre><code>url = pdata.get_string_network_link(key=\"UserSearch1\")\nprint(url)\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def get_string_network_link(self, key=None, string_ids=None, species=None, show_labels=True):\n    \"\"\"\n    Generate a direct STRING network URL to visualize protein interactions online.\n\n    This method constructs a STRING website link to view a network of proteins,\n    using either a list of STRING IDs or a key from previously stored enrichment results.\n\n    Args:\n        key (str, optional): Key from `.stats[\"functional\"]` to extract STRING IDs and species info.\n        string_ids (list of str, optional): List of STRING identifiers to include in the network.\n        species (int or str, optional): NCBI taxonomy ID (e.g., 9606 for human). Required if not using a stored key.\n        show_labels (bool): If True (default), node labels will be shown in the network view.\n\n    Returns:\n        str: URL to open the network in the STRING web interface.\n\n    Example:\n        Get a STRING network link for a stored enrichment result:\n            ```python\n            url = pdata.get_string_network_link(key=\"UserSearch1\")\n            print(url)\n            ```\n    \"\"\"\n    if string_ids is None:\n        if key is None:\n            raise ValueError(\"Must provide either a list of STRING IDs or a key.\")\n        metadata = self.stats.get(\"functional\", {}).get(key)\n        if metadata is None:\n            raise ValueError(f\"Key '{key}' not found in self.stats['functional'].\")\n        string_ids = metadata.get(\"string_ids\")\n        species = species or metadata.get(\"species\")\n\n    if not string_ids:\n        raise ValueError(\"No STRING IDs found or provided.\")\n\n    base_url = \"https://string-db.org/cgi/network\"\n    params = [\n        f\"identifiers={'%0d'.join(string_ids)}\",\n        f\"caller_identity=scpviz\"\n    ]\n    if species:\n        params.append(f\"species={species}\")\n    if show_labels:\n        params.append(\"show_query_node_labels=1\")\n\n    return f\"{base_url}?{'&amp;'.join(params)}\"\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.enrichment.EnrichmentMixin.list_enrichments","title":"list_enrichments","text":"<pre><code>list_enrichments()\n</code></pre> <p>List available STRING enrichment results and unprocessed DE contrasts.</p> <p>This method prints available functional and PPI enrichment entries stored in <code>.stats[\"functional\"]</code> and <code>.stats[\"ppi\"]</code>, as well as DE comparisons in  <code>.stats[\"de_results\"]</code> that have not yet been analyzed.</p> <p>Returns:</p> Type Description <p>None</p> Example <p>List enrichment results stored after running functional or PPI enrichment:     <pre><code>pdata.list_enrichments()\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def list_enrichments(self):\n    \"\"\"\n    List available STRING enrichment results and unprocessed DE contrasts.\n\n    This method prints available functional and PPI enrichment entries stored in\n    `.stats[\"functional\"]` and `.stats[\"ppi\"]`, as well as DE comparisons in \n    `.stats[\"de_results\"]` that have not yet been analyzed.\n\n    Returns:\n        None\n\n    Example:\n        List enrichment results stored after running functional or PPI enrichment:\n            ```python\n            pdata.list_enrichments()\n            ```\n    \"\"\"\n\n    functional = self.stats.get(\"functional\", {})\n    ppi_keys = self.stats.get(\"ppi\", {}).keys()\n    de_keys = {k for k in self.stats if \"vs\" in k and not k.endswith((\"_up\", \"_down\"))}\n\n    # Collect enriched DE keys based on input_key metadata\n    enriched_de = set()\n    enriched_results = []\n\n    for k, meta in functional.items():\n        input_key = meta.get(\"input_key\", None)\n        is_de = \"vs\" in k\n\n        if input_key and input_key in de_keys:\n            base = input_key\n            suffix = k.rsplit(\"_\", 1)[-1]\n            pretty = f\"{_pretty_vs_key(base)}_{suffix}\"\n            enriched_de.add(base)\n            enriched_results.append((pretty, k, \"DE-based\"))\n        else:\n            enriched_results.append((k, k, \"User\"))\n\n    de_unenriched = sorted(_pretty_vs_key(k) for k in (de_keys - enriched_de))\n\n    print(f\"{format_log_prefix('user')} Listing STRING enrichment status\\n\")\n\n    print(f\"{format_log_prefix('info_only',2)} Available DE comparisons (not yet enriched):\")\n    if de_unenriched:\n        for pk in de_unenriched:\n            print(f\"        - {pk}\")\n    else:\n        print(\"  (none)\\n\")\n\n    print(\"\\n  \ud83d\udd39 To run enrichment:\")\n    print(\"      pdata.enrichment_functional(from_de=True, de_key=\\\"...\\\")\")\n\n    print(f\"\\n{format_log_prefix('result_only')} Completed STRING enrichment results:\")\n    if not enriched_results:\n        print(\"    (none)\")\n    for pretty, raw_key, kind in enriched_results:\n        if kind == \"DE-based\":\n            base, suffix = pretty.rsplit(\"_\", 1)\n            print(f\"  - {pretty} ({kind})\")\n            print(f\"    \u2022 Table: pdata.stats['functional'][\\\"{raw_key}\\\"]['result']\")\n            print(f\"    \u2022 Plot : pdata.plot_enrichment_svg(\\\"{base}\\\", direction=\\\"{suffix}\\\")\")\n            url = self.stats[\"functional\"].get(raw_key, {}).get(\"string_url\")\n            if url:\n                print(f\"    \u2022 Link  : {url}\")\n        else:\n            print(f\"  - {pretty} ({kind})\")\n            print(f\"    \u2022 Table: pdata.stats['functional'][\\\"{raw_key}\\\"]['result']\")\n            print(f\"    \u2022 Plot : pdata.plot_enrichment_svg(\\\"{pretty}\\\")\")\n            url = self.stats[\"functional\"].get(raw_key, {}).get(\"string_url\")\n            if url:\n                print(f\"    \u2022 Link  : {url}\")\n\n    if ppi_keys:\n        print(f\"\\n{format_log_prefix('result_only')} Completed STRING enrichment results:\")\n        for key in sorted(ppi_keys):\n            print(f\"  - {key} (User)\")\n            print(f\"    \u2022 Table: pdata.stats['ppi']['{key}']['result']\")\n    else:\n        print(f\"\\n{format_log_prefix('result_only')} Completed STRING PPI results:\")\n        print(\"    (none)\")\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.enrichment.EnrichmentMixin.plot_enrichment_svg","title":"plot_enrichment_svg","text":"<pre><code>plot_enrichment_svg(key, direction=None, category=None, save_as=None)\n</code></pre> <p>Display STRING enrichment SVG inline in a Jupyter notebook.</p> <p>This method fetches and renders a STRING-generated SVG for a previously completed functional enrichment result. Optionally, the SVG can also be saved to disk.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Enrichment result key from <code>.stats[\"functional\"]</code>. For DE-based comparisons, this  includes both contrast and direction (e.g., <code>\"GroupA_Treatment1_vs_Control_up\"</code>).</p> required <code>direction</code> <code>str</code> <p>Direction of DE result, either <code>\"up\"</code> or <code>\"down\"</code>. Use <code>None</code> for  user-defined gene lists.</p> <code>None</code> <code>category</code> <code>str</code> <p>STRING enrichment category to filter by (e.g., <code>\"GO\"</code>, <code>\"KEGG\"</code>).</p> <code>None</code> <code>save_as</code> <code>str</code> <p>If provided, saves the retrieved SVG to the given file path.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Display a STRING enrichment network for a user-supplied gene list:     <pre><code>pdata.plot_enrichment_svg(\"UserSearch1\")\n</code></pre></p> Note <p>The <code>key</code> must correspond to an existing entry in <code>.stats[\"functional\"]</code>, created via  <code>enrichment_functional()</code>.</p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def plot_enrichment_svg(self, key, direction=None, category=None, save_as=None):\n    \"\"\"\n    Display STRING enrichment SVG inline in a Jupyter notebook.\n\n    This method fetches and renders a STRING-generated SVG for a previously completed\n    functional enrichment result. Optionally, the SVG can also be saved to disk.\n\n    Args:\n        key (str): Enrichment result key from `.stats[\"functional\"]`. For DE-based comparisons, this \n            includes both contrast and direction (e.g., `\"GroupA_Treatment1_vs_Control_up\"`).\n        direction (str, optional): Direction of DE result, either `\"up\"` or `\"down\"`. Use `None` for \n            user-defined gene lists.\n        category (str, optional): STRING enrichment category to filter by (e.g., `\"GO\"`, `\"KEGG\"`).\n        save_as (str, optional): If provided, saves the retrieved SVG to the given file path.\n\n    Returns:\n        None\n\n    Example:\n        Display a STRING enrichment network for a user-supplied gene list:\n            ```python\n            pdata.plot_enrichment_svg(\"UserSearch1\")\n            ```\n\n    Note:\n        The `key` must correspond to an existing entry in `.stats[\"functional\"]`, created via \n        `enrichment_functional()`.\n    \"\"\"\n    if \"functional\" not in self.stats:\n        raise ValueError(\"No STRING enrichment results found in .stats['functional'].\")\n\n    all_keys = list(self.stats[\"functional\"].keys())\n\n    # Handle DE-type key\n    if \"vs\" in key:\n        if direction not in {\"up\", \"down\"}:\n            raise ValueError(\"You must specify direction='up' or 'down' for DE-based enrichment keys.\")\n        lookup_key = _resolve_de_key(self.stats[\"functional\"], f\"{key}_{direction}\")\n    else:\n        # Handle user-supplied key (e.g. \"userSearch1\")\n        if direction is not None:\n            print(f\"[WARNING] Ignoring direction='{direction}' for user-supplied key: '{key}'\")\n        lookup_key = key\n\n    if lookup_key not in self.stats[\"functional\"]:\n        available = \"\\n\".join(f\"  - {k}\" for k in self.stats[\"functional\"].keys())\n        raise ValueError(f\"Could not find enrichment results for '{lookup_key}'. Available keys:\\n{available}\")\n\n    meta = self.stats[\"functional\"][lookup_key]\n    string_ids = meta[\"string_ids\"]\n    species_id = meta[\"species\"]\n\n    url = \"https://string-db.org/api/svg/enrichmentfigure\"\n    params = {\n        \"identifiers\": \"%0d\".join(string_ids),\n        \"species\": species_id\n    }\n    if category:\n        params[\"category\"] = category\n\n    print(f\"{format_log_prefix('user')} Fetching STRING SVG for key '{lookup_key}' (n={len(string_ids)})...\")\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n\n    if save_as:\n        with open(save_as, \"wb\") as f:\n            f.write(response.content)\n        print(f\"{format_log_prefix('info_only')} Saved SVG to: {save_as}\")\n\n    with tempfile.NamedTemporaryFile(\"wb\", suffix=\".svg\", delete=False) as tmp:\n        tmp.write(response.content)\n        tmp_path = tmp.name\n\n    try:\n        display(SVG(filename=tmp_path))\n    finally:\n        os.remove(tmp_path)\n</code></pre>"},{"location":"reference/pAnnData/analysis_mixins/#src.scpviz.pAnnData.enrichment.EnrichmentMixin.resolve_to_accessions","title":"resolve_to_accessions","text":"<pre><code>resolve_to_accessions(mixed_list)\n</code></pre> <p>Convert gene names or accessions into standardized UniProt accession IDs.</p> <p>This method resolves input items using the internal gene-to-accession map, ensuring all returned entries are accessions present in the <code>.prot</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>mixed_list</code> <code>list of str</code> <p>A list containing gene names and/or UniProt accessions.</p> required <p>Returns:</p> Type Description <p>list of str: List of resolved UniProt accession IDs.</p> Note <p>This function is similar to <code>utils.resolve_accessions()</code> but operates in the context  of the current <code>pAnnData</code> object and its internal gene mappings.</p> Todo <p>Add example comparing results from <code>resolve_to_accessions()</code> and <code>utils.resolve_accessions()</code>.</p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def resolve_to_accessions(self, mixed_list):\n    \"\"\"\n    Convert gene names or accessions into standardized UniProt accession IDs.\n\n    This method resolves input items using the internal gene-to-accession map,\n    ensuring all returned entries are accessions present in the `.prot` object.\n\n    Args:\n        mixed_list (list of str): A list containing gene names and/or UniProt accessions.\n\n    Returns:\n        list of str: List of resolved UniProt accession IDs.\n\n    Note:\n        This function is similar to `utils.resolve_accessions()` but operates in the context \n        of the current `pAnnData` object and its internal gene mappings.\n\n    Todo:\n        Add example comparing results from `resolve_to_accessions()` and `utils.resolve_accessions()`.\n    \"\"\"\n    gene_to_acc, _ = self.get_gene_maps(on='protein') \n    accs = []\n    unresolved_accs = []\n    for item in mixed_list:\n        if item in self.prot.var.index:\n            accs.append(item)  # already an accession\n        elif item in gene_to_acc:\n            accs.append(gene_to_acc[item])\n        else:\n            unresolved_accs.append(item)\n            # print(f\"{format_log_prefix('warn_only',2)} Could not resolve '{item}' to an accession \u2014 skipping.\")\n    return accs, unresolved_accs\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/","title":"Core Utilities","text":"<p>Includes foundational methods for validation, copying, and history tracking.</p>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.base.BaseMixin","title":"BaseMixin","text":"<p>Core base methods for pAnnData.</p> <p>This mixin provides essential utility and management functions for cloning,  checking, and managing core attributes of a <code>pAnnData</code> object. These methods serve as foundational building blocks for other mixins and functions.</p> <p>Features:</p> <ul> <li>Checks presence of data (.prot or .pep)</li> <li>Safe object copying with state preservation</li> <li>Internal metadata management (stats, history, summary)</li> </ul> <p>Methods:</p> Name Description <code>_has_data</code> <p>Check whether .prot and/or .pep data are present  </p> <code>copy</code> <p>Return a new <code>pAnnData</code> object with retained internal state</p> Source code in <code>src/scpviz/pAnnData/base.py</code> <pre><code>class BaseMixin:\n    \"\"\"\n    Core base methods for pAnnData.\n\n    This mixin provides essential utility and management functions for cloning, \n    checking, and managing core attributes of a `pAnnData` object. These methods\n    serve as foundational building blocks for other mixins and functions.\n\n    Features:\n\n    - Checks presence of data (.prot or .pep)\n    - Safe object copying with state preservation\n    - Internal metadata management (stats, history, summary)\n\n    Functions:\n        _has_data: Check whether .prot and/or .pep data are present  \n        copy: Return a new `pAnnData` object with retained internal state\n    \"\"\"\n    def _has_data(self) -&gt; bool:\n        \"\"\"\n        Check whether the pAnnData object contains either protein or peptide data.\n\n        Returns:\n            bool: True if either .prot or .pep is not None; otherwise False.\n        \"\"\"\n        return self.prot is not None or self.pep is not None # type: ignore[attr-defined]\n\n    def copy(self):\n        \"\"\"\n        Return a new `pAnnData` object with the current state of all components.\n\n        This method performs a shallow copy of core data (.prot, .pep) and a deep copy of internal attributes\n        (e.g., RS matrix, summary, stats, and cached maps). It avoids full deepcopy for efficiency and retains\n        the current filtered or processed state of the object.\n\n        Returns:\n            pAnnData: A new object containing copies of the current data and metadata.\n        \"\"\"\n        new_obj = self.__class__.__new__(self.__class__)\n\n        # Copy core AnnData components\n        new_obj.prot = self.prot.copy() if self.prot is not None else None # type: ignore[attr-defined]\n        new_obj.pep = self.pep.copy() if self.pep is not None else None # type: ignore[attr-defined]\n        new_obj._rs = copy.deepcopy(self._rs) # type: ignore[attr-defined]\n\n        # Copy summary and stats\n        new_obj._stats = copy.deepcopy(self._stats) # type: ignore[attr-defined]\n        new_obj._history = copy.deepcopy(self._history) # type: ignore[attr-defined]\n        new_obj._previous_summary = copy.deepcopy(self._previous_summary) # type: ignore[attr-defined]\n        new_obj._suppress_summary_log = True # type: ignore[attr-defined]\n        new_obj.summary = self._summary.copy(deep=True) if self._summary is not None else None # go through setter to mark as stale, # type: ignore[attr-defined]\n        del new_obj._suppress_summary_log # type: ignore[attr-defined]\n\n        # Optional: cached maps\n        if hasattr(self, \"_gene_maps_protein\"):\n            new_obj._gene_maps_protein = copy.deepcopy(self._gene_maps_protein) # type: ignore[attr-defined]\n        if hasattr(self, \"_protein_maps_peptide\"):\n            new_obj._protein_maps_peptide = copy.deepcopy(self._protein_maps_peptide) # type: ignore[attr-defined]\n\n        return new_obj\n\n    def compare_current_to_raw(self, on=\"protein\"):\n        \"\"\"\n        Compare current pdata object to original raw data, showing how many samples and features were dropped.\n        Compares current obs/var names to the original raw data (stored in .uns).\n\n        Args:\n            on (str): Dataset to compare ('protein' or 'peptide').\n\n        Returns:\n            dict: Dictionary summarizing dropped samples and features.\n        \"\"\"\n        print(f\"{format_log_prefix('user', 1)} Comparing current pdata to X_raw [{on}]:\")\n\n        adata = getattr(self, \"prot\" if on == \"protein\" else \"pep\", None)\n        if adata is None:\n            print(f\"{format_log_prefix('warn', 2)} No {on} data found.\")\n            return None\n\n        orig_obs = set(adata.uns.get(\"X_raw_obs_names\", []))\n        orig_var = set(adata.uns.get(\"X_raw_var_names\", []))\n        current_obs = set(adata.obs_names)\n        current_var = set(adata.var_names)\n\n        dropped_obs = sorted(list(orig_obs - current_obs))\n        dropped_var = sorted(list(orig_var - current_var))\n\n        print(f\"   \u2192 Samples dropped: {len(dropped_obs)}\")\n        print(f\"   \u2192 Features dropped: {len(dropped_var)}\")\n\n        return {\"dropped_samples\": dropped_obs, \"dropped_features\": dropped_var}\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.base.BaseMixin.compare_current_to_raw","title":"compare_current_to_raw","text":"<pre><code>compare_current_to_raw(on='protein')\n</code></pre> <p>Compare current pdata object to original raw data, showing how many samples and features were dropped. Compares current obs/var names to the original raw data (stored in .uns).</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Dataset to compare ('protein' or 'peptide').</p> <code>'protein'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary summarizing dropped samples and features.</p> Source code in <code>src/scpviz/pAnnData/base.py</code> <pre><code>def compare_current_to_raw(self, on=\"protein\"):\n    \"\"\"\n    Compare current pdata object to original raw data, showing how many samples and features were dropped.\n    Compares current obs/var names to the original raw data (stored in .uns).\n\n    Args:\n        on (str): Dataset to compare ('protein' or 'peptide').\n\n    Returns:\n        dict: Dictionary summarizing dropped samples and features.\n    \"\"\"\n    print(f\"{format_log_prefix('user', 1)} Comparing current pdata to X_raw [{on}]:\")\n\n    adata = getattr(self, \"prot\" if on == \"protein\" else \"pep\", None)\n    if adata is None:\n        print(f\"{format_log_prefix('warn', 2)} No {on} data found.\")\n        return None\n\n    orig_obs = set(adata.uns.get(\"X_raw_obs_names\", []))\n    orig_var = set(adata.uns.get(\"X_raw_var_names\", []))\n    current_obs = set(adata.obs_names)\n    current_var = set(adata.var_names)\n\n    dropped_obs = sorted(list(orig_obs - current_obs))\n    dropped_var = sorted(list(orig_var - current_var))\n\n    print(f\"   \u2192 Samples dropped: {len(dropped_obs)}\")\n    print(f\"   \u2192 Features dropped: {len(dropped_var)}\")\n\n    return {\"dropped_samples\": dropped_obs, \"dropped_features\": dropped_var}\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.base.BaseMixin.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Return a new <code>pAnnData</code> object with the current state of all components.</p> <p>This method performs a shallow copy of core data (.prot, .pep) and a deep copy of internal attributes (e.g., RS matrix, summary, stats, and cached maps). It avoids full deepcopy for efficiency and retains the current filtered or processed state of the object.</p> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>A new object containing copies of the current data and metadata.</p> Source code in <code>src/scpviz/pAnnData/base.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Return a new `pAnnData` object with the current state of all components.\n\n    This method performs a shallow copy of core data (.prot, .pep) and a deep copy of internal attributes\n    (e.g., RS matrix, summary, stats, and cached maps). It avoids full deepcopy for efficiency and retains\n    the current filtered or processed state of the object.\n\n    Returns:\n        pAnnData: A new object containing copies of the current data and metadata.\n    \"\"\"\n    new_obj = self.__class__.__new__(self.__class__)\n\n    # Copy core AnnData components\n    new_obj.prot = self.prot.copy() if self.prot is not None else None # type: ignore[attr-defined]\n    new_obj.pep = self.pep.copy() if self.pep is not None else None # type: ignore[attr-defined]\n    new_obj._rs = copy.deepcopy(self._rs) # type: ignore[attr-defined]\n\n    # Copy summary and stats\n    new_obj._stats = copy.deepcopy(self._stats) # type: ignore[attr-defined]\n    new_obj._history = copy.deepcopy(self._history) # type: ignore[attr-defined]\n    new_obj._previous_summary = copy.deepcopy(self._previous_summary) # type: ignore[attr-defined]\n    new_obj._suppress_summary_log = True # type: ignore[attr-defined]\n    new_obj.summary = self._summary.copy(deep=True) if self._summary is not None else None # go through setter to mark as stale, # type: ignore[attr-defined]\n    del new_obj._suppress_summary_log # type: ignore[attr-defined]\n\n    # Optional: cached maps\n    if hasattr(self, \"_gene_maps_protein\"):\n        new_obj._gene_maps_protein = copy.deepcopy(self._gene_maps_protein) # type: ignore[attr-defined]\n    if hasattr(self, \"_protein_maps_peptide\"):\n        new_obj._protein_maps_peptide = copy.deepcopy(self._protein_maps_peptide) # type: ignore[attr-defined]\n\n    return new_obj\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.summary.SummaryMixin","title":"SummaryMixin","text":"<p>Handles creation, synchronization, and metric updates for the <code>.summary</code> attribute.</p> <p>This mixin maintains a unified sample-level summary table by merging <code>.prot.obs</code> and <code>.pep.obs</code>, with automatic flagging and update mechanisms to track when recomputation or syncing is needed.</p> <p>Features:</p> <ul> <li>Merges sample-level metadata from <code>.prot.obs</code> and <code>.pep.obs</code> into <code>.summary</code></li> <li>Tracks when <code>.summary</code> becomes out of sync via <code>_summary_is_stale</code></li> <li>Supports recomputing per-sample metrics and syncing edits back to <code>.obs</code></li> <li>Enables passive refresh of <code>.summary</code> after filtering or manual editing</li> </ul> <p>Methods:</p> Name Description <code>update_summary</code> <p>Rebuild and optionally recompute or sync <code>.summary</code></p> <code>_update_summary</code> <p>Legacy alias for <code>update_summary()</code></p> <code>_merge_obs</code> <p>Internal merge logic for <code>.prot.obs</code> and <code>.pep.obs</code></p> <code>_push_summary_to_obs</code> <p>Sync edited <code>.summary</code> values back into <code>.obs</code></p> <code>_mark_summary_stale</code> <p>Mark the summary as stale for downstream tracking</p> Source code in <code>src/scpviz/pAnnData/summary.py</code> <pre><code>class SummaryMixin:\n    \"\"\"\n    Handles creation, synchronization, and metric updates for the `.summary` attribute.\n\n    This mixin maintains a unified sample-level summary table by merging `.prot.obs` and `.pep.obs`,\n    with automatic flagging and update mechanisms to track when recomputation or syncing is needed.\n\n    Features:\n\n    - Merges sample-level metadata from `.prot.obs` and `.pep.obs` into `.summary`\n    - Tracks when `.summary` becomes out of sync via `_summary_is_stale`\n    - Supports recomputing per-sample metrics and syncing edits back to `.obs`\n    - Enables passive refresh of `.summary` after filtering or manual editing\n\n    Functions:\n        update_summary: Rebuild and optionally recompute or sync `.summary`\n        _update_summary: Legacy alias for `update_summary()`\n        _merge_obs: Internal merge logic for `.prot.obs` and `.pep.obs`\n        _push_summary_to_obs: Sync edited `.summary` values back into `.obs`\n        _mark_summary_stale: Mark the summary as stale for downstream tracking\n    \"\"\"\n    def update_summary(self, recompute=True, sync_back=False, verbose=True):\n        \"\"\"\n        Update the `.summary` DataFrame to reflect current state of `.obs` and metadata.\n\n        This function ensures `.summary` stays synchronized with sample-level metadata\n        stored in `.prot.obs` / `.pep.obs`. You can choose to recompute metrics,\n        sync edits back to `.obs`, or simply refresh the merged view.\n\n        Args:\n            recompute (bool): If True, re-calculate protein/peptide stats.\n            sync_back (bool): If True, push edited `.summary` values back to `.prot.obs` / `.pep.obs`.\n                False by default, as `.summary` is derived.\n            verbose (bool): If True, print action messages.\n\n        ??? example \"Typical Usage Scenarios\"\n            | Scenario                        | Call                                | recompute | sync_back | _summary_is_stale | Effect                                                           |\n            |---------------------------------|-------------------------------------|-----------|-----------|-------------------|------------------------------------------------------------------|\n            | Filtering `.prot` or `.pep`     | `.update_summary(recompute=True)`   | \u2705        | \u274c        | \u274c                | Recalculate protein/peptide stats and merge into `.summary`.     |\n            | Filtering samples               | `.update_summary(recompute=False)`  | \u274c        | \u274c        | \u274c                | Refresh `.summary` view of `.obs` without recomputation.         |\n            | Manual `.summary[...] = ...`    | `.update_summary()`                 | \u2705/\u274c     | \u2705        | \u2705                | Push edited `.summary` values back to `.obs`.                    |\n            | After setting `.summary = ...`  | `.update_summary()`                 | \u2705        | \u2705        | \u2705                | Sync back and recompute stats from new `.summary`.               |\n            | No changes                      | `.update_summary()`                 | \u274c        | \u274c        | \u274c                | No-op other than passive re-merge.                               |\n\n        Note:\n            - For most typical use cases, we auto-detect which flags need to be applied.\n                You usually don\u2019t need to set `recompute` or `sync_back` manually.\n            - `recompute=True` triggers `_update_metrics()` from `.prot` / `.pep` data.\n            - `sync_back=True` ensures changes to `.summary` are reflected in `.obs`.\n            - `.summary_is_stale` is automatically set when `.summary` is edited directly\n            (e.g. via `TrackedDataFrame`) or when assigned via the setter.\n        \"\"\"\n\n        # 1. Push back first if summary was edited by the user\n        if sync_back or getattr(self, \"_summary_is_stale\", False):\n            updated_prot, updated_pep = self._push_summary_to_obs()\n            updated_cols = list(set(updated_prot + updated_pep))\n            updated_str = f\" Columns updated: {', '.join(updated_cols)}.\" if updated_cols else \"\"\n\n            if verbose:\n                reason = \" (marked stale)\" if not sync_back else \"\"\n                print(f\"{format_log_prefix('update',indent=1)} Updating summary [sync_back]: pushed edits from `.summary` to `.obs`{reason}.\\n{format_log_prefix('blank',indent=2)}{updated_str}\")\n\n            self._summary_is_stale = False  # reset before recompute\n\n        # 2. Recompute or re-merge afterward\n        if recompute:\n            self._update_metrics() # type: ignore #, in MetricsMixin\n        self._merge_obs()\n        self._update_summary_metrics() # type: ignore #, in MetricsMixin\n        self.refresh_identifier_maps() # type: ignore #, in IdentifierMixin\n\n        # 3. Final messaging\n        if verbose and not (sync_back or self._summary_is_stale):\n            if recompute:\n                print(f\"{format_log_prefix('update',indent=3)} Updating summary [recompute]: Recomputed metrics and refreshed `.summary` from `.obs`.\")\n            else:\n                print(f\"{format_log_prefix('update',indent=3)} Updating summary [refresh]: Refreshed `.summary` view (no recompute).\")\n\n        # 4. Final cleanup\n        self._summary_is_stale = False\n\n    def _update_summary(self):\n        \"\"\"\n        Legacy method for updating the `.summary` table.\n\n        This method is retained for backward compatibility and simply calls the newer\n        `update_summary()` function with default arguments:\n        `recompute=True`, `sync_back=False`, and `verbose=False`.\n\n        Note:\n            This method is deprecated and may be removed in a future version.\n            Use `update_summary()` instead.\n        \"\"\"\n        print(\"\u26a0\ufe0f  Legacy _update_summary() called \u2014 consider switching to update_summary()\")\n        self.update_summary(recompute=True, sync_back=False, verbose=False)\n\n    def _merge_obs(self):\n        \"\"\"\n        Merge `.prot.obs` and `.pep.obs` into a unified sample-level summary.\n\n        This function combines metadata from protein-level and peptide-level `.obs` tables\n        into a single summary DataFrame. Shared columns (e.g., 'gradient', 'condition') \n        are taken from `.prot.obs` by default if present in both.\n\n        Returns:\n            pandas.DataFrame: Merged observation metadata for all samples.\n        \"\"\"\n        if self.prot is not None:\n            summary = self.prot.obs.copy()\n            if self.pep is not None:\n                for col in self.pep.obs.columns:\n                    if col not in summary.columns:\n                        summary[col] = self.pep.obs[col]\n        elif self.pep is not None:\n            summary = self.pep.obs.copy()\n        else:\n            summary = pd.DataFrame()\n\n\n        self._summary = TrackedDataFrame(\n            summary, parent=self, mark_stale_fn=self._mark_summary_stale)\n        self._previous_summary = summary.copy()\n\n    def _push_summary_to_obs(self, skip_if_contains='pep', verbose=False):\n        \"\"\"\n        Push changes from `.summary` back into `.prot.obs` and `.pep.obs`.\n\n        This function updates `.prot.obs` and `.pep.obs` with any modified columns\n        in `.summary`. To avoid overwriting incompatible fields, columns containing\n        `skip_if_contains` are excluded when updating `.prot.obs`, and similarly,\n        columns containing 'prot' are excluded when updating `.pep.obs`.\n\n        Args:\n            skip_if_contains (str): Substring used to skip incompatible columns for `.prot.obs`.\n                                    Defaults to 'pep'.\n            verbose (bool): If True, print updates being pushed to `.obs`.\n\n        Note:\n            This is typically called internally by `update_summary(sync_back=True)`.\n        \"\"\"\n        if not self._has_data():\n            return\n\n        def update_obs_with_summary(obs, summary, skip_if_contains):\n            skipped, updated = [], []\n            for col in summary.columns:\n                if skip_if_contains in str(col):\n                    skipped.append(col)\n                    continue\n                if col not in obs.columns or not obs[col].equals(summary[col]):\n                    updated.append(col)\n                obs[col] = summary[col]\n            return skipped, updated\n\n        if self.prot is not None:\n            if not self.prot.obs.index.equals(self._summary.index):\n                raise ValueError(\"Mismatch: .summary and .prot.obs have different sample indices.\")\n            skipped_prot, updated_prot = update_obs_with_summary(self.prot.obs, self._summary, skip_if_contains)\n        else:\n            skipped_prot, updated_prot = None, []\n\n        if self.pep is not None:\n            if not self.pep.obs.index.equals(self._summary.index):\n                raise ValueError(\"Mismatch: .summary and .pep.obs have different sample indices.\")\n            skipped_pep, updated_pep = update_obs_with_summary(self.pep.obs, self._summary, skip_if_contains='prot')\n        else:\n            skipped_pep, updated_pep = None, []\n\n        msg = \"Pushed summary values back to obs. \"\n        if skipped_prot:\n            msg += f\"Skipped for prot: {', '.join(skipped_prot)}. \"\n        if skipped_pep:\n            msg += f\"Skipped for pep: {', '.join(skipped_pep)}. \"\n\n        self._append_history(msg)\n        if verbose:\n            print(msg)\n\n        return updated_prot, updated_pep\n\n    def _mark_summary_stale(self):\n        \"\"\"\n        Mark the `.summary` as stale.\n\n        This sets the `_summary_is_stale` flag to True, indicating that the\n        summary is out of sync with `.obs` or metrics and should be updated\n        using `update_summary()`.\n\n        Note:\n            This is typically triggered automatically when `.summary` is edited\n            (e.g., via `TrackedDataFrame`) or reassigned.\n        \"\"\"\n        self._summary_is_stale = True\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.summary.SummaryMixin.update_summary","title":"update_summary","text":"<pre><code>update_summary(recompute=True, sync_back=False, verbose=True)\n</code></pre> <p>Update the <code>.summary</code> DataFrame to reflect current state of <code>.obs</code> and metadata.</p> <p>This function ensures <code>.summary</code> stays synchronized with sample-level metadata stored in <code>.prot.obs</code> / <code>.pep.obs</code>. You can choose to recompute metrics, sync edits back to <code>.obs</code>, or simply refresh the merged view.</p> <p>Parameters:</p> Name Type Description Default <code>recompute</code> <code>bool</code> <p>If True, re-calculate protein/peptide stats.</p> <code>True</code> <code>sync_back</code> <code>bool</code> <p>If True, push edited <code>.summary</code> values back to <code>.prot.obs</code> / <code>.pep.obs</code>. False by default, as <code>.summary</code> is derived.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print action messages.</p> <code>True</code> Typical Usage Scenarios Scenario Call recompute sync_back _summary_is_stale Effect Filtering <code>.prot</code> or <code>.pep</code> <code>.update_summary(recompute=True)</code> \u2705 \u274c \u274c Recalculate protein/peptide stats and merge into <code>.summary</code>. Filtering samples <code>.update_summary(recompute=False)</code> \u274c \u274c \u274c Refresh <code>.summary</code> view of <code>.obs</code> without recomputation. Manual <code>.summary[...] = ...</code> <code>.update_summary()</code> \u2705/\u274c \u2705 \u2705 Push edited <code>.summary</code> values back to <code>.obs</code>. After setting <code>.summary = ...</code> <code>.update_summary()</code> \u2705 \u2705 \u2705 Sync back and recompute stats from new <code>.summary</code>. No changes <code>.update_summary()</code> \u274c \u274c \u274c No-op other than passive re-merge. Note <ul> <li>For most typical use cases, we auto-detect which flags need to be applied.     You usually don\u2019t need to set <code>recompute</code> or <code>sync_back</code> manually.</li> <li><code>recompute=True</code> triggers <code>_update_metrics()</code> from <code>.prot</code> / <code>.pep</code> data.</li> <li><code>sync_back=True</code> ensures changes to <code>.summary</code> are reflected in <code>.obs</code>.</li> <li><code>.summary_is_stale</code> is automatically set when <code>.summary</code> is edited directly (e.g. via <code>TrackedDataFrame</code>) or when assigned via the setter.</li> </ul> Source code in <code>src/scpviz/pAnnData/summary.py</code> <pre><code>def update_summary(self, recompute=True, sync_back=False, verbose=True):\n    \"\"\"\n    Update the `.summary` DataFrame to reflect current state of `.obs` and metadata.\n\n    This function ensures `.summary` stays synchronized with sample-level metadata\n    stored in `.prot.obs` / `.pep.obs`. You can choose to recompute metrics,\n    sync edits back to `.obs`, or simply refresh the merged view.\n\n    Args:\n        recompute (bool): If True, re-calculate protein/peptide stats.\n        sync_back (bool): If True, push edited `.summary` values back to `.prot.obs` / `.pep.obs`.\n            False by default, as `.summary` is derived.\n        verbose (bool): If True, print action messages.\n\n    ??? example \"Typical Usage Scenarios\"\n        | Scenario                        | Call                                | recompute | sync_back | _summary_is_stale | Effect                                                           |\n        |---------------------------------|-------------------------------------|-----------|-----------|-------------------|------------------------------------------------------------------|\n        | Filtering `.prot` or `.pep`     | `.update_summary(recompute=True)`   | \u2705        | \u274c        | \u274c                | Recalculate protein/peptide stats and merge into `.summary`.     |\n        | Filtering samples               | `.update_summary(recompute=False)`  | \u274c        | \u274c        | \u274c                | Refresh `.summary` view of `.obs` without recomputation.         |\n        | Manual `.summary[...] = ...`    | `.update_summary()`                 | \u2705/\u274c     | \u2705        | \u2705                | Push edited `.summary` values back to `.obs`.                    |\n        | After setting `.summary = ...`  | `.update_summary()`                 | \u2705        | \u2705        | \u2705                | Sync back and recompute stats from new `.summary`.               |\n        | No changes                      | `.update_summary()`                 | \u274c        | \u274c        | \u274c                | No-op other than passive re-merge.                               |\n\n    Note:\n        - For most typical use cases, we auto-detect which flags need to be applied.\n            You usually don\u2019t need to set `recompute` or `sync_back` manually.\n        - `recompute=True` triggers `_update_metrics()` from `.prot` / `.pep` data.\n        - `sync_back=True` ensures changes to `.summary` are reflected in `.obs`.\n        - `.summary_is_stale` is automatically set when `.summary` is edited directly\n        (e.g. via `TrackedDataFrame`) or when assigned via the setter.\n    \"\"\"\n\n    # 1. Push back first if summary was edited by the user\n    if sync_back or getattr(self, \"_summary_is_stale\", False):\n        updated_prot, updated_pep = self._push_summary_to_obs()\n        updated_cols = list(set(updated_prot + updated_pep))\n        updated_str = f\" Columns updated: {', '.join(updated_cols)}.\" if updated_cols else \"\"\n\n        if verbose:\n            reason = \" (marked stale)\" if not sync_back else \"\"\n            print(f\"{format_log_prefix('update',indent=1)} Updating summary [sync_back]: pushed edits from `.summary` to `.obs`{reason}.\\n{format_log_prefix('blank',indent=2)}{updated_str}\")\n\n        self._summary_is_stale = False  # reset before recompute\n\n    # 2. Recompute or re-merge afterward\n    if recompute:\n        self._update_metrics() # type: ignore #, in MetricsMixin\n    self._merge_obs()\n    self._update_summary_metrics() # type: ignore #, in MetricsMixin\n    self.refresh_identifier_maps() # type: ignore #, in IdentifierMixin\n\n    # 3. Final messaging\n    if verbose and not (sync_back or self._summary_is_stale):\n        if recompute:\n            print(f\"{format_log_prefix('update',indent=3)} Updating summary [recompute]: Recomputed metrics and refreshed `.summary` from `.obs`.\")\n        else:\n            print(f\"{format_log_prefix('update',indent=3)} Updating summary [refresh]: Refreshed `.summary` view (no recompute).\")\n\n    # 4. Final cleanup\n    self._summary_is_stale = False\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.validation.ValidationMixin","title":"ValidationMixin","text":"<p>Provides internal validation checks for data consistency across <code>.prot</code>, <code>.pep</code>, <code>.summary</code>, and the RS matrix.</p> <p>This mixin ensures that core components of the pAnnData object are structurally aligned and reports any mismatches or inconsistencies in dimensionality, identifiers, and matrix shapes.</p> <p>Functions:</p> <pre><code>validate:\n    Comprehensive integrity check for .prot, .pep, .summary, and RS matrix. Returns True if all checks pass.\n\n_check_data:\n    Utility method to verify that protein or peptide data exists. Raises error if missing.\n\n_check_rankcol:\n    Verifies presence of 'Average: &lt;class&gt;' and 'Rank: &lt;class&gt;' columns in `.var` for use with rank-based plotting.\n</code></pre> Source code in <code>src/scpviz/pAnnData/validation.py</code> <pre><code>class ValidationMixin:\n    \"\"\"\n    Provides internal validation checks for data consistency across `.prot`, `.pep`, `.summary`, and the RS matrix.\n\n    This mixin ensures that core components of the pAnnData object are structurally aligned and reports any mismatches\n    or inconsistencies in dimensionality, identifiers, and matrix shapes.\n\n    Functions:\n\n        validate:\n            Comprehensive integrity check for .prot, .pep, .summary, and RS matrix. Returns True if all checks pass.\n\n        _check_data:\n            Utility method to verify that protein or peptide data exists. Raises error if missing.\n\n        _check_rankcol:\n            Verifies presence of 'Average: &lt;class&gt;' and 'Rank: &lt;class&gt;' columns in `.var` for use with rank-based plotting.\n    \"\"\"\n    def validate(self, verbose=True):\n        \"\"\"\n        Check internal consistency of the pAnnData object.\n\n        This function verifies that `.prot`, `.pep`, `.summary`, and `.rs` are \n        internally aligned, with matching dimensions, index values, and consistency\n        between shared sample identifiers. It prints helpful diagnostics if issues\n        are detected.\n\n        Checks Performed:\n        -----------------\n        - `.obs` and `.var` shapes match `.X` for both `.prot` and `.pep`\n        - `.obs` and `.var` indices are unique\n        - `.prot.obs_names` match `.pep.obs_names`\n        - `.summary.index` matches `.obs.index` for both `.prot` and `.pep`\n        - `.rs.shape` matches (n_proteins, n_peptides)\n        - Prints RS matrix sparsity and connectivity stats (if verbose=True)\n\n        Args:\n            verbose (bool): If True, print summary of validation and RS stats.\n\n        Returns:\n            bool: True if all checks pass, False otherwise.\n\n        Example:\n            To validate the pAnnData object and check for consistency issues:\n                ```python\n                is_valid = pdata.validate()\n                if not is_valid:\n                    print(\"Fix issues before proceeding.\")\n                ```\n        \"\"\"\n        issues = []\n\n        # --- Check prot and pep dimensions ---\n        for label, ad in [('prot', self.prot), ('pep', self.pep)]:\n            if ad is not None:\n                if ad.obs.shape[0] != ad.X.shape[0]:\n                    issues.append(f\"{label}.obs rows ({ad.obs.shape[0]}) != {label}.X rows ({ad.X.shape[0]})\")\n                if ad.var.shape[0] != ad.X.shape[1]:\n                    issues.append(f\"{label}.var rows ({ad.var.shape[0]}) != {label}.X cols ({ad.X.shape[1]})\")\n                if ad.obs.index.duplicated().any():\n                    issues.append(f\"{label}.obs has duplicated index values\")\n                if ad.var.index.duplicated().any():\n                    issues.append(f\"{label}.var has duplicated index values\")\n\n        # --- Check obs name overlap between prot and pep ---\n        if self.prot is not None and self.pep is not None:\n            prot_names = set(self.prot.obs_names)\n            pep_names = set(self.pep.obs_names)\n            if prot_names != pep_names:\n                missing_in_pep = prot_names - pep_names\n                missing_in_prot = pep_names - prot_names\n                issues.append(\"prot and pep obs_names do not match\")\n                if missing_in_pep:\n                    issues.append(f\"  - {len(missing_in_pep)} samples in prot but not in pep\")\n                if missing_in_prot:\n                    issues.append(f\"  - {len(missing_in_prot)} samples in pep but not in prot\")\n\n        # --- Check .summary alignment ---\n        if self._summary is not None:\n            for label, ad in [('prot', self.prot), ('pep', self.pep)]:\n                if ad is not None:\n                    if not ad.obs.index.equals(self._summary.index):\n                        issues.append(f\"{label}.obs index does not match .summary index\")\n\n        # --- Check RS matrix shape + stats ---\n        if self.rs is not None and self.prot is not None and self.pep is not None:\n            rs_shape = self.rs.shape\n            expected_shape = (self.prot.shape[1], self.pep.shape[1])\n            if rs_shape != expected_shape:\n                issues.append(f\"RS shape mismatch: got {rs_shape}, expected {expected_shape} (proteins \u00d7 peptides)\")\n            elif verbose:\n                nnz = self.rs.nnz if sparse.issparse(self.rs) else np.count_nonzero(self.rs)\n                total = self.rs.shape[0] * self.rs.shape[1]\n                sparsity = 100 * (1 - nnz / total)\n                print(f\"{format_log_prefix('info_only', indent=1)} RS matrix: {rs_shape} (proteins \u00d7 peptides), sparsity: {sparsity:.2f}%\")\n\n                rs = self.rs\n\n                row_links = rs.getnnz(axis=1)  # peptides per protein\n                col_links = rs.getnnz(axis=0)  # proteins per peptide\n\n                # Unique peptides (linked to only 1 protein)\n                unique_peptides_mask = col_links == 1\n                unique_counts = rs[:, unique_peptides_mask].getnnz(axis=1)  # unique peptides per protein\n\n                # Summary stats\n                print(f\"   - Proteins with \u22652 *unique* linked peptides: {(unique_counts &gt;= 2).sum()}/{rs_shape[0]}\")\n                print(f\"   - Peptides linked to \u22652 proteins: {(col_links &gt;= 2).sum()}/{rs_shape[1]}\")\n                print(f\"   - Mean peptides per protein: {row_links.mean():.2f}\")\n                print(f\"   - Mean proteins per peptide: {col_links.mean():.2f}\")\n\n        # --- Summary of results ---\n        if issues:\n            if verbose:\n                print(f\"{format_log_prefix('error')} Validation failed with the following issues:\")\n                for issue in issues:\n                    print(\" -\", issue)\n            return False\n        else:\n            if verbose:\n                print(f\"{format_log_prefix('result')} pAnnData object is valid.\")\n            return True\n\n    def _check_data(self, on):\n        \"\"\"\n        Internal check for existence of protein or peptide data.\n\n        Args:\n            on (str): One of 'protein', 'peptide', 'prot', or 'pep'.\n\n        Returns:\n            bool: True if corresponding data exists.\n\n        Raises:\n            ValueError: If `on` is not a valid option or if the corresponding data is missing.\n        \"\"\"\n        # check if protein or peptide data exists\n        if on not in ['protein', 'peptide' , 'prot', 'pep']:\n            raise ValueError(\"Invalid input: on must be either 'protein' or 'peptide'.\")\n        elif (on == 'protein' or on == 'prot') and self.prot is None:\n            raise ValueError(\"No protein data found in AnnData object.\")\n        elif (on == 'peptide' or on == 'pep') and self.pep is None:\n            raise ValueError(\"No peptide data found in AnnData object.\")\n        else:\n            return True\n\n    def _check_rankcol(self, on = 'protein', class_values = None):\n        \"\"\"\n        Internal check for existence of average and rank columns in `.var`.\n\n        This function ensures that for each `class_value`, both of the following \n        columns exist in the `.var` of the chosen modality:\n            - 'Average: &lt;class_value&gt;'\n            - 'Rank: &lt;class_value&gt;'\n\n        Args:\n            on (str): 'protein' or 'peptide'.\n            class_values (list of str): Class values expected to have been used in plot_rankquank().\n\n        Raises:\n            ValueError: If class_values is None, or if required columns are missing from `.var`.\n        \"\"\"\n        # check if average and rank columns exist for the specified class values\n        if on == 'protein':\n            adata = self.prot\n        elif on == 'peptide':\n            adata = self.pep\n\n        if class_values is None:\n            raise ValueError(\"class_values must be None\")\n\n        for class_value in class_values:\n            average_col = f'Average: {class_value}'\n            rank_col = f'Rank: {class_value}'\n            if average_col not in adata.var.columns or rank_col not in adata.var.columns:\n                raise ValueError(f\"Class name not found in .var. Please run plot_rankquank() beforehand and check that the input matches the class names in {on}.var['Average: ']\")\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.validation.ValidationMixin.validate","title":"validate","text":"<pre><code>validate(verbose=True)\n</code></pre> <p>Check internal consistency of the pAnnData object.</p> <p>This function verifies that <code>.prot</code>, <code>.pep</code>, <code>.summary</code>, and <code>.rs</code> are  internally aligned, with matching dimensions, index values, and consistency between shared sample identifiers. It prints helpful diagnostics if issues are detected.</p>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.validation.ValidationMixin.validate--checks-performed","title":"Checks Performed:","text":"<ul> <li><code>.obs</code> and <code>.var</code> shapes match <code>.X</code> for both <code>.prot</code> and <code>.pep</code></li> <li><code>.obs</code> and <code>.var</code> indices are unique</li> <li><code>.prot.obs_names</code> match <code>.pep.obs_names</code></li> <li><code>.summary.index</code> matches <code>.obs.index</code> for both <code>.prot</code> and <code>.pep</code></li> <li><code>.rs.shape</code> matches (n_proteins, n_peptides)</li> <li>Prints RS matrix sparsity and connectivity stats (if verbose=True)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, print summary of validation and RS stats.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if all checks pass, False otherwise.</p> Example <p>To validate the pAnnData object and check for consistency issues:     <pre><code>is_valid = pdata.validate()\nif not is_valid:\n    print(\"Fix issues before proceeding.\")\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/validation.py</code> <pre><code>def validate(self, verbose=True):\n    \"\"\"\n    Check internal consistency of the pAnnData object.\n\n    This function verifies that `.prot`, `.pep`, `.summary`, and `.rs` are \n    internally aligned, with matching dimensions, index values, and consistency\n    between shared sample identifiers. It prints helpful diagnostics if issues\n    are detected.\n\n    Checks Performed:\n    -----------------\n    - `.obs` and `.var` shapes match `.X` for both `.prot` and `.pep`\n    - `.obs` and `.var` indices are unique\n    - `.prot.obs_names` match `.pep.obs_names`\n    - `.summary.index` matches `.obs.index` for both `.prot` and `.pep`\n    - `.rs.shape` matches (n_proteins, n_peptides)\n    - Prints RS matrix sparsity and connectivity stats (if verbose=True)\n\n    Args:\n        verbose (bool): If True, print summary of validation and RS stats.\n\n    Returns:\n        bool: True if all checks pass, False otherwise.\n\n    Example:\n        To validate the pAnnData object and check for consistency issues:\n            ```python\n            is_valid = pdata.validate()\n            if not is_valid:\n                print(\"Fix issues before proceeding.\")\n            ```\n    \"\"\"\n    issues = []\n\n    # --- Check prot and pep dimensions ---\n    for label, ad in [('prot', self.prot), ('pep', self.pep)]:\n        if ad is not None:\n            if ad.obs.shape[0] != ad.X.shape[0]:\n                issues.append(f\"{label}.obs rows ({ad.obs.shape[0]}) != {label}.X rows ({ad.X.shape[0]})\")\n            if ad.var.shape[0] != ad.X.shape[1]:\n                issues.append(f\"{label}.var rows ({ad.var.shape[0]}) != {label}.X cols ({ad.X.shape[1]})\")\n            if ad.obs.index.duplicated().any():\n                issues.append(f\"{label}.obs has duplicated index values\")\n            if ad.var.index.duplicated().any():\n                issues.append(f\"{label}.var has duplicated index values\")\n\n    # --- Check obs name overlap between prot and pep ---\n    if self.prot is not None and self.pep is not None:\n        prot_names = set(self.prot.obs_names)\n        pep_names = set(self.pep.obs_names)\n        if prot_names != pep_names:\n            missing_in_pep = prot_names - pep_names\n            missing_in_prot = pep_names - prot_names\n            issues.append(\"prot and pep obs_names do not match\")\n            if missing_in_pep:\n                issues.append(f\"  - {len(missing_in_pep)} samples in prot but not in pep\")\n            if missing_in_prot:\n                issues.append(f\"  - {len(missing_in_prot)} samples in pep but not in prot\")\n\n    # --- Check .summary alignment ---\n    if self._summary is not None:\n        for label, ad in [('prot', self.prot), ('pep', self.pep)]:\n            if ad is not None:\n                if not ad.obs.index.equals(self._summary.index):\n                    issues.append(f\"{label}.obs index does not match .summary index\")\n\n    # --- Check RS matrix shape + stats ---\n    if self.rs is not None and self.prot is not None and self.pep is not None:\n        rs_shape = self.rs.shape\n        expected_shape = (self.prot.shape[1], self.pep.shape[1])\n        if rs_shape != expected_shape:\n            issues.append(f\"RS shape mismatch: got {rs_shape}, expected {expected_shape} (proteins \u00d7 peptides)\")\n        elif verbose:\n            nnz = self.rs.nnz if sparse.issparse(self.rs) else np.count_nonzero(self.rs)\n            total = self.rs.shape[0] * self.rs.shape[1]\n            sparsity = 100 * (1 - nnz / total)\n            print(f\"{format_log_prefix('info_only', indent=1)} RS matrix: {rs_shape} (proteins \u00d7 peptides), sparsity: {sparsity:.2f}%\")\n\n            rs = self.rs\n\n            row_links = rs.getnnz(axis=1)  # peptides per protein\n            col_links = rs.getnnz(axis=0)  # proteins per peptide\n\n            # Unique peptides (linked to only 1 protein)\n            unique_peptides_mask = col_links == 1\n            unique_counts = rs[:, unique_peptides_mask].getnnz(axis=1)  # unique peptides per protein\n\n            # Summary stats\n            print(f\"   - Proteins with \u22652 *unique* linked peptides: {(unique_counts &gt;= 2).sum()}/{rs_shape[0]}\")\n            print(f\"   - Peptides linked to \u22652 proteins: {(col_links &gt;= 2).sum()}/{rs_shape[1]}\")\n            print(f\"   - Mean peptides per protein: {row_links.mean():.2f}\")\n            print(f\"   - Mean proteins per peptide: {col_links.mean():.2f}\")\n\n    # --- Summary of results ---\n    if issues:\n        if verbose:\n            print(f\"{format_log_prefix('error')} Validation failed with the following issues:\")\n            for issue in issues:\n                print(\" -\", issue)\n        return False\n    else:\n        if verbose:\n            print(f\"{format_log_prefix('result')} pAnnData object is valid.\")\n        return True\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.history.HasHistory","title":"HasHistory","text":"<p>               Bases: <code>Protocol</code></p> <p>Tracks the operation history applied to the pAnnData object.</p> <p>Methods:</p> Name Description <code>_append_history</code> <p>Adds a string to the transformation history log.</p> <code>print_history</code> <p>Prints the current transformation history.</p> Source code in <code>src/scpviz/pAnnData/history.py</code> <pre><code>class HasHistory(Protocol):\n    \"\"\"\n    Tracks the operation history applied to the pAnnData object.\n\n    Functions:\n        _append_history: Adds a string to the transformation history log.\n        print_history: Prints the current transformation history.\n    \"\"\"\n    _history: list\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.history.HistoryMixin","title":"HistoryMixin","text":"<p>Mixin for tracking the history of operations performed on a pAnnData object.</p> <p>This mixin provides simple utilities to log and review transformations or analysis steps performed on the data object. Each action is stored as a  string in the internal <code>_history</code> list.</p> <p>Features:</p> <ul> <li>Track transformations or analysis steps as text entries</li> <li>Print chronological history of actions performed</li> </ul> <p>Methods:</p> Name Description <code>_append_history</code> <p>Add a custom string to the internal history list  </p> <code>print_history</code> <p>Print the full history in a formatted list</p> Source code in <code>src/scpviz/pAnnData/history.py</code> <pre><code>class HistoryMixin:\n    \"\"\"\n    Mixin for tracking the history of operations performed on a pAnnData object.\n\n    This mixin provides simple utilities to log and review transformations or\n    analysis steps performed on the data object. Each action is stored as a \n    string in the internal `_history` list.\n\n    Features:\n\n    - Track transformations or analysis steps as text entries\n    - Print chronological history of actions performed\n\n    Functions:\n        _append_history: Add a custom string to the internal history list  \n        print_history: Print the full history in a formatted list\n    \"\"\"\n    def _append_history(self: HasHistory, action):\n        \"\"\"\n        Append a timestamped entry to the internal transformation history log.\n\n        Each entry records the current date and time alongside a string description \n        of the action performed. Useful for tracking the sequence and timing of \n        transformations on the pAnnData object.\n\n        Args:\n            action (str): Description of the action performed.\n        \"\"\"\n        timestamp = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n        self._history.append(f\"{timestamp} {action}\")\n\n    def print_history(self: HasHistory): \n        \"\"\"\n        Print the current transformation history in a numbered format.\n\n        Each logged action is printed with its index, showing the chronological\n        sequence of operations applied to the object.\n        \"\"\"\n        formatted_history = \"\\n\".join(f\"{i}: {action}\" for i, action in enumerate(self._history, 1))\n        print(\"-------------------------------\\nHistory:\\n-------------------------------\\n\" + formatted_history)\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.history.HistoryMixin.print_history","title":"print_history","text":"<pre><code>print_history()\n</code></pre> <p>Print the current transformation history in a numbered format.</p> <p>Each logged action is printed with its index, showing the chronological sequence of operations applied to the object.</p> Source code in <code>src/scpviz/pAnnData/history.py</code> <pre><code>def print_history(self: HasHistory): \n    \"\"\"\n    Print the current transformation history in a numbered format.\n\n    Each logged action is printed with its index, showing the chronological\n    sequence of operations applied to the object.\n    \"\"\"\n    formatted_history = \"\\n\".join(f\"{i}: {action}\" for i, action in enumerate(self._history, 1))\n    print(\"-------------------------------\\nHistory:\\n-------------------------------\\n\" + formatted_history)\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin","title":"IdentifierMixin","text":"<p>Handles mapping between genes, accessions, and peptides.</p> <p>This mixin provides utilities for:</p> <ul> <li> <p>Building and caching bidirectional mappings:</p> <ul> <li>For proteins: gene \u2194 accession  </li> <li>For peptides: peptide \u2194 protein accession</li> </ul> </li> <li> <p>Updating or refreshing identifier maps manually or via UniProt</p> </li> <li>Automatically filling in missing gene names using the UniProt API</li> </ul> <p>These mappings are cached and used throughout the <code>pAnnData</code> object to support resolution of user queries and consistent gene-accession-peptide tracking.</p> <p>Methods:</p> Name Description <code>_build_identifier_maps</code> <p>Create forward/reverse maps based on protein or peptide data</p> <code>refresh_identifier_maps</code> <p>Clear cached mappings to force rebuild</p> <code>get_identifier_maps</code> <p>Retrieve (gene \u2192 acc, acc \u2192 gene) or (peptide \u2194 protein) maps</p> <code>update_identifier_maps</code> <p>Add or overwrite mappings (e.g., manual corrections)</p> <code>update_missing_genes</code> <p>Fill missing gene names using the UniProt API</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>class IdentifierMixin:\n    \"\"\"\n    Handles mapping between genes, accessions, and peptides.\n\n    This mixin provides utilities for:\n\n    - Building and caching bidirectional mappings:\n        * For proteins: gene \u2194 accession  \n        * For peptides: peptide \u2194 protein accession\n\n    - Updating or refreshing identifier maps manually or via UniProt\n    - Automatically filling in missing gene names using the UniProt API\n\n    These mappings are cached and used throughout the `pAnnData` object to support resolution of user queries and consistent gene-accession-peptide tracking.\n\n    Functions:\n        _build_identifier_maps: Create forward/reverse maps based on protein or peptide data\n        refresh_identifier_maps: Clear cached mappings to force rebuild\n        get_identifier_maps: Retrieve (gene \u2192 acc, acc \u2192 gene) or (peptide \u2194 protein) maps\n        update_identifier_maps: Add or overwrite mappings (e.g., manual corrections)\n        update_missing_genes: Fill missing gene names using the UniProt API\n    \"\"\"\n\n    def _build_identifier_maps(self, adata, gene_col=\"Genes\"):\n        \"\"\"\n        Build bidirectional identifier mappings for genes/proteins or peptides/proteins.\n\n        Depending on whether `adata` is `.prot` or `.pep`, this builds:\n\n        - For proteins: gene \u2194 accession\n        - For peptides: peptide \u2194 protein accession\n\n        Args:\n            adata (AnnData): Either `self.prot` or `self.pep`.\n            gene_col (str): Column name in `.var` containing gene names (default: \"Genes\").\n\n        Returns:\n            tuple: A pair of dictionaries (`forward`, `reverse`) for identifier lookup.\n\n        Note:\n            For peptides, mapping relies on `utils.get_pep_prot_mapping()` to resolve protein accessions.\n\n        Raises:\n            Warning if peptide-to-protein mapping cannot be built.\n        \"\"\"\n        from pandas import notna\n\n        forward = {}\n        reverse = {}\n\n        if adata is self.prot:\n            if gene_col in adata.var.columns:\n                for acc, gene in zip(adata.var_names, adata.var[gene_col]):\n                    if notna(gene):\n                        gene = str(gene)\n                        forward[gene] = acc\n                        reverse[acc] = gene\n\n        elif adata is self.pep:\n            try:\n                prot_acc_col = utils.get_pep_prot_mapping(self)\n                pep_to_prot = adata.var[prot_acc_col]\n                for pep, prot in zip(adata.var_names, pep_to_prot):\n                    if notna(prot):\n                        forward[prot] = pep\n                        reverse[pep] = prot\n            except Exception as e:\n                warnings.warn(f\"Could not build peptide-to-protein map: {e}\")\n\n        return forward, reverse\n\n    def refresh_identifier_maps(self):\n        \"\"\"\n        Clear cached identifier maps to force regeneration on next access.\n\n        This removes the following attributes if present:\n\n        - `_gene_maps_protein`: Gene \u2194 Accession map for proteins\n        - `_protein_maps_peptide`: Protein \u2194 Peptide map for peptides\n\n        Useful when `.var` annotations are updated and identifier mappings may have changed.\n        \"\"\"\n        for attr in [\"_gene_maps_protein\", \"_protein_maps_peptide\"]:\n            if hasattr(self, attr):\n                delattr(self, attr)\n\n    def get_identifier_maps(self, on='protein'):\n        \"\"\"\n        Retrieve gene/accession or peptide/protein mapping dictionaries.\n\n        Depending on the `on` argument, returns a tuple of forward and reverse mappings:\n\n        - If `on='protein'`: (gene \u2192 accession, accession \u2192 gene)\n\n        - If `on='peptide'`: (protein accession \u2192 peptide, peptide \u2192 protein accession)\n\n        Note: Alias `get_gene_maps()` also calls this function for compatibility.\n\n        Args:\n            on (str): Source of mapping. Must be `'protein'` or `'peptide'`.\n\n        Returns:\n            Tuple[dict, dict]: (forward mapping, reverse mapping)\n\n        Raises:\n            ValueError: If `on` is not `'protein'` or `'peptide'`.\n        \"\"\"\n        if on in ('protein','prot'):\n            return self._cached_identifier_maps_protein\n        elif on in ('peptide','pep'):\n            return self._cached_identifier_maps_peptide\n        else:\n            raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n    # TODO: add peptide remapping to var, but need to also update rs if you do this.\n    def update_identifier_maps(self, mapping, on='protein', direction='forward', overwrite=False, verbose=True):\n        \"\"\"\n        Update cached identifier maps with user-supplied mappings.\n\n        This function updates the internal forward and reverse identifier maps\n        for either proteins or peptides. Ensures consistency by updating both\n        directions of the mapping.\n\n        - For `'protein'`:\n            * forward: gene \u2192 accession  \n            * reverse: accession \u2192 gene\n\n        - For `'peptide'`:\n            * forward: protein accession \u2192 peptide\n            * reverse: peptide \u2192 protein accession\n\n        Args:\n            mapping (dict): Dictionary of mappings to add.\n            on (str): Which maps to update. Must be `'protein'` or `'peptide'`.\n            direction (str): `'forward'` or `'reverse'` \u2014 determines how the `mapping` should be interpreted.\n            overwrite (bool): If True, allows overwriting existing entries.\n            verbose (bool): If True, prints a summary of updated keys.\n\n        Note:\n            The corresponding reverse map is automatically updated to maintain bidirectional consistency.\n\n        Example:\n            Add new gene-to-accession mappings (protein):\n                ```python\n                pdata.update_identifier_maps(\n                    {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n                    on='protein',\n                    direction='forward'\n                )\n                ```\n\n            Add peptide \u2192 protein mappings:\n                ```python\n                pdata.update_identifier_maps(\n                    {'PEPTIDE_ABC': 'P12345'},\n                    on='peptide',\n                    direction='reverse'\n                )\n                ```\n\n            Overwrite a protein \u2192 gene mapping:\n                ```python\n                pdata.update_identifier_maps(\n                    {'P12345': 'NEWGENE'},\n                    on='protein',\n                    direction='reverse',\n                    overwrite=True\n                )\n                ```\n\n        \"\"\"\n        if on == 'protein':\n            forward, reverse = self._cached_identifier_maps_protein\n        elif on == 'peptide':\n            forward, reverse = self._cached_identifier_maps_peptide\n        else:\n            raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n        source_map = forward if direction == 'forward' else reverse\n        target_map = reverse if direction == 'forward' else forward\n\n        added, updated, skipped = 0, 0, 0\n\n        for key, val in mapping.items():\n            if key in source_map:\n                if overwrite:\n                    source_map[key] = val\n                    target_map[val] = key\n                    updated += 1\n                else:\n                    skipped += 1\n            else:\n                source_map[key] = val\n                target_map[val] = key\n                added += 1\n\n        message = (\n            f\"[update_identifier_maps] Updated '{on}' ({direction}): \"\n            f\"{added} added, {updated} overwritten, {skipped} skipped.\"\n        )\n\n        if verbose:\n            print(message)\n        self._append_history(message)\n\n        # Update .prot.var[\"Genes\"] if updating protein identifier reverse map (accession \u2192 gene)\n        if on == 'protein' and direction == 'reverse':\n            updated_var_count = 0\n            updated_accessions = []\n\n            for acc, gene in mapping.items():\n                if acc in self.prot.var_names:\n                    self.prot.var.at[acc, \"Genes\"] = gene\n                    updated_accessions.append(acc)\n                    updated_var_count += 1\n\n            if updated_var_count &gt; 0:\n                var_message = (\n                    f\"\ud83d\udd01 Updated `.prot.var['Genes']` for {updated_var_count} entries from custom mapping. \"\n                    f\"(View details in `pdata.metadata['identifier_map_history']`)\"\n                )\n                if verbose:\n                    print(var_message)\n                self._append_history(var_message)\n\n        # Log detailed update history for all cases\n        import datetime\n\n        record = {\n            'on': on,\n            'direction': direction,\n            'input_mapping': dict(mapping),  # shallow copy\n            'overwrite': overwrite,\n            'timestamp': datetime.datetime.now().isoformat(timespec='seconds'),\n            'summary': {\n                'added': added,\n                'updated': updated,\n                'skipped': skipped,\n            }\n        }\n\n        if on == 'protein' and direction == 'reverse':\n            record['updated_var_column'] = {\n                'column': 'Genes',\n                'accessions': updated_accessions,\n                'n_updated': updated_var_count\n            }\n\n        self.metadata.setdefault(\"identifier_map_history\", []).append(record)\n\n    get_gene_maps = get_identifier_maps\n\n    def update_missing_genes(self, gene_col=\"Genes\", verbose=True):\n        \"\"\"\n        Fill missing gene names in `.prot.var` using UniProt API.\n\n        This function searches for missing values in the specified gene column\n        and attempts to fill them by querying the UniProt API using protein\n        accession IDs. If a gene name cannot be found, a placeholder\n        'UNKNOWN_&lt;accession&gt;' is used instead.\n\n        Args:\n            gene_col (str): Column name in `.prot.var` to update (default: \"Genes\").\n            verbose (bool): Whether to print summary information (default: True).\n\n        Returns:\n            None\n\n        Note:\n            - This function only operates on `.prot.var`, not `.pep.var`.\n            - If UniProt is unavailable or returns no match, the missing entry is filled as `'UNKNOWN_&lt;accession&gt;'`.\n            - To manually correct unknown entries later, use `update_identifier_maps()` with `direction='reverse'`.\n\n        Example:\n            Automatically fill missing gene names using UniProt:\n                ```python\n                pdata.update_missing_genes()\n                ```\n        \"\"\"\n        var = self.prot.var\n\n        if gene_col not in var.columns:\n            if verbose:\n                print(f\"{format_log_prefix('warn')} Column '{gene_col}' not found in .prot.var.\")\n            return\n\n        missing_mask = var[gene_col].isna()\n        if not missing_mask.any():\n            if verbose:\n                print(f\"{format_log_prefix('result')} No missing gene names found.\")\n            return\n\n        accessions = var.index[missing_mask].tolist()\n        if verbose:\n            print(f\"{format_log_prefix('info_only')} {len(accessions)} proteins with missing gene names.\")\n\n        try:\n            df = utils.get_uniprot_fields(\n                accessions,\n                search_fields=[\"accession\", \"gene_primary\"],\n                standardize=True\n            )\n        except Exception as e:\n            print(f\"{format_log_prefix('error')} UniProt query failed: {e}\")\n            return\n        df = utils.standardize_uniprot_columns(df)\n\n        if df.empty or \"accession\" not in df.columns or \"gene_primary\" not in df.columns:\n            print(f\"{format_log_prefix('warn')} UniProt returned no usable gene mapping columns.\")\n            return\n\n        gene_map = dict(zip(df[\"accession\"], df[\"gene_primary\"]))\n        filled = self.prot.var.loc[missing_mask].index.map(lambda acc: gene_map.get(acc))\n        final_genes = [\n            gene if pd.notna(gene) else f\"UNKNOWN_{acc}\"\n            for acc, gene in zip(self.prot.var.loc[missing_mask].index, filled)\n        ]\n        self.prot.var.loc[missing_mask, gene_col] = final_genes\n\n        found = sum(pd.notna(filled))\n        unknown = len(final_genes) - found\n        if verbose:\n            if found:\n                print(f\"{format_log_prefix('result')} Recovered {found} gene name(s) from UniProt. Genes found:\")\n                filled_clean = [str(g) for g in filled if pd.notna(g)]\n                preview = \", \".join(filled_clean[:10])\n                if found &gt; 10:\n                    preview += \"...\"\n                print(\"        \", preview)\n            if unknown:\n                missing_ids = self.prot.var.loc[missing_mask].index[pd.isna(filled)]\n                print(f\"{format_log_prefix('warn')} {unknown} gene name(s) still missing. Assigned as 'UNKNOWN_&lt;accession&gt;' for:\")\n                print(\"        \", \", \".join(missing_ids[:5]) + (\"...\" if unknown &gt; 10 else \"\"))\n                print(\"     \ud83d\udca1 Tip: You can update these using `pdata.update_identifier_maps({'GENE': 'ACCESSION'}, on='protein', direction='reverse', overwrite=True)`\\n\")\n\n    def search_annotations(self, query, on='protein', search_columns=None, case=False, return_all_matches=True):\n        \"\"\"\n        Search protein or peptide annotations for matching biological terms.\n\n        This function scans `.prot.var` or `.pep.var` for entries containing the provided keyword(s),\n        across common annotation fields.\n\n        Args:\n            query (str or list of str): Term(s) to search for (e.g., \"keratin\", \"KRT\").\n            on (str): Whether to search `\"protein\"` or `\"peptide\"` annotations (default: `\"protein\"`).\n            search_columns (list of str, optional): Columns to search in. Defaults to common biological fields.\n            case (bool): Case-sensitive search (default: False).\n            return_all_matches (bool): If True, return matches from any column. If False, returns only rows that match all terms.\n\n        Returns:\n            pd.DataFrame: Filtered dataframe with a `Matched` column (True/False) and optionally match columns per term.\n\n        Example:\n            ```python\n            pdata.search_annotations(\"keratin\")\n            pdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n            ```\n        \"\"\"\n        import pandas as pd\n\n        adata = self.prot if on == \"protein\" else self.pep\n        df = adata.var.copy()\n\n        if search_columns is None:\n            search_columns = [\n                \"Accession\", \"Description\", \"Biological Process\", \"Cellular Component\",\n                \"Molecular Function\", \"Genes\", \"Gene ID\", \"Reactome Pathways\"\n            ]\n\n        # Ensure index is available as a searchable column\n        df = df.copy()\n        df[\"Accession\"] = df.index.astype(str)\n\n        # Convert query to list\n        if isinstance(query, str):\n            query = [query]\n\n        # Search logic\n        def match_func(val, term):\n            if pd.isnull(val):\n                return False\n            return term in val if case else term.lower() in str(val).lower()\n\n        match_results = pd.DataFrame(index=df.index)\n\n        for term in query:\n            per_col_match = pd.DataFrame({\n                col: df[col].apply(match_func, args=(term,)) if col in df.columns else False\n                for col in search_columns\n            })\n            row_match = per_col_match.any(axis=1)\n            match_results[f\"Matched_{term}\"] = row_match\n\n        if return_all_matches:\n            matched_any = match_results.any(axis=1)\n        else:\n            matched_any = match_results.all(axis=1)\n\n        result_df = df.copy()\n        result_df[\"Matched\"] = matched_any\n        for col in match_results.columns:\n            result_df[col] = match_results[col]\n\n        return result_df[result_df[\"Matched\"]]\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.get_identifier_maps","title":"get_identifier_maps","text":"<pre><code>get_identifier_maps(on='protein')\n</code></pre> <p>Retrieve gene/accession or peptide/protein mapping dictionaries.</p> <p>Depending on the <code>on</code> argument, returns a tuple of forward and reverse mappings:</p> <ul> <li> <p>If <code>on='protein'</code>: (gene \u2192 accession, accession \u2192 gene)</p> </li> <li> <p>If <code>on='peptide'</code>: (protein accession \u2192 peptide, peptide \u2192 protein accession)</p> </li> </ul> <p>Note: Alias <code>get_gene_maps()</code> also calls this function for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Source of mapping. Must be <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <p>Returns:</p> Type Description <p>Tuple[dict, dict]: (forward mapping, reverse mapping)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>on</code> is not <code>'protein'</code> or <code>'peptide'</code>.</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def get_identifier_maps(self, on='protein'):\n    \"\"\"\n    Retrieve gene/accession or peptide/protein mapping dictionaries.\n\n    Depending on the `on` argument, returns a tuple of forward and reverse mappings:\n\n    - If `on='protein'`: (gene \u2192 accession, accession \u2192 gene)\n\n    - If `on='peptide'`: (protein accession \u2192 peptide, peptide \u2192 protein accession)\n\n    Note: Alias `get_gene_maps()` also calls this function for compatibility.\n\n    Args:\n        on (str): Source of mapping. Must be `'protein'` or `'peptide'`.\n\n    Returns:\n        Tuple[dict, dict]: (forward mapping, reverse mapping)\n\n    Raises:\n        ValueError: If `on` is not `'protein'` or `'peptide'`.\n    \"\"\"\n    if on in ('protein','prot'):\n        return self._cached_identifier_maps_protein\n    elif on in ('peptide','pep'):\n        return self._cached_identifier_maps_peptide\n    else:\n        raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.refresh_identifier_maps","title":"refresh_identifier_maps","text":"<pre><code>refresh_identifier_maps()\n</code></pre> <p>Clear cached identifier maps to force regeneration on next access.</p> <p>This removes the following attributes if present:</p> <ul> <li><code>_gene_maps_protein</code>: Gene \u2194 Accession map for proteins</li> <li><code>_protein_maps_peptide</code>: Protein \u2194 Peptide map for peptides</li> </ul> <p>Useful when <code>.var</code> annotations are updated and identifier mappings may have changed.</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def refresh_identifier_maps(self):\n    \"\"\"\n    Clear cached identifier maps to force regeneration on next access.\n\n    This removes the following attributes if present:\n\n    - `_gene_maps_protein`: Gene \u2194 Accession map for proteins\n    - `_protein_maps_peptide`: Protein \u2194 Peptide map for peptides\n\n    Useful when `.var` annotations are updated and identifier mappings may have changed.\n    \"\"\"\n    for attr in [\"_gene_maps_protein\", \"_protein_maps_peptide\"]:\n        if hasattr(self, attr):\n            delattr(self, attr)\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.search_annotations","title":"search_annotations","text":"<pre><code>search_annotations(query, on='protein', search_columns=None, case=False, return_all_matches=True)\n</code></pre> <p>Search protein or peptide annotations for matching biological terms.</p> <p>This function scans <code>.prot.var</code> or <code>.pep.var</code> for entries containing the provided keyword(s), across common annotation fields.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str or list of str</code> <p>Term(s) to search for (e.g., \"keratin\", \"KRT\").</p> required <code>on</code> <code>str</code> <p>Whether to search <code>\"protein\"</code> or <code>\"peptide\"</code> annotations (default: <code>\"protein\"</code>).</p> <code>'protein'</code> <code>search_columns</code> <code>list of str</code> <p>Columns to search in. Defaults to common biological fields.</p> <code>None</code> <code>case</code> <code>bool</code> <p>Case-sensitive search (default: False).</p> <code>False</code> <code>return_all_matches</code> <code>bool</code> <p>If True, return matches from any column. If False, returns only rows that match all terms.</p> <code>True</code> <p>Returns:</p> Type Description <p>pd.DataFrame: Filtered dataframe with a <code>Matched</code> column (True/False) and optionally match columns per term.</p> Example <pre><code>pdata.search_annotations(\"keratin\")\npdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n</code></pre> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def search_annotations(self, query, on='protein', search_columns=None, case=False, return_all_matches=True):\n    \"\"\"\n    Search protein or peptide annotations for matching biological terms.\n\n    This function scans `.prot.var` or `.pep.var` for entries containing the provided keyword(s),\n    across common annotation fields.\n\n    Args:\n        query (str or list of str): Term(s) to search for (e.g., \"keratin\", \"KRT\").\n        on (str): Whether to search `\"protein\"` or `\"peptide\"` annotations (default: `\"protein\"`).\n        search_columns (list of str, optional): Columns to search in. Defaults to common biological fields.\n        case (bool): Case-sensitive search (default: False).\n        return_all_matches (bool): If True, return matches from any column. If False, returns only rows that match all terms.\n\n    Returns:\n        pd.DataFrame: Filtered dataframe with a `Matched` column (True/False) and optionally match columns per term.\n\n    Example:\n        ```python\n        pdata.search_annotations(\"keratin\")\n        pdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n        ```\n    \"\"\"\n    import pandas as pd\n\n    adata = self.prot if on == \"protein\" else self.pep\n    df = adata.var.copy()\n\n    if search_columns is None:\n        search_columns = [\n            \"Accession\", \"Description\", \"Biological Process\", \"Cellular Component\",\n            \"Molecular Function\", \"Genes\", \"Gene ID\", \"Reactome Pathways\"\n        ]\n\n    # Ensure index is available as a searchable column\n    df = df.copy()\n    df[\"Accession\"] = df.index.astype(str)\n\n    # Convert query to list\n    if isinstance(query, str):\n        query = [query]\n\n    # Search logic\n    def match_func(val, term):\n        if pd.isnull(val):\n            return False\n        return term in val if case else term.lower() in str(val).lower()\n\n    match_results = pd.DataFrame(index=df.index)\n\n    for term in query:\n        per_col_match = pd.DataFrame({\n            col: df[col].apply(match_func, args=(term,)) if col in df.columns else False\n            for col in search_columns\n        })\n        row_match = per_col_match.any(axis=1)\n        match_results[f\"Matched_{term}\"] = row_match\n\n    if return_all_matches:\n        matched_any = match_results.any(axis=1)\n    else:\n        matched_any = match_results.all(axis=1)\n\n    result_df = df.copy()\n    result_df[\"Matched\"] = matched_any\n    for col in match_results.columns:\n        result_df[col] = match_results[col]\n\n    return result_df[result_df[\"Matched\"]]\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.update_identifier_maps","title":"update_identifier_maps","text":"<pre><code>update_identifier_maps(mapping, on='protein', direction='forward', overwrite=False, verbose=True)\n</code></pre> <p>Update cached identifier maps with user-supplied mappings.</p> <p>This function updates the internal forward and reverse identifier maps for either proteins or peptides. Ensures consistency by updating both directions of the mapping.</p> <ul> <li> <p>For <code>'protein'</code>:</p> <ul> <li>forward: gene \u2192 accession  </li> <li>reverse: accession \u2192 gene</li> </ul> </li> <li> <p>For <code>'peptide'</code>:</p> <ul> <li>forward: protein accession \u2192 peptide</li> <li>reverse: peptide \u2192 protein accession</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>dict</code> <p>Dictionary of mappings to add.</p> required <code>on</code> <code>str</code> <p>Which maps to update. Must be <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <code>direction</code> <code>str</code> <p><code>'forward'</code> or <code>'reverse'</code> \u2014 determines how the <code>mapping</code> should be interpreted.</p> <code>'forward'</code> <code>overwrite</code> <code>bool</code> <p>If True, allows overwriting existing entries.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, prints a summary of updated keys.</p> <code>True</code> Note <p>The corresponding reverse map is automatically updated to maintain bidirectional consistency.</p> Example <p>Add new gene-to-accession mappings (protein):     <pre><code>pdata.update_identifier_maps(\n    {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n    on='protein',\n    direction='forward'\n)\n</code></pre></p> <p>Add peptide \u2192 protein mappings:     <pre><code>pdata.update_identifier_maps(\n    {'PEPTIDE_ABC': 'P12345'},\n    on='peptide',\n    direction='reverse'\n)\n</code></pre></p> <p>Overwrite a protein \u2192 gene mapping:     <pre><code>pdata.update_identifier_maps(\n    {'P12345': 'NEWGENE'},\n    on='protein',\n    direction='reverse',\n    overwrite=True\n)\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def update_identifier_maps(self, mapping, on='protein', direction='forward', overwrite=False, verbose=True):\n    \"\"\"\n    Update cached identifier maps with user-supplied mappings.\n\n    This function updates the internal forward and reverse identifier maps\n    for either proteins or peptides. Ensures consistency by updating both\n    directions of the mapping.\n\n    - For `'protein'`:\n        * forward: gene \u2192 accession  \n        * reverse: accession \u2192 gene\n\n    - For `'peptide'`:\n        * forward: protein accession \u2192 peptide\n        * reverse: peptide \u2192 protein accession\n\n    Args:\n        mapping (dict): Dictionary of mappings to add.\n        on (str): Which maps to update. Must be `'protein'` or `'peptide'`.\n        direction (str): `'forward'` or `'reverse'` \u2014 determines how the `mapping` should be interpreted.\n        overwrite (bool): If True, allows overwriting existing entries.\n        verbose (bool): If True, prints a summary of updated keys.\n\n    Note:\n        The corresponding reverse map is automatically updated to maintain bidirectional consistency.\n\n    Example:\n        Add new gene-to-accession mappings (protein):\n            ```python\n            pdata.update_identifier_maps(\n                {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n                on='protein',\n                direction='forward'\n            )\n            ```\n\n        Add peptide \u2192 protein mappings:\n            ```python\n            pdata.update_identifier_maps(\n                {'PEPTIDE_ABC': 'P12345'},\n                on='peptide',\n                direction='reverse'\n            )\n            ```\n\n        Overwrite a protein \u2192 gene mapping:\n            ```python\n            pdata.update_identifier_maps(\n                {'P12345': 'NEWGENE'},\n                on='protein',\n                direction='reverse',\n                overwrite=True\n            )\n            ```\n\n    \"\"\"\n    if on == 'protein':\n        forward, reverse = self._cached_identifier_maps_protein\n    elif on == 'peptide':\n        forward, reverse = self._cached_identifier_maps_peptide\n    else:\n        raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n    source_map = forward if direction == 'forward' else reverse\n    target_map = reverse if direction == 'forward' else forward\n\n    added, updated, skipped = 0, 0, 0\n\n    for key, val in mapping.items():\n        if key in source_map:\n            if overwrite:\n                source_map[key] = val\n                target_map[val] = key\n                updated += 1\n            else:\n                skipped += 1\n        else:\n            source_map[key] = val\n            target_map[val] = key\n            added += 1\n\n    message = (\n        f\"[update_identifier_maps] Updated '{on}' ({direction}): \"\n        f\"{added} added, {updated} overwritten, {skipped} skipped.\"\n    )\n\n    if verbose:\n        print(message)\n    self._append_history(message)\n\n    # Update .prot.var[\"Genes\"] if updating protein identifier reverse map (accession \u2192 gene)\n    if on == 'protein' and direction == 'reverse':\n        updated_var_count = 0\n        updated_accessions = []\n\n        for acc, gene in mapping.items():\n            if acc in self.prot.var_names:\n                self.prot.var.at[acc, \"Genes\"] = gene\n                updated_accessions.append(acc)\n                updated_var_count += 1\n\n        if updated_var_count &gt; 0:\n            var_message = (\n                f\"\ud83d\udd01 Updated `.prot.var['Genes']` for {updated_var_count} entries from custom mapping. \"\n                f\"(View details in `pdata.metadata['identifier_map_history']`)\"\n            )\n            if verbose:\n                print(var_message)\n            self._append_history(var_message)\n\n    # Log detailed update history for all cases\n    import datetime\n\n    record = {\n        'on': on,\n        'direction': direction,\n        'input_mapping': dict(mapping),  # shallow copy\n        'overwrite': overwrite,\n        'timestamp': datetime.datetime.now().isoformat(timespec='seconds'),\n        'summary': {\n            'added': added,\n            'updated': updated,\n            'skipped': skipped,\n        }\n    }\n\n    if on == 'protein' and direction == 'reverse':\n        record['updated_var_column'] = {\n            'column': 'Genes',\n            'accessions': updated_accessions,\n            'n_updated': updated_var_count\n        }\n\n    self.metadata.setdefault(\"identifier_map_history\", []).append(record)\n</code></pre>"},{"location":"reference/pAnnData/core_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.update_missing_genes","title":"update_missing_genes","text":"<pre><code>update_missing_genes(gene_col='Genes', verbose=True)\n</code></pre> <p>Fill missing gene names in <code>.prot.var</code> using UniProt API.</p> <p>This function searches for missing values in the specified gene column and attempts to fill them by querying the UniProt API using protein accession IDs. If a gene name cannot be found, a placeholder 'UNKNOWN_' is used instead. <p>Parameters:</p> Name Type Description Default <code>gene_col</code> <code>str</code> <p>Column name in <code>.prot.var</code> to update (default: \"Genes\").</p> <code>'Genes'</code> <code>verbose</code> <code>bool</code> <p>Whether to print summary information (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Note <ul> <li>This function only operates on <code>.prot.var</code>, not <code>.pep.var</code>.</li> <li>If UniProt is unavailable or returns no match, the missing entry is filled as <code>'UNKNOWN_&lt;accession&gt;'</code>.</li> <li>To manually correct unknown entries later, use <code>update_identifier_maps()</code> with <code>direction='reverse'</code>.</li> </ul> Example <p>Automatically fill missing gene names using UniProt:     <pre><code>pdata.update_missing_genes()\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def update_missing_genes(self, gene_col=\"Genes\", verbose=True):\n    \"\"\"\n    Fill missing gene names in `.prot.var` using UniProt API.\n\n    This function searches for missing values in the specified gene column\n    and attempts to fill them by querying the UniProt API using protein\n    accession IDs. If a gene name cannot be found, a placeholder\n    'UNKNOWN_&lt;accession&gt;' is used instead.\n\n    Args:\n        gene_col (str): Column name in `.prot.var` to update (default: \"Genes\").\n        verbose (bool): Whether to print summary information (default: True).\n\n    Returns:\n        None\n\n    Note:\n        - This function only operates on `.prot.var`, not `.pep.var`.\n        - If UniProt is unavailable or returns no match, the missing entry is filled as `'UNKNOWN_&lt;accession&gt;'`.\n        - To manually correct unknown entries later, use `update_identifier_maps()` with `direction='reverse'`.\n\n    Example:\n        Automatically fill missing gene names using UniProt:\n            ```python\n            pdata.update_missing_genes()\n            ```\n    \"\"\"\n    var = self.prot.var\n\n    if gene_col not in var.columns:\n        if verbose:\n            print(f\"{format_log_prefix('warn')} Column '{gene_col}' not found in .prot.var.\")\n        return\n\n    missing_mask = var[gene_col].isna()\n    if not missing_mask.any():\n        if verbose:\n            print(f\"{format_log_prefix('result')} No missing gene names found.\")\n        return\n\n    accessions = var.index[missing_mask].tolist()\n    if verbose:\n        print(f\"{format_log_prefix('info_only')} {len(accessions)} proteins with missing gene names.\")\n\n    try:\n        df = utils.get_uniprot_fields(\n            accessions,\n            search_fields=[\"accession\", \"gene_primary\"],\n            standardize=True\n        )\n    except Exception as e:\n        print(f\"{format_log_prefix('error')} UniProt query failed: {e}\")\n        return\n    df = utils.standardize_uniprot_columns(df)\n\n    if df.empty or \"accession\" not in df.columns or \"gene_primary\" not in df.columns:\n        print(f\"{format_log_prefix('warn')} UniProt returned no usable gene mapping columns.\")\n        return\n\n    gene_map = dict(zip(df[\"accession\"], df[\"gene_primary\"]))\n    filled = self.prot.var.loc[missing_mask].index.map(lambda acc: gene_map.get(acc))\n    final_genes = [\n        gene if pd.notna(gene) else f\"UNKNOWN_{acc}\"\n        for acc, gene in zip(self.prot.var.loc[missing_mask].index, filled)\n    ]\n    self.prot.var.loc[missing_mask, gene_col] = final_genes\n\n    found = sum(pd.notna(filled))\n    unknown = len(final_genes) - found\n    if verbose:\n        if found:\n            print(f\"{format_log_prefix('result')} Recovered {found} gene name(s) from UniProt. Genes found:\")\n            filled_clean = [str(g) for g in filled if pd.notna(g)]\n            preview = \", \".join(filled_clean[:10])\n            if found &gt; 10:\n                preview += \"...\"\n            print(\"        \", preview)\n        if unknown:\n            missing_ids = self.prot.var.loc[missing_mask].index[pd.isna(filled)]\n            print(f\"{format_log_prefix('warn')} {unknown} gene name(s) still missing. Assigned as 'UNKNOWN_&lt;accession&gt;' for:\")\n            print(\"        \", \", \".join(missing_ids[:5]) + (\"...\" if unknown &gt; 10 else \"\"))\n            print(\"     \ud83d\udca1 Tip: You can update these using `pdata.update_identifier_maps({'GENE': 'ACCESSION'}, on='protein', direction='reverse', overwrite=True)`\\n\")\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/","title":"Editing &amp; Filtering","text":"<p>Mixins for manipulating or reducing the dataset through selection and exports.</p>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing.EditingMixin","title":"EditingMixin","text":"<p>Provides utilities for modifying core components of a <code>pAnnData</code> object, including  matrix layers, abundance formatting, exports, and the protein\u2013peptide mapping.</p> <p>This mixin includes utilities for:</p> <ul> <li>Replacing <code>.X</code> with a specific layer from protein or peptide data.</li> <li>Extracting long-form abundance DataFrames with metadata for plotting or analysis.</li> <li>Exporting internal data (summary, matrix layers) to disk.</li> <li>Setting or updating the RS (protein \u00d7 peptide) relational mapping matrix.</li> </ul> <p>Methods:</p> Name Description <code>set_X</code> <p>Sets the <code>.X</code> matrix of protein or peptide data to a specified layer.</p> <code>get_abundance</code> <p>Returns long-form abundance + metadata for selected features.</p> <code>export</code> <p>Exports summary, matrix values, and layers to CSV.</p> <code>_set_RS</code> <p>Sets the RS (protein \u00d7 peptide) mapping matrix, with optional validation.</p> Source code in <code>src/scpviz/pAnnData/editing.py</code> <pre><code>class EditingMixin:\n    \"\"\"\n    Provides utilities for modifying core components of a `pAnnData` object, including \n    matrix layers, abundance formatting, exports, and the protein\u2013peptide mapping.\n\n    This mixin includes utilities for:\n\n    - Replacing `.X` with a specific layer from protein or peptide data.\n    - Extracting long-form abundance DataFrames with metadata for plotting or analysis.\n    - Exporting internal data (summary, matrix layers) to disk.\n    - Setting or updating the RS (protein \u00d7 peptide) relational mapping matrix.\n\n    Functions:\n        set_X: Sets the `.X` matrix of protein or peptide data to a specified layer.\n        get_abundance: Returns long-form abundance + metadata for selected features.\n        export: Exports summary, matrix values, and layers to CSV.\n        _set_RS: Sets the RS (protein \u00d7 peptide) mapping matrix, with optional validation.\n    \"\"\"\n\n    def set_X(self, layer, on = 'protein'):\n        \"\"\"\n        Set the `.X` matrix of protein or peptide data to a specified layer.\n\n        This method replaces the active `.X` matrix with the contents of a named layer \n        from `.prot.layers` or `.pep.layers`. This is useful for switching between \n        different processing stages (e.g., normalized, imputed, or raw data).\n\n        Args:\n            layer (str): Name of the data layer to assign to `.X`.\n            on (str): Whether to operate on `\"protein\"` or `\"peptide\"` data (default is `\"protein\"`).\n\n        Returns:\n            None\n\n        Example:\n            Set the protein matrix `.X` to the \"normalized\" layer:\n                ```python\n                pdata.set_X(layer=\"normalized\", on=\"protein\")\n                ```\n\n            Set the peptide matrix `.X` to the \"imputed\" layer:\n                ```python\n                pdata.set_X(layer=\"imputed\", on=\"peptide\")\n                ```\n        \"\"\"\n        # defines which layer to set X to\n        if not self._check_data(on): # type: ignore[attr-defined]\n            pass\n\n        if on == 'protein':\n            if layer not in self.prot.layers: # type: ignore[attr-defined]\n                raise ValueError(f\"Layer {layer} not found in protein data.\")\n            self.prot.X = self.prot.layers[layer] # type: ignore[attr-defined]\n            print(f\"{format_log_prefix('info_only', indent=2)} Set {on} data to layer {layer}.\")\n\n        else:\n            if layer not in self.pep.layers: # type: ignore[attr-defined]\n                raise ValueError(f\"Layer {layer} not found in peptide data.\")\n            self.pep.X = self.pep.layers[layer] # type: ignore[attr-defined]\n            print(f\"{format_log_prefix('info_only', indent=2)} Set {on} data (.X) to layer {layer}.\")\n\n        self._history.append(f\"{on}: Set X to layer {layer}.\") # type: ignore[attr-defined]\n\n    def get_abundance(self, namelist=None, layer='X', on='protein', classes=None, log=True, x_label='gene'):\n        \"\"\"\n        Extract a long-form abundance DataFrame from a pAnnData object.\n\n        This method returns a melted (long-form) DataFrame containing abundance values\n        along with optional sample metadata and protein/peptide annotations.\n\n        Args:\n            namelist (list of str, optional): List of accessions or gene names to extract. If None, returns all features.\n            layer (str): Name of the data layer to use (default is \"X\").\n            on (str): Whether to extract from \"protein\" or \"peptide\" data.\n            classes (str or list of str, optional): Sample-level `.obs` column(s) to include for grouping or plotting.\n            log (bool): If True, applies log2 transform to abundance values.\n            x_label (str): Whether to label features by \"gene\" or \"accession\" in the output.\n\n        Returns:\n            pd.DataFrame: Long-form DataFrame with abundance values and associated metadata.\n\n        Example:\n            Extract abundance values for selected proteins, grouped by sample-level metadata:\n                ```python\n                df_abund = pdata.get_abundance(\n                    namelist=[\"UBE4B\", \"GAPDH\"],\n                    on=\"protein\",\n                    classes=[\"treatment\", \"cellline\"]\n                )\n                ```\n\n        Note:\n            This method is also available as a utility function in utils, for `AnnData` or `pAnnData` objects:\n                ```python\n                from scutils import get_abundance\n                df_abund = get_abundance(pdata, namelist=[\"UBE4B\", \"GAPDH\"], on=\"protein\", classes=[\"treatment\", \"cellline\"])\n                ```\n        \"\"\"\n        on_user = on.lower()\n        gene_to_acc, _ = self.get_gene_maps(on='protein' if on_user in ('peptide', 'pep') else on_user) # type: ignore[attr-defined]\n\n\n        if on == 'peptide' and namelist:\n            pep_names = self.pep.var_names.astype(str) # type: ignore[attr-defined]\n            matched_peptides = [name for name in namelist if name in pep_names]\n            non_peptides = [name for name in namelist if name not in matched_peptides]\n\n            adata = None\n            if len(matched_peptides) &lt; len(namelist):\n                filtered = self.filter_prot(accessions=non_peptides, return_copy=True) # type: ignore[attr-defined]\n                adata = filtered.pep\n\n            if matched_peptides:\n                direct_peps = self.pep[:, matched_peptides] # type: ignore[attr-defined]\n                adata = direct_peps if adata is None else adata.concatenate(direct_peps, join='outer')\n\n            if adata is None or adata.n_vars == 0:\n                raise ValueError(\"No matching peptides found from the provided `namelist`.\")\n\n            adata = adata[:, ~adata.var_names.duplicated()]\n\n        else:\n            adata = utils.get_adata(self, on)\n\n            if namelist:\n                resolved = utils.resolve_accessions(adata, namelist, gene_map=gene_to_acc)\n                adata = adata[:, resolved]\n\n        # Extract the abundance matrix\n        X = adata.layers[layer] if layer in adata.layers else adata.X\n        if hasattr(X, \"toarray\"):\n            X = X.toarray()\n\n        # Melt into long form\n        df = pd.DataFrame(X, columns=adata.var_names, index=adata.obs_names).reset_index()\n        df = df.melt(id_vars=\"index\", var_name=\"accession\", value_name=\"abundance\")\n        df = df.rename(columns={\"index\": \"cell\"})\n\n        # Merge obs metadata\n        df = df.merge(adata.obs.reset_index(), left_on=\"cell\", right_on=\"index\")\n\n        _, pep_to_prot = self.get_gene_maps(on='peptide')  # peptide \u2192 protein map, # type: ignore[attr-defined]\n        _, acc_to_gene = self.get_gene_maps(on='protein')  # protein accession \u2192 gene, # type: ignore[attr-defined]\n        # Map to gene names\n        if on == 'peptide':\n            try:\n                df['protein_accession'] = df['accession'].map(pep_to_prot)\n                df['gene'] = df['protein_accession'].map(acc_to_gene)\n\n                # Report unmapped peptides\n                unmapped = df[df['gene'].isna()]['accession'].unique().tolist()\n                if unmapped:\n                    print(f\"[get_abundance] {len(unmapped)} peptides could not be mapped to genes: {unmapped}\")\n            except Exception as e:\n                print(f\"[get_abundance] Mapping error: {e}\")\n                df['gene'] = None\n        else:\n            df['gene'] = df['accession'].map(acc_to_gene)\n\n        # Determine x-axis label\n        if x_label == 'gene':\n            df['x_label_name'] = df['gene'].fillna(df['accession'])\n        elif x_label == 'accession':\n            if on == 'protein':\n                df['x_label_name'] = df['accession']\n            elif on == 'peptide':\n                try:\n                    mapping_col = utils.get_pep_prot_mapping(self)\n                    pep_to_prot = self.pep.var[mapping_col].to_dict() # type: ignore[attr-defined]\n                    df['x_label_name'] = df['protein_accession']\n                except Exception as e:\n                    warnings.warn(f\"Could not map peptides to accessions: {e}\")\n                    df['x_label_name'] = df['accession']\n        else:\n            df['x_label_name'] = df['accession']  # fallback\n\n        # Annotate class/grouping\n        if classes:\n            df['class'] = df[classes] if isinstance(classes, str) else df[classes].astype(str).agg('_'.join, axis=1)\n        else:\n            df['class'] = 'all'\n\n        # Log transform\n        if log:\n            df['log2_abundance'] = np.log2(np.clip(df['abundance'], 1e-6, None))\n\n        return df\n\n    def export(self, filename, format = 'csv', verbose = True):\n        \"\"\"\n        Export the pAnnData object's contents to file, including layers and summary metadata.\n\n        This method saves the summary table, protein matrix, and all data layers as separate \n        CSV files using the specified filename as a prefix.\n\n        Args:\n            filename (str): Prefix for exported files. If None, uses the current date and time.\n            format (str): File format to export (default is \"csv\").\n            verbose (bool): Whether to print progress messages.\n\n        Returns:\n            None\n\n        Todo:\n            Add example usage showing how to export data and where files are saved. (HDF5, Parquet?)\n        \"\"\"\n        if filename is None:\n            filename = setup.get_datetime()\n\n        if not self._has_data():\n            raise ValueError(\"No data found in pAnnData object.\")\n\n        if verbose:\n            print(f\"{format_log_prefix('user')} Exporting pAnnData to &lt;{filename}&gt;...\")\n\n        # --- Summary ---\n        self.summary.to_csv(f\"{filename}_summary.csv\")\n        if verbose:\n            print(f\"{format_log_prefix('result_only',2)} Exported summary table \u2192 {filename}_summary.csv\")\n\n        # --- Protein matrix ---\n        if self.prot is not None:\n            self.prot.to_df().to_csv(f\"{filename}_protein.csv\")\n            if verbose:\n                print(f\"{format_log_prefix('result_only',2)} Exported protein matrix \u2192 {filename}_protein.csv\")\n\n            for layer in self.prot.layers:\n                arr = self.prot.layers[layer]\n                if hasattr(arr, 'toarray'):\n                    arr = arr.toarray()\n                df = pd.DataFrame(arr, index=self.prot.obs_names, columns=self.prot.var_names)\n                df.to_csv(f\"{filename}_protein_{layer}.csv\")\n                if verbose:\n                    print(f\"{format_log_prefix('result_only',2)} Exported protein layer '{layer}' \u2192 {filename}_protein_{layer}.csv\")\n\n        # --- Peptide matrix ---\n        if self.pep is not None:\n            self.pep.to_df().to_csv(f\"{filename}_peptide.csv\")\n            if verbose:\n                print(f\"{format_log_prefix('result_only',2)} Exported peptide matrix \u2192 {filename}_peptide.csv\")\n\n            for layer in self.pep.layers:\n                arr = self.pep.layers[layer]\n                if hasattr(arr, 'toarray'):\n                    arr = arr.toarray()\n                df = pd.DataFrame(arr, index=self.pep.obs_names, columns=self.pep.var_names)\n                df.to_csv(f\"{filename}_peptide_{layer}.csv\")\n                if verbose:\n                    print(f\"{format_log_prefix('result_only',2)} Exported peptide layer '{layer}' \u2192 {filename}_peptide_{layer}.csv\")\n\n    def export_layer(self, layer_name, filename=None, on='protein', obs_names=None, var_names=None, transpose=False):\n        \"\"\"\n        Export a specified layer from the protein or peptide data to CSV with labeled rows and columns.\n\n        Args:\n            layer_name (str): Name of the layer to export (e.g., \"X_raw\"). If \"X\" is provided, exports `pdata.X`\n            filename (str, optional): Output file name. Defaults to \"&lt;layer_name&gt;.csv\".\n            on (str): One of 'protein' or 'peptide' to specify which data to use.\n            obs_names (str or None): If a string, the column name in .obs to use for row labels.\n            var_names (str or None): If a string, the column name in .var to use for column labels.\n            transpose: If True, then export as proteins/peptides (rows) by samples (columns)\n\n        Returns:\n            None\n        \"\"\"\n        # Select the appropriate AnnData object\n        adata = self.prot if on == 'protein' else self.pep\n        if layer_name == \"X\":\n            layer=adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n        else:\n            layer = adata.layers[layer_name]\n\n        # Convert to dense array if needed\n        if not isinstance(layer, pd.DataFrame):\n            layer = layer.toarray() if hasattr(layer, 'toarray') else layer\n\n        # Get row (obs) and column (var) labels\n        row_labels = adata.obs[obs_names] if obs_names else adata.obs_names\n        col_labels = adata.var[var_names] if var_names else adata.var_names\n\n        # Build the DataFrame\n        if transpose:\n            df = pd.DataFrame(layer.T, columns=row_labels, index=col_labels)\n        else:\n            df = pd.DataFrame(layer, index=row_labels, columns=col_labels)\n\n        # Save to CSV\n        if filename is None:\n            filename = f\"{layer_name}.csv\"\n        df.to_csv(filename)\n\n\n    def export_morpheus(self, filename='pdata', on='protein'):\n        if not self._check_data(on):  # type: ignore[attr-defined], ValidationMixin\n            return\n\n        adata = self.prot if on == 'protein' else self.pep\n\n        # alternatively, use morpheus to plot clustermap\n        # will need two things\n        # 1. dataset (proteins in column, samples in rows)\n        dense_matrix = adata.X.toarray()\n        df = pd.DataFrame(dense_matrix, index=adata.obs_names, columns=adata.var_names)\n        df.to_csv(f'{filename}_protein_matrix.csv')\n        # 2. File Annotations (each sample in a row, different annotations in columns)\n        adata.obs.to_csv(f'{filename}_protein_annotations.csv')\n        # 3. Protein Annotations (each protein in a row, different annotations in columns)\n        adata.var.to_csv(f'{filename}_protein_annotations.csv')\n\n        print(f\"{format_log_prefix('result')} Morpheus export complete.\")\n\n    def _set_RS(self, rs, debug=False, validate=True):\n        \"\"\"\n        Set the RS (protein \u00d7 peptide) mapping matrix.\n\n        This internal method assigns a new RS matrix to the object. If the input appears \n        to be in peptide \u00d7 protein format, it will be automatically transposed.\n\n        Args:\n            rs (np.ndarray or sparse matrix): The new RS matrix to assign.\n            debug (bool): If True, prints diagnostic information.\n            validate (bool): If True (default), checks that the RS shape matches `.prot` and `.pep`.\n\n        Returns:\n            None\n        \"\"\"\n        if debug:\n            print(f\"Setting rs matrix with dimensions {rs.shape}\")\n\n        # Only validate if requested (e.g. for external .rs = ... use)\n        if validate:\n            prot_n = self.prot.shape[1] if self.prot is not None else None\n            pep_n = self.pep.shape[1] if self.pep is not None else None\n            rs_shape = rs.shape\n\n            valid_prot_pep = (prot_n is None or rs_shape[0] == prot_n) and (pep_n is None or rs_shape[1] == pep_n)\n            valid_pep_prot = (prot_n is None or rs_shape[1] == prot_n) and (pep_n is None or rs_shape[0] == pep_n)\n\n            if not (valid_prot_pep or valid_pep_prot):\n                raise ValueError(\n                    f\"\u274c RS shape {rs_shape} does not match expected protein \u00d7 peptide \"\n                    f\"({prot_n} \u00d7 {pep_n}) or peptide \u00d7 protein ({pep_n} \u00d7 {prot_n}).\"\n                )\n\n            # Transpose if necessary\n            if self.prot is not None and rs_shape[0] != prot_n:\n                if debug:\n                    print(\"\u2194\ufe0f  Transposing RS matrix to match protein \u00d7 peptide format\")\n                rs = rs.T\n\n        # Always store as sparse\n        self._rs = sparse.csr_matrix(rs)\n\n        if debug:\n            nnz = self._rs.nnz\n            total = self._rs.shape[0] * self._rs.shape[1]\n            sparsity = 100 * (1 - nnz / total)\n            print(f\"{format_log_prefix('result',indent=1)} RS matrix set: {self._rs.shape} (proteins \u00d7 peptides), sparsity: {sparsity:.2f}%\")\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing.EditingMixin.export","title":"export","text":"<pre><code>export(filename, format='csv', verbose=True)\n</code></pre> <p>Export the pAnnData object's contents to file, including layers and summary metadata.</p> <p>This method saves the summary table, protein matrix, and all data layers as separate  CSV files using the specified filename as a prefix.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Prefix for exported files. If None, uses the current date and time.</p> required <code>format</code> <code>str</code> <p>File format to export (default is \"csv\").</p> <code>'csv'</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress messages.</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Todo <p>Add example usage showing how to export data and where files are saved. (HDF5, Parquet?)</p> Source code in <code>src/scpviz/pAnnData/editing.py</code> <pre><code>def export(self, filename, format = 'csv', verbose = True):\n    \"\"\"\n    Export the pAnnData object's contents to file, including layers and summary metadata.\n\n    This method saves the summary table, protein matrix, and all data layers as separate \n    CSV files using the specified filename as a prefix.\n\n    Args:\n        filename (str): Prefix for exported files. If None, uses the current date and time.\n        format (str): File format to export (default is \"csv\").\n        verbose (bool): Whether to print progress messages.\n\n    Returns:\n        None\n\n    Todo:\n        Add example usage showing how to export data and where files are saved. (HDF5, Parquet?)\n    \"\"\"\n    if filename is None:\n        filename = setup.get_datetime()\n\n    if not self._has_data():\n        raise ValueError(\"No data found in pAnnData object.\")\n\n    if verbose:\n        print(f\"{format_log_prefix('user')} Exporting pAnnData to &lt;{filename}&gt;...\")\n\n    # --- Summary ---\n    self.summary.to_csv(f\"{filename}_summary.csv\")\n    if verbose:\n        print(f\"{format_log_prefix('result_only',2)} Exported summary table \u2192 {filename}_summary.csv\")\n\n    # --- Protein matrix ---\n    if self.prot is not None:\n        self.prot.to_df().to_csv(f\"{filename}_protein.csv\")\n        if verbose:\n            print(f\"{format_log_prefix('result_only',2)} Exported protein matrix \u2192 {filename}_protein.csv\")\n\n        for layer in self.prot.layers:\n            arr = self.prot.layers[layer]\n            if hasattr(arr, 'toarray'):\n                arr = arr.toarray()\n            df = pd.DataFrame(arr, index=self.prot.obs_names, columns=self.prot.var_names)\n            df.to_csv(f\"{filename}_protein_{layer}.csv\")\n            if verbose:\n                print(f\"{format_log_prefix('result_only',2)} Exported protein layer '{layer}' \u2192 {filename}_protein_{layer}.csv\")\n\n    # --- Peptide matrix ---\n    if self.pep is not None:\n        self.pep.to_df().to_csv(f\"{filename}_peptide.csv\")\n        if verbose:\n            print(f\"{format_log_prefix('result_only',2)} Exported peptide matrix \u2192 {filename}_peptide.csv\")\n\n        for layer in self.pep.layers:\n            arr = self.pep.layers[layer]\n            if hasattr(arr, 'toarray'):\n                arr = arr.toarray()\n            df = pd.DataFrame(arr, index=self.pep.obs_names, columns=self.pep.var_names)\n            df.to_csv(f\"{filename}_peptide_{layer}.csv\")\n            if verbose:\n                print(f\"{format_log_prefix('result_only',2)} Exported peptide layer '{layer}' \u2192 {filename}_peptide_{layer}.csv\")\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing.EditingMixin.export_layer","title":"export_layer","text":"<pre><code>export_layer(layer_name, filename=None, on='protein', obs_names=None, var_names=None, transpose=False)\n</code></pre> <p>Export a specified layer from the protein or peptide data to CSV with labeled rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>Name of the layer to export (e.g., \"X_raw\"). If \"X\" is provided, exports <code>pdata.X</code></p> required <code>filename</code> <code>str</code> <p>Output file name. Defaults to \".csv\". <code>None</code> <code>on</code> <code>str</code> <p>One of 'protein' or 'peptide' to specify which data to use.</p> <code>'protein'</code> <code>obs_names</code> <code>str or None</code> <p>If a string, the column name in .obs to use for row labels.</p> <code>None</code> <code>var_names</code> <code>str or None</code> <p>If a string, the column name in .var to use for column labels.</p> <code>None</code> <code>transpose</code> <p>If True, then export as proteins/peptides (rows) by samples (columns)</p> <code>False</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/scpviz/pAnnData/editing.py</code> <pre><code>def export_layer(self, layer_name, filename=None, on='protein', obs_names=None, var_names=None, transpose=False):\n    \"\"\"\n    Export a specified layer from the protein or peptide data to CSV with labeled rows and columns.\n\n    Args:\n        layer_name (str): Name of the layer to export (e.g., \"X_raw\"). If \"X\" is provided, exports `pdata.X`\n        filename (str, optional): Output file name. Defaults to \"&lt;layer_name&gt;.csv\".\n        on (str): One of 'protein' or 'peptide' to specify which data to use.\n        obs_names (str or None): If a string, the column name in .obs to use for row labels.\n        var_names (str or None): If a string, the column name in .var to use for column labels.\n        transpose: If True, then export as proteins/peptides (rows) by samples (columns)\n\n    Returns:\n        None\n    \"\"\"\n    # Select the appropriate AnnData object\n    adata = self.prot if on == 'protein' else self.pep\n    if layer_name == \"X\":\n        layer=adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n    else:\n        layer = adata.layers[layer_name]\n\n    # Convert to dense array if needed\n    if not isinstance(layer, pd.DataFrame):\n        layer = layer.toarray() if hasattr(layer, 'toarray') else layer\n\n    # Get row (obs) and column (var) labels\n    row_labels = adata.obs[obs_names] if obs_names else adata.obs_names\n    col_labels = adata.var[var_names] if var_names else adata.var_names\n\n    # Build the DataFrame\n    if transpose:\n        df = pd.DataFrame(layer.T, columns=row_labels, index=col_labels)\n    else:\n        df = pd.DataFrame(layer, index=row_labels, columns=col_labels)\n\n    # Save to CSV\n    if filename is None:\n        filename = f\"{layer_name}.csv\"\n    df.to_csv(filename)\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing.EditingMixin.get_abundance","title":"get_abundance","text":"<pre><code>get_abundance(namelist=None, layer='X', on='protein', classes=None, log=True, x_label='gene')\n</code></pre> <p>Extract a long-form abundance DataFrame from a pAnnData object.</p> <p>This method returns a melted (long-form) DataFrame containing abundance values along with optional sample metadata and protein/peptide annotations.</p> <p>Parameters:</p> Name Type Description Default <code>namelist</code> <code>list of str</code> <p>List of accessions or gene names to extract. If None, returns all features.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Name of the data layer to use (default is \"X\").</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Whether to extract from \"protein\" or \"peptide\" data.</p> <code>'protein'</code> <code>classes</code> <code>str or list of str</code> <p>Sample-level <code>.obs</code> column(s) to include for grouping or plotting.</p> <code>None</code> <code>log</code> <code>bool</code> <p>If True, applies log2 transform to abundance values.</p> <code>True</code> <code>x_label</code> <code>str</code> <p>Whether to label features by \"gene\" or \"accession\" in the output.</p> <code>'gene'</code> <p>Returns:</p> Type Description <p>pd.DataFrame: Long-form DataFrame with abundance values and associated metadata.</p> Example <p>Extract abundance values for selected proteins, grouped by sample-level metadata:     <pre><code>df_abund = pdata.get_abundance(\n    namelist=[\"UBE4B\", \"GAPDH\"],\n    on=\"protein\",\n    classes=[\"treatment\", \"cellline\"]\n)\n</code></pre></p> Note <p>This method is also available as a utility function in utils, for <code>AnnData</code> or <code>pAnnData</code> objects:     <pre><code>from scutils import get_abundance\ndf_abund = get_abundance(pdata, namelist=[\"UBE4B\", \"GAPDH\"], on=\"protein\", classes=[\"treatment\", \"cellline\"])\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/editing.py</code> <pre><code>def get_abundance(self, namelist=None, layer='X', on='protein', classes=None, log=True, x_label='gene'):\n    \"\"\"\n    Extract a long-form abundance DataFrame from a pAnnData object.\n\n    This method returns a melted (long-form) DataFrame containing abundance values\n    along with optional sample metadata and protein/peptide annotations.\n\n    Args:\n        namelist (list of str, optional): List of accessions or gene names to extract. If None, returns all features.\n        layer (str): Name of the data layer to use (default is \"X\").\n        on (str): Whether to extract from \"protein\" or \"peptide\" data.\n        classes (str or list of str, optional): Sample-level `.obs` column(s) to include for grouping or plotting.\n        log (bool): If True, applies log2 transform to abundance values.\n        x_label (str): Whether to label features by \"gene\" or \"accession\" in the output.\n\n    Returns:\n        pd.DataFrame: Long-form DataFrame with abundance values and associated metadata.\n\n    Example:\n        Extract abundance values for selected proteins, grouped by sample-level metadata:\n            ```python\n            df_abund = pdata.get_abundance(\n                namelist=[\"UBE4B\", \"GAPDH\"],\n                on=\"protein\",\n                classes=[\"treatment\", \"cellline\"]\n            )\n            ```\n\n    Note:\n        This method is also available as a utility function in utils, for `AnnData` or `pAnnData` objects:\n            ```python\n            from scutils import get_abundance\n            df_abund = get_abundance(pdata, namelist=[\"UBE4B\", \"GAPDH\"], on=\"protein\", classes=[\"treatment\", \"cellline\"])\n            ```\n    \"\"\"\n    on_user = on.lower()\n    gene_to_acc, _ = self.get_gene_maps(on='protein' if on_user in ('peptide', 'pep') else on_user) # type: ignore[attr-defined]\n\n\n    if on == 'peptide' and namelist:\n        pep_names = self.pep.var_names.astype(str) # type: ignore[attr-defined]\n        matched_peptides = [name for name in namelist if name in pep_names]\n        non_peptides = [name for name in namelist if name not in matched_peptides]\n\n        adata = None\n        if len(matched_peptides) &lt; len(namelist):\n            filtered = self.filter_prot(accessions=non_peptides, return_copy=True) # type: ignore[attr-defined]\n            adata = filtered.pep\n\n        if matched_peptides:\n            direct_peps = self.pep[:, matched_peptides] # type: ignore[attr-defined]\n            adata = direct_peps if adata is None else adata.concatenate(direct_peps, join='outer')\n\n        if adata is None or adata.n_vars == 0:\n            raise ValueError(\"No matching peptides found from the provided `namelist`.\")\n\n        adata = adata[:, ~adata.var_names.duplicated()]\n\n    else:\n        adata = utils.get_adata(self, on)\n\n        if namelist:\n            resolved = utils.resolve_accessions(adata, namelist, gene_map=gene_to_acc)\n            adata = adata[:, resolved]\n\n    # Extract the abundance matrix\n    X = adata.layers[layer] if layer in adata.layers else adata.X\n    if hasattr(X, \"toarray\"):\n        X = X.toarray()\n\n    # Melt into long form\n    df = pd.DataFrame(X, columns=adata.var_names, index=adata.obs_names).reset_index()\n    df = df.melt(id_vars=\"index\", var_name=\"accession\", value_name=\"abundance\")\n    df = df.rename(columns={\"index\": \"cell\"})\n\n    # Merge obs metadata\n    df = df.merge(adata.obs.reset_index(), left_on=\"cell\", right_on=\"index\")\n\n    _, pep_to_prot = self.get_gene_maps(on='peptide')  # peptide \u2192 protein map, # type: ignore[attr-defined]\n    _, acc_to_gene = self.get_gene_maps(on='protein')  # protein accession \u2192 gene, # type: ignore[attr-defined]\n    # Map to gene names\n    if on == 'peptide':\n        try:\n            df['protein_accession'] = df['accession'].map(pep_to_prot)\n            df['gene'] = df['protein_accession'].map(acc_to_gene)\n\n            # Report unmapped peptides\n            unmapped = df[df['gene'].isna()]['accession'].unique().tolist()\n            if unmapped:\n                print(f\"[get_abundance] {len(unmapped)} peptides could not be mapped to genes: {unmapped}\")\n        except Exception as e:\n            print(f\"[get_abundance] Mapping error: {e}\")\n            df['gene'] = None\n    else:\n        df['gene'] = df['accession'].map(acc_to_gene)\n\n    # Determine x-axis label\n    if x_label == 'gene':\n        df['x_label_name'] = df['gene'].fillna(df['accession'])\n    elif x_label == 'accession':\n        if on == 'protein':\n            df['x_label_name'] = df['accession']\n        elif on == 'peptide':\n            try:\n                mapping_col = utils.get_pep_prot_mapping(self)\n                pep_to_prot = self.pep.var[mapping_col].to_dict() # type: ignore[attr-defined]\n                df['x_label_name'] = df['protein_accession']\n            except Exception as e:\n                warnings.warn(f\"Could not map peptides to accessions: {e}\")\n                df['x_label_name'] = df['accession']\n    else:\n        df['x_label_name'] = df['accession']  # fallback\n\n    # Annotate class/grouping\n    if classes:\n        df['class'] = df[classes] if isinstance(classes, str) else df[classes].astype(str).agg('_'.join, axis=1)\n    else:\n        df['class'] = 'all'\n\n    # Log transform\n    if log:\n        df['log2_abundance'] = np.log2(np.clip(df['abundance'], 1e-6, None))\n\n    return df\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing.EditingMixin.set_X","title":"set_X","text":"<pre><code>set_X(layer, on='protein')\n</code></pre> <p>Set the <code>.X</code> matrix of protein or peptide data to a specified layer.</p> <p>This method replaces the active <code>.X</code> matrix with the contents of a named layer  from <code>.prot.layers</code> or <code>.pep.layers</code>. This is useful for switching between  different processing stages (e.g., normalized, imputed, or raw data).</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>Name of the data layer to assign to <code>.X</code>.</p> required <code>on</code> <code>str</code> <p>Whether to operate on <code>\"protein\"</code> or <code>\"peptide\"</code> data (default is <code>\"protein\"</code>).</p> <code>'protein'</code> <p>Returns:</p> Type Description <p>None</p> Example <p>Set the protein matrix <code>.X</code> to the \"normalized\" layer:     <pre><code>pdata.set_X(layer=\"normalized\", on=\"protein\")\n</code></pre></p> <p>Set the peptide matrix <code>.X</code> to the \"imputed\" layer:     <pre><code>pdata.set_X(layer=\"imputed\", on=\"peptide\")\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/editing.py</code> <pre><code>def set_X(self, layer, on = 'protein'):\n    \"\"\"\n    Set the `.X` matrix of protein or peptide data to a specified layer.\n\n    This method replaces the active `.X` matrix with the contents of a named layer \n    from `.prot.layers` or `.pep.layers`. This is useful for switching between \n    different processing stages (e.g., normalized, imputed, or raw data).\n\n    Args:\n        layer (str): Name of the data layer to assign to `.X`.\n        on (str): Whether to operate on `\"protein\"` or `\"peptide\"` data (default is `\"protein\"`).\n\n    Returns:\n        None\n\n    Example:\n        Set the protein matrix `.X` to the \"normalized\" layer:\n            ```python\n            pdata.set_X(layer=\"normalized\", on=\"protein\")\n            ```\n\n        Set the peptide matrix `.X` to the \"imputed\" layer:\n            ```python\n            pdata.set_X(layer=\"imputed\", on=\"peptide\")\n            ```\n    \"\"\"\n    # defines which layer to set X to\n    if not self._check_data(on): # type: ignore[attr-defined]\n        pass\n\n    if on == 'protein':\n        if layer not in self.prot.layers: # type: ignore[attr-defined]\n            raise ValueError(f\"Layer {layer} not found in protein data.\")\n        self.prot.X = self.prot.layers[layer] # type: ignore[attr-defined]\n        print(f\"{format_log_prefix('info_only', indent=2)} Set {on} data to layer {layer}.\")\n\n    else:\n        if layer not in self.pep.layers: # type: ignore[attr-defined]\n            raise ValueError(f\"Layer {layer} not found in peptide data.\")\n        self.pep.X = self.pep.layers[layer] # type: ignore[attr-defined]\n        print(f\"{format_log_prefix('info_only', indent=2)} Set {on} data (.X) to layer {layer}.\")\n\n    self._history.append(f\"{on}: Set X to layer {layer}.\") # type: ignore[attr-defined]\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.filtering.FilterMixin","title":"FilterMixin","text":"<p>Provides flexible filtering and annotation methods for samples, proteins, and peptides.</p> <p>This mixin includes utilities for:</p> <ul> <li>Filtering proteins and peptides by metadata conditions, group-level detection, or peptide mapping structure.</li> <li>Filtering samples based on class annotations, numeric thresholds, file lists, or query strings.</li> <li>Annotating detection status (\"Found In\") across samples and class-based groups.</li> <li>Managing and validating the protein\u2013peptide relational structure (RS matrix) after filtering.</li> </ul> <p>Methods:</p> Name Description <code>filter_prot</code> <p>Filters proteins using <code>.var</code> metadata conditions or a list of accessions/genes to retain.</p> <code>filter_prot_found</code> <p>Keeps proteins or peptides found in a minimum number or proportion of samples within a group or file list.</p> <code>_filter_sync_peptides_to_proteins</code> <p>Removes peptides orphaned by upstream protein filtering.</p> <code>filter_sample</code> <p>Filters samples using categorical metadata, numeric thresholds, or file/sample lists.</p> <code>_filter_sample_condition</code> <p>Internal helper for filtering samples using <code>.summary</code> conditions or name lists.</p> <code>_filter_sample_values</code> <p>Filters samples using dictionary-style matching on metadata fields.</p> <code>_filter_sample_query</code> <p>Parses and applies a raw pandas-style query string to <code>.obs</code> or <code>.summary</code>.</p> <code>filter_rs</code> <p>Filters the RS matrix by peptide count and ambiguity, and updates <code>.prot</code>/<code>.pep</code> accordingly.</p> <code>_apply_rs_filter</code> <p>Applies protein/peptide masks to <code>.prot</code>, <code>.pep</code>, and <code>.rs</code> matrices.</p> <code>_format_filter_query</code> <p>Formats filter conditions for <code>.eval()</code> by quoting fields and handling <code>includes</code> syntax.</p> <code>annotate_found</code> <p>Adds group-level \"Found In\" indicators to <code>.prot.var</code> or <code>.pep.var</code>.</p> <code>_annotate_found_samples</code> <p>Computes per-sample detection flags for use by <code>annotate_found()</code>.</p> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>class FilterMixin:\n    \"\"\"\n    Provides flexible filtering and annotation methods for samples, proteins, and peptides.\n\n    This mixin includes utilities for:\n\n    - Filtering proteins and peptides by metadata conditions, group-level detection, or peptide mapping structure.\n    - Filtering samples based on class annotations, numeric thresholds, file lists, or query strings.\n    - Annotating detection status (\"Found In\") across samples and class-based groups.\n    - Managing and validating the protein\u2013peptide relational structure (RS matrix) after filtering.\n\n    Functions:\n        filter_prot: Filters proteins using `.var` metadata conditions or a list of accessions/genes to retain.\n        filter_prot_found: Keeps proteins or peptides found in a minimum number or proportion of samples within a group or file list.\n        _filter_sync_peptides_to_proteins: Removes peptides orphaned by upstream protein filtering.\n        filter_sample: Filters samples using categorical metadata, numeric thresholds, or file/sample lists.\n        _filter_sample_condition: Internal helper for filtering samples using `.summary` conditions or name lists.\n        _filter_sample_values: Filters samples using dictionary-style matching on metadata fields.\n        _filter_sample_query: Parses and applies a raw pandas-style query string to `.obs` or `.summary`.\n        filter_rs: Filters the RS matrix by peptide count and ambiguity, and updates `.prot`/`.pep` accordingly.\n        _apply_rs_filter: Applies protein/peptide masks to `.prot`, `.pep`, and `.rs` matrices.\n        _format_filter_query: Formats filter conditions for `.eval()` by quoting fields and handling `includes` syntax.\n        annotate_found: Adds group-level \"Found In\" indicators to `.prot.var` or `.pep.var`.\n        _annotate_found_samples: Computes per-sample detection flags for use by `annotate_found()`.\n    \"\"\"\n\n    def filter_prot(self, condition = None, accessions=None, valid_genes=False, unique_profiles=False, return_copy = True, debug=False):\n        \"\"\"\n        Filter protein data based on metadata conditions or accession list (protein name and gene name).\n\n        This method filters the protein-level data either by evaluating a string condition on the protein metadata,\n        or by providing a list of protein accession numbers (or gene names) to keep. Peptides that are exclusively\n        linked to removed proteins are also removed, and the RS matrix is updated accordingly.\n\n        Args:\n            condition (str): A condition string to filter protein metadata. Supports:\n\n                - Standard comparisons, e.g. `\"Protein FDR Confidence: Combined == 'High'\"`\n                - Substring queries using `includes`, e.g. `\"Description includes 'p97'\"`\n            accessions (list of str, optional): List of accession numbers (var_names) to keep.\n            valid_genes (bool): If True, removes rows with missing gene names and resolves duplicate gene names by appending numeric suffixes.\n            unique_profiles (bool): If True, remove rows with duplicate abundance profiles across samples.\n            return_copy (bool): If True, returns a filtered copy. If False, modifies in place.\n            debug (bool): If True, prints debugging information.\n\n        Returns:\n            pAnnData (pAnnData): Returns a filtered pAnnData object if `return_copy=True`. \n            None (None): Otherwise, modifies in-place and returns None.\n\n        Examples:\n            Filter by metadata condition:\n                ```python\n                condition = \"Protein FDR Confidence: Combined == 'High'\"\n                pdata.filter_prot(condition=condition)\n                ```\n\n            Substring match on protein description:\n                ```python\n                condition = \"Description includes 'p97'\"\n                pdata.filter_prot(condition=condition)\n                ```\n\n            Numerical condition on metadata:\n                ```python\n                condition = \"Score &gt; 0.75\"\n                pdata.filter_prot(condition=condition)\n                ```\n\n            Filter by specific protein accessions:\n                ```python\n                accessions = ['GAPDH', 'P53']\n                pdata.filter_prot(accessions=accessions)\n                ```\n\n            Filter out all that have no valid genes (potentially artefacts):\n                ```python\n                pdata.filter_prot(valid_genes=True)\n                ```\n\n            !!! tip\n                Multiple filters can be combined in a single call. For example, to filger by condition and valid genes:\n                ```python\n                condition = \"Score &gt; 0.75\"\n                pdata.filter_prot(condition=condition, valid_genes=True)\n        \"\"\"\n        from scipy.sparse import issparse\n\n        if not self._check_data('protein'): # type: ignore[attr-defined]\n            raise ValueError(f\"No protein data found. Check that protein data was imported.\")\n\n        pdata = self.copy() if return_copy else self # type: ignore[attr-defined]\n        action = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n\n        message_parts = []\n\n        # 1. Filter by condition\n        if condition is not None:\n            formatted_condition = self._format_filter_query(condition, pdata.prot.var)\n            if debug:\n                print(f\"Formatted condition: {formatted_condition}\")\n            filtered_proteins = pdata.prot.var[pdata.prot.var.eval(formatted_condition)]\n            pdata.prot = pdata.prot[:, filtered_proteins.index]\n            message_parts.append(f\"condition: {condition}\")\n\n        # 2. Filter by accession list or gene names\n        if accessions is not None:\n            gene_map, _ = pdata.get_gene_maps(on='protein') # type: ignore[attr-defined]\n\n            resolved, unmatched = [], []\n            var_names = pdata.prot.var_names.astype(str)\n\n            for name in accessions:\n                name = str(name)\n                if name in var_names:\n                    resolved.append(name)\n                elif name in gene_map:\n                    resolved.append(gene_map[name])\n                else:\n                    unmatched.append(name)\n\n            if unmatched:\n                warnings.warn(\n                    f\"The following accession(s) or gene name(s) were not found and will be ignored: {unmatched}\"\n                )\n\n            if not resolved:\n                warnings.warn(\"No matching accessions found. No proteins will be retained.\")\n                pdata.prot = pdata.prot[:, []]\n                message_parts.append(\"accessions: 0 matched\")\n            else:\n                pdata.prot = pdata.prot[:, pdata.prot.var_names.isin(resolved)]\n                message_parts.append(f\"accessions: {len(resolved)} matched / {len(accessions)} requested\")\n\n        # 3. Valid genes\n        if valid_genes:\n            # A. Remove invalid gene entries\n            var = pdata.prot.var\n\n            mask_missing_gene = var[\"Genes\"].isna() | (var[\"Genes\"].astype(str).str.strip() == \"\")\n            keep_mask = ~mask_missing_gene\n\n            if debug:\n                print(f\"Missing genes: {mask_missing_gene.sum()}\")\n                missing_names = pdata.prot.var_names[mask_missing_gene]\n                print(f\"Examples of proteins missing names: {missing_names[:5].tolist()}\")\n\n            pdata.prot = pdata.prot[:, keep_mask].copy()\n            message_parts.append(f\"valid_genes: removed {int(mask_missing_gene.sum())} proteins with invalid gene names\")            \n\n            # B. Resolve duplicate gene names\n            var = pdata.prot.var  # refresh after filtering\n            var_genes = var[\"Genes\"].astype(str).str.strip()\n            gene_counts = var_genes.value_counts()\n            duplicates = gene_counts[gene_counts &gt; 1].index.tolist()\n\n            if len(duplicates) &gt; 0:\n                if debug:\n                    print(f\"Found {len(duplicates)} duplicate gene names.\")\n\n                # Track how many times each duplicate has appeared\n                seen = {}\n                new_names = []\n                for gene in var[\"Genes\"]:\n                    if gene in duplicates:\n                        seen[gene] = seen.get(gene, 0) + 1\n                        if seen[gene] &gt; 1:\n                            gene = f\"{gene}-{seen[gene]}\"\n                    new_names.append(gene)\n\n                # Assign back to var\n                pdata.prot.var[\"Genes\"] = new_names\n\n                message_parts.append(f\"valid_genes: resolved {len(duplicates)} duplicate gene names by appending numeric suffixes\")\n                if debug:\n                    example_dupes = [d for d in duplicates[:5]]\n                    print(f\"Examples of duplicate genes resolved: {example_dupes}\")\n\n        # 4. Remove duplicate profiles\n        if unique_profiles:\n            X = pdata.prot.X.toarray() if issparse(pdata.prot.X) else pdata.prot.X\n            df_X = pd.DataFrame(X.T, index=pdata.prot.var_names)\n\n            all_nan = np.all(np.isnan(X), axis=0)\n            all_zero = np.all(X == 0, axis=0)\n            empty_mask = all_nan | all_zero\n\n            duplicated_mask = df_X.duplicated(keep=\"first\").values  # mark duplicates\n\n            # Combine removal conditions\n            remove_mask = duplicated_mask | empty_mask\n            keep_mask = ~remove_mask\n\n            # Counts for each type\n            n_dup = int(duplicated_mask.sum())\n            n_empty = int(empty_mask.sum())\n            n_total = int(remove_mask.sum())\n\n            if debug:\n                dup_names = pdata.prot.var_names[duplicated_mask]\n                print(f\"Duplicate abundance profiles detected: {n_dup} proteins\")\n                if len(dup_names) &gt; 0:\n                    print(f\"Examples of duplicates: {dup_names[:5].tolist()}\")\n                print(f\"Empty (all-zero or all-NaN) proteins detected: {n_empty}\")\n\n            # Apply filter\n            pdata.prot = pdata.prot[:, keep_mask].copy()\n\n            # Add summary message\n            message_parts.append(\n                f\"unique_profiles: removed {n_dup} duplicate and {n_empty} empty abundance profiles \"\n                f\"({n_total} total)\"\n            )\n\n        if not message_parts:\n            # no filters were applied\n            message = f\"{format_log_prefix('user')} Filtering proteins [failed]: {action} protein data.\\n    \u2192 No filters applied.\"\n        else:\n            # at least 1 filter applied\n            # PEPTIDES: also filter out peptides that belonged only to the filtered proteins\n            if pdata.pep is not None and pdata.rs is not None: # type: ignore[attr-defined]\n                proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = pdata._filter_sync_peptides_to_proteins(\n                    original=self, \n                    updated_prot=pdata.prot, \n                    debug=debug)\n\n                # Apply filtered RS and update .prot and .pep using the helper\n                pdata._apply_rs_filter(\n                    keep_proteins=proteins_to_keep,\n                    keep_peptides=peptides_to_keep,\n                    orig_prot_names=orig_prot_names,\n                    orig_pep_names=orig_pep_names,\n                    debug=debug\n                )\n\n            # detect which filters were applied\n            active_filters = []\n            if condition is not None:\n                active_filters.append(\"condition\")\n            if accessions is not None:\n                active_filters.append(\"accession\")\n            if valid_genes:\n                active_filters.append(\"valid genes\")\n            if unique_profiles:\n                active_filters.append(\"unique profiles\")\n\n            # build the header, joining multiple filters nicely\n            joined_filters = \", \".join(active_filters) if active_filters else \"unspecified\"\n            message = (\n                f\"{format_log_prefix('user')} Filtering proteins [{joined_filters}]:\\n\"\n                f\"    {action} protein data based on {joined_filters}:\"\n            )\n\n            for part in message_parts:\n                    message += f\"\\n    \u2192 {part}\"\n\n            # Protein and peptide counts summary\n            message += f\"\\n    \u2192 Proteins kept: {pdata.prot.shape[1]}\"\n            if pdata.pep is not None:\n                message += f\"\\n    \u2192 Peptides kept (linked): {pdata.pep.shape[1]}\\n\"\n\n        print(message)\n        pdata._append_history(message) # type: ignore[attr-defined]\n        pdata.update_summary(recompute=True) # type: ignore[attr-defined]\n        return pdata if return_copy else None\n\n    def filter_prot_found(self, group, min_ratio=None, min_count=None, on='protein', return_copy=True, verbose=True, match_any=False):\n        \"\"\"\n        Filter proteins or peptides based on 'Found In' detection across samples or groups.\n\n        This method filters features by checking whether they are found in a minimum number or proportion \n        of samples, either at the group level (e.g., biological condition) or based on individual files.\n\n        Args:\n            group (str or list of str): Group name(s) corresponding to 'Found In: {group} ratio' \n                (e.g., \"HCT116_DMSO\") or a list of filenames (e.g., [\"F1\", \"F2\"]). If this argument matches one or more `.obs` columns, the function automatically \n                interprets it as a class name, expands it to all class values, and annotates the\n                necessary `'Found In:'` features.\n            min_ratio (float, optional): Minimum proportion (0.0\u20131.0) of samples the feature must be \n                found in. Ignored for file-based filtering.\n            min_count (int, optional): Minimum number of samples the feature must be found in. Alternative \n                to `min_ratio`. Ignored for file-based filtering.\n            on (str): Feature level to filter: either \"protein\" or \"peptide\".\n            return_copy (bool): If True, returns a filtered copy. If False, modifies in place.\n            verbose (bool): If True, prints verbose summary information.\n            match_any (bool): Defaults to False, for a AND search condition. If True, matches features found in any of the specified groups/files (i.e. union).\n\n        Returns:\n            pAnnData: A filtered pAnnData object if `return_copy=True`; otherwise, modifies in place and returns None.\n\n        Note:\n            - If `group` matches `.obs` column names, the method automatically annotates found \n              features by class before filtering.\n            - For file-based filtering, use the file identifiers from `.prot.obs_names`.            \n\n        Examples:\n            Filter proteins found in all \"cellline\" groups (e.g. Cellline A, and cellline B), with at least 2 samples each:\n                ```python\n                pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_count=2, match_any=False)\n                ```\n\n            Filter proteins found in any \"cellline\" groups (e.g. Cellline A, and cellline B), as long as they meet a minimum ratio of 0.4:\n                ```python\n                pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_ratio=0.4, match_any=True)\n                ```                \n\n            Filter proteins found in all three input files:\n                ```python\n                pdata.filter_prot_found(group=[\"F1\", \"F2\", \"F3\"])\n                ```\n\n            Filter proteins found in files of a specific sub-group:\n                ```python\n                pdata.annotate_found(classes=['group','treatment'])\n                pdata.filter_prot_found(group=[\"groupA_control\", \"groupB_treated\"])\n                ```\n\n            If a single class column (e.g., `\"cellline\"`) is given, filter proteins based on each of its unique values (e.g. Line A, Line B):\n                ```python\n                pdata.filter_prot_found(group=\"cellline\", min_ratio=0.5)\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined]\n            return\n\n        adata = self.prot if on == 'protein' else self.pep\n        var = adata.var\n\n        # Normalize group to list\n        if isinstance(group, str):\n            group = [group]\n        if not isinstance(group, (list, tuple)):\n            raise TypeError(\"`group` must be a string or list of strings.\")\n\n        # Auto-resolve obs columns passed instead of group values\n        auto_value_msg = None\n        if all(g in adata.obs.columns for g in group):\n            if len(group) == 1:\n                obs_col = group[0]\n                expanded_groups = adata.obs[obs_col].unique().tolist()\n            else:\n                expanded_groups = (\n                    adata.obs[group].astype(str)\n                        .agg(\"_\".join, axis=1)\n                        .unique()\n                        .tolist()\n                )\n            # auto-annotate found features by these obs columns\n            self.annotate_found(classes=group, on=on, verbose=False)\n            group = expanded_groups\n            auto_value_msg = (\n                f\"{format_log_prefix('info', 2)} Found matching obs column(s): {group}. \"\n                \"Automatically annotating detection by group values.\"\n            )\n\n        if verbose and auto_value_msg:\n            print(auto_value_msg)\n\n        # Determine filtering mode: group vs file or handle ambiguity/missing\n        group_metrics = adata.uns.get(f\"found_metrics_{on}\")\n\n        mode = None\n        all_file_cols = all(f\"Found In: {g}\" in var.columns for g in group)\n        all_group_cols = (\n            group_metrics is not None\n            and all((g, \"count\") in group_metrics.columns for g in group)\n        )\n\n        # --- 1\ufe0f\u20e3 Explicit ambiguity: both file- and group-level indicators exist ---\n        is_ambiguous, annotated_files, annotated_groups = _detect_ambiguous_input(group, var, group_metrics)\n        if is_ambiguous:\n            raise ValueError(\n                f\"Ambiguous input: items in {group} include both file identifiers {annotated_files} \"\n                f\"and group values {annotated_groups}.\\n\"\n                \"Please separate group-based and file-based filters into separate calls.\"\n            )\n\n        # --- 2\ufe0f\u20e3 Group-based mode ---\n        elif all_group_cols:\n            mode = \"group\"\n\n        # --- 3\ufe0f\u20e3 File-based mode ---\n        elif all_file_cols:\n            mode = \"file\"\n\n        # --- 4\ufe0f\u20e3 Mixed or unresolved case (fallback) ---\n        else:\n            missing = []\n            for g in group:\n                group_missing = (\n                    group_metrics is None\n                    or (g, \"count\") not in group_metrics.columns\n                    or (g, \"ratio\") not in group_metrics.columns\n                )\n                file_missing = f\"Found In: {g}\" not in var.columns\n\n                if group_missing and file_missing:\n                    missing.append(g)\n\n            # Consistent, readable user message\n            msg = [f\"The following group(s)/file(s) could not be found: {missing or '\u2014'}\"]\n            msg.append(\"\u2192 If these are group names, make sure you ran:\")\n            msg.append(f\"   pdata.annotate_found(classes={group})\")\n            msg.append(\"\u2192 If these are file names, ensure 'Found In: &lt;file&gt;' columns exist.\\n\")\n            raise ValueError(\"\\n\".join(msg))\n\n        # ---------------\n        # Apply filtering\n        mask = np.ones(len(var), dtype=bool)\n\n        if mode == \"file\":\n            if match_any: # OR logic\n                mask = np.zeros(len(var), dtype=bool)\n                for g in group:\n                    col = f\"Found In: {g}\"\n                    mask |= var[col]  \n                if verbose:\n                    print(f\"{format_log_prefix('user')} Filtering proteins [Found|File-mode|ANY]: keeping {mask.sum()} / {len(mask)} features found in ANY of files: {group}\")\n            else: # AND logic (default)\n                for g in group:\n                    col = f\"Found In: {g}\"\n                    mask &amp;= var[col]\n                if verbose:\n                    print(f\"{format_log_prefix('user')} Filtering proteins [Found|File-mode|ALL]: keeping {mask.sum()} / {len(mask)} features found in ALL files: {group}\")\n\n        elif mode == \"group\":\n            if min_ratio is None and min_count is None:\n                raise ValueError(\"You must specify either `min_ratio` or `min_count` when filtering by group.\")\n\n            if match_any: # ANY logic\n                mask = np.zeros(len(var), dtype=bool)\n                for g in group:\n                    count_series = group_metrics[(g, \"count\")]\n                    ratio_series = group_metrics[(g, \"ratio\")]\n\n                    if min_ratio is not None:\n                        this_mask = ratio_series &gt;= min_ratio\n                    else:\n                        this_mask = count_series &gt;= min_count\n\n                    mask |= this_mask\n                if verbose:\n                    print(f\"{format_log_prefix('user')} Filtering proteins [Found|Group-mode|ANY]: keeping {mask.sum()} / {len(mask)} features passing threshold in ANY of groups: {group}\")\n\n            else:\n                for g in group:\n                    count_series = group_metrics[(g, \"count\")]\n                    ratio_series = group_metrics[(g, \"ratio\")]\n\n                    if min_ratio is not None:\n                        this_mask = ratio_series &gt;= min_ratio\n                    else:\n                        this_mask = count_series &gt;= min_count\n\n                    mask &amp;= this_mask\n\n                if verbose:\n                    print(f\"{format_log_prefix('user')} Filtering proteins [Found|Group-mode|ALL]: keeping {mask.sum()} / {len(mask)} features passing threshold {min_ratio if min_ratio is not None else min_count} across groups: {group}\")\n\n        # Apply filtering\n        filtered = self.copy() if return_copy else self # type: ignore[attr-defined], EditingMixin\n        adata_filtered = adata[:, mask.values]\n\n        if on == 'protein':\n            filtered.prot = adata_filtered\n\n            # Optional: filter peptides + rs as well\n            if filtered.pep is not None and filtered.rs is not None:\n                proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = filtered._filter_sync_peptides_to_proteins(\n                    original=self,\n                    updated_prot=filtered.prot,\n                    debug=verbose\n                )\n\n                filtered._apply_rs_filter(\n                    keep_proteins=proteins_to_keep,\n                    keep_peptides=peptides_to_keep,\n                    orig_prot_names=orig_prot_names,\n                    orig_pep_names=orig_pep_names,\n                    debug=verbose\n                )\n\n        else:\n            filtered.pep = adata_filtered\n            # Optionally, we could also remove proteins no longer linked to any peptides,\n            # but that's less common and we can leave it out unless requested.\n\n        criteria_str = (\n            f\"min_ratio={min_ratio}\" if mode == \"group\" and min_ratio is not None else\n            f\"min_count={min_count}\" if mode == \"group\" else\n            (\"ANY files\" if match_any else \"ALL files\")\n        )\n\n        logic_str = \"ANY\" if match_any else \"ALL\"\n\n        filtered._append_history(  # type: ignore[attr-defined], HistoryMixin\n            f\"{on}: Filtered by detection in {mode} group(s) {group} using {criteria_str} (match_{logic_str}).\"\n        )\n        filtered.update_summary(recompute=True) # type: ignore[attr-defined], SummaryMixin\n\n        return filtered if return_copy else None\n\n    def filter_prot_significant(self, group=None, min_ratio=None, min_count=None, fdr_threshold=0.01, return_copy=True, verbose=True, match_any=True):\n        \"\"\"\n        Filter proteins based on significance across samples or groups using FDR thresholds.\n\n        This method filters proteins by checking whether they are significant (e.g. PG.Q.Value &lt; 0.01)\n        in a minimum number or proportion of samples, either per file or grouped.\n\n        Args:\n            group (str, list, or None): Group name(s) (e.g., sample classes or filenames). If None, uses all files.\n            min_ratio (float, optional): Minimum proportion of samples to be significant.\n            min_count (int, optional): Minimum number of samples to be significant.\n            fdr_threshold (float): Significance threshold (default = 0.01).\n            return_copy (bool): Whether to return a filtered copy or modify in-place.\n            verbose (bool): Whether to print summary.\n            match_any (bool): If True, retain proteins significant in *any* group/file (OR logic). If False, require *all* groups/files to be significant (AND logic).\n\n        Returns:\n            pAnnData or None: Filtered object (if `return_copy=True`) or modifies in-place.\n\n        Examples:\n            Filter proteins significant by their global significance (e.g. PD-based imports):\n                ```python\n                pdata.filter_prot_significant()\n                ```\n\n            Filter proteins significant in the \"cellline\" group containing e.g. \"groupA\" and \"groupB\" groups, FDR of 0.01 (default):\n                ```python\n                pdata.filter_prot_significant(group=[\"cellline\"], min_count=2)\n                ```\n\n            Filter proteins significant in all three input files:\n                ```\n                pdata.filter_prot_significant(group=[\"F1\", \"F2\", \"F3\"])\n                ```\n\n            Filter proteins significant in files of a specific sub-group:\n                ```python\n                pdata.annotate_significant(classes=['group','treatment'])\n                pdata.filter_prot_significant(group=[\"groupA_control\", \"groupB_treated\"])            \n                ```\n\n        Todo:\n            Implement peptide then protein filter\n        \"\"\"\n        if not self._check_data(\"protein\"): # type: ignore[attr-defined]\n            return\n\n        adata = self.prot \n        var = adata.var\n\n        # Detect per-sample significance layer\n        has_protein_level_significance = any(\n            k.lower().endswith(\"_qval\") or k.lower().endswith(\"_fdr\") for k in adata.layers.keys()\n        )\n\n        # --- Handle missing significance data entirely ---\n        if not has_protein_level_significance and \"Global_Q_value\" not in adata.var.columns:\n            raise ValueError(\n                \"No per-sample layer (e.g., *_qval) or global significance column ('Global_Q_value') \"\n                \"found in .prot. Please ensure your data includes q-values or run annotate_significant().\"\n            )\n\n        # --- 1\ufe0f\u20e3 Global fallback mode (e.g. PD-based imports) ---\n        if not has_protein_level_significance and \"Global_Q_value\" in adata.var.columns:\n            if group is not None:\n                raise ValueError(\n                    f\"Cannot filter by group {group}: per-sample significance data missing \"\n                    \"and only global q-values available.\"\n                )\n\n            global_mask = adata.var[\"Global_Q_value\"] &lt; fdr_threshold\n\n            n_total = len(global_mask)\n            n_kept = int(global_mask.sum())\n            n_dropped = n_total - n_kept\n\n            filtered = self.copy() if return_copy else self\n            filtered.prot = adata[:, global_mask]\n\n            if filtered.pep is not None and filtered.rs is not None:\n                proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = filtered._filter_sync_peptides_to_proteins(\n                    original=self, updated_prot=filtered.prot, debug=verbose\n                )\n                filtered._apply_rs_filter(\n                    keep_proteins=proteins_to_keep,\n                    keep_peptides=peptides_to_keep,\n                    orig_prot_names=orig_prot_names,\n                    orig_pep_names=orig_pep_names,\n                    debug=verbose,\n                )\n\n            filtered.update_summary(recompute=True)\n            filtered._append_history(\n                f\"Filtered by global significance (Global_Q_value &lt; {fdr_threshold}); \"\n                f\"{n_kept}/{n_total} proteins retained.\"\n            )\n\n            if verbose:\n                print(f\"{format_log_prefix('user')} Filtering proteins by significance [Global-mode]:\")\n                print(f\"{format_log_prefix('info', 2)} Using global protein-level q-values (no per-sample significance available).\")\n                return_copy_str = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n                print(f\"    {return_copy_str} protein data based on significance thresholds:\")\n                print(f\"{format_log_prefix('filter_conditions')}Files requested: All\")\n                print(f\"{format_log_prefix('filter_conditions')}FDR threshold: {fdr_threshold}\")\n                print(f\"    \u2192 Proteins kept: {n_kept}, Proteins dropped: {n_dropped}\\n\")\n\n            return filtered if return_copy else None\n\n        # --- 2\ufe0f\u20e3 Per-sample significance data available ---\n        no_group_msg = None\n        auto_group_msg = None\n        auto_value_msg = None\n\n        if group is None:\n            group_list = list(adata.obs_names)\n            if verbose:\n                no_group_msg = f\"{format_log_prefix('info', 2)} No group provided. Defaulting to sample-level significance filtering.\"\n        else:\n            group_list = [group] if isinstance(group, str) else group\n\n        # Ensure annotations exist or auto-generate\n        missing_cols = [f\"Significant In: {g}\" for g in group_list]\n        if all(col in var.columns for col in missing_cols):\n            # Case A: user passed actual group values, already annotated\n            pass\n        else:\n            # Case B: need to resolve automatically\n            if all(g in adata.obs.columns for g in group_list):\n                # User passed obs column(s)\n                if len(group_list) == 1:\n                    obs_col = group_list[0]\n                    expanded_groups = adata.obs[obs_col].unique().tolist()\n                else:\n                    expanded_groups = (\n                        adata.obs[group_list].astype(str)\n                            .agg(\"_\".join, axis=1)\n                            .unique()\n                            .tolist()\n                    )\n                self.annotate_significant(classes=group_list,\n                                        fdr_threshold=fdr_threshold,\n                                        on=\"protein\", verbose=False)\n                group_list = expanded_groups\n                auto_group_msg = f\"{format_log_prefix('info', 2)} Found matching obs column '{group_list}'. Automatically annotating significance by group: {group_list} using FDR threshold {fdr_threshold}.\"\n\n            else:\n                # User passed group values, but not annotated yet\n                found_obs_col = None\n                for obs_col in adata.obs.columns:\n                    if set(group_list).issubset(set(adata.obs[obs_col].unique())):\n                        found_obs_col = obs_col\n                        break\n\n                if found_obs_col is not None:\n                    self.annotate_significant(classes=[found_obs_col],\n                                            fdr_threshold=fdr_threshold,\n                                            on=\"protein\", indent=2, verbose=False)\n                    auto_value_msg = (f\"{format_log_prefix('info', 2)} Found matching obs column '{found_obs_col}'\"\n                    f\"for groups {group_list}. Automatically annotating significant features by group {found_obs_col} \"\n                    f\"using FDR threshold {fdr_threshold}.\")    \n                else:\n                    raise ValueError(\n                        f\"Could not find existing significance annotations for groups {group_list}. \"\n                        \"Please either pass valid obs column(s), provide values from a valid `.obs` column or run `annotate_significant()` first.\"\n                    )\n\n        # --- 3\ufe0f\u20e3 Mode detection and ambiguity handling ---\n        metrics_key = \"significance_metrics_protein\"\n        metrics_df = adata.uns.get(metrics_key, pd.DataFrame())\n\n        is_ambiguous, annotated_files, annotated_groups = _detect_ambiguous_input(group_list, var, metrics_df)\n        if is_ambiguous:\n            raise ValueError(\n                f\"Ambiguous input: items in {group_list} include both file identifiers {annotated_files} \"\n                f\"and group values {annotated_groups}.\\n\"\n                \"Please separate group-based and file-based filters into separate calls.\"\n            )\n\n        all_group_cols = (\n            metrics_df is not None\n            and all((g, \"count\") in metrics_df.columns for g in group_list)\n        )\n        all_file_cols = all(f\"Significant In: {g}\" in var.columns for g in group_list)\n        mode = \"group\" if all_group_cols else \"file\"\n\n        # Build filtering mask\n        mask = np.zeros(len(var), dtype=bool) if match_any else np.ones(len(var), dtype=bool)\n\n        if mode == \"group\":\n            if min_ratio is None and min_count is None:\n                raise ValueError(\"Specify `min_ratio` or `min_count` for group-based filtering.\")\n            for g in group_list:\n                count = metrics_df[(g, \"count\")]\n                ratio = metrics_df[(g, \"ratio\")]\n                this_mask = ratio &gt;= min_ratio if min_ratio is not None else count &gt;= min_count\n                mask = mask | this_mask if match_any else mask &amp; this_mask\n        else:  # file mode\n            for g in group_list:\n                col = f\"Significant In: {g}\"\n                this_mask = var[col].values\n                mask = mask | this_mask if match_any else mask &amp; this_mask\n\n        # --- 4\ufe0f\u20e3 Apply filtering and sync ---\n        filtered = self.copy() if return_copy else self\n        filtered.prot = adata[:, mask]\n\n        # Sync peptides and RS\n        if filtered.pep is not None and filtered.rs is not None:\n            proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = filtered._filter_sync_peptides_to_proteins(\n                original=self, updated_prot=filtered.prot, debug=verbose\n            )\n            filtered._apply_rs_filter(\n                keep_proteins=proteins_to_keep,\n                keep_peptides=peptides_to_keep,\n                orig_prot_names=orig_prot_names,\n                orig_pep_names=orig_pep_names,\n                debug=verbose\n            )\n\n        filtered.update_summary(recompute=True)\n        filtered._append_history(\n            f\"Filtered by significance (FDR &lt; {fdr_threshold}) in group(s): {group_list}, \"\n            f\"using min_ratio={min_ratio} / min_count={min_count}, match_any={match_any}\"\n        )\n\n        if verbose:\n            logic = \"any\" if match_any else \"all\"\n            mode_str = \"Group-mode\" if mode == \"group\" else \"File-mode\"\n\n            print(f\"{format_log_prefix('user')} Filtering proteins [Significance|{mode_str}]:\")\n\n            if no_group_msg:\n                print(no_group_msg)\n            if auto_group_msg:\n                print(auto_group_msg)\n            if auto_value_msg:\n                print(auto_value_msg)\n\n            return_copy_str = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n            print(f\"    {return_copy_str} protein data based on significance thresholds:\")\n\n            if mode == \"group\":\n                # Case A: obs column(s) expanded \u2192 show expanded_groups and add note\n                if auto_group_msg:\n                    group_note = f\" (all values of obs column(s))\"\n                    print(f\"{format_log_prefix('filter_conditions')}Groups requested: {group_list}{group_note}\")\n                else:\n                    print(f\"{format_log_prefix('filter_conditions')}Groups requested: {group_list}\")\n                print(f\"{format_log_prefix('filter_conditions')}FDR threshold: {fdr_threshold}\")\n                if min_ratio is not None:\n                    print(f\"{format_log_prefix('filter_conditions')}Minimum ratio: {min_ratio} (match_{logic} = {match_any})\")\n                if min_count is not None:\n                    print(f\"{format_log_prefix('filter_conditions')}Minimum count: {min_count} (match_{logic} = {match_any})\")\n            else:\n                print(f\"{format_log_prefix('filter_conditions')}Files requested: All\")\n                print(f\"{format_log_prefix('filter_conditions')}FDR threshold: {fdr_threshold}\")\n                print(f\"{format_log_prefix('filter_conditions')}Logic: {logic} \"\n                    f\"(protein must be significant in {'\u22651' if match_any else 'all'} file(s))\")\n\n            n_kept = int(mask.sum())\n            n_total = len(mask)\n            n_dropped = n_total - n_kept\n            print(f\"    \u2192 Proteins kept: {n_kept}, Proteins dropped: {n_dropped}\\n\")\n\n        return filtered if return_copy else None\n\n    def _filter_sync_peptides_to_proteins(self, original, updated_prot, debug=None):\n        \"\"\"\n        Helper function to filter peptides based on the updated protein list.\n\n        This method determines which peptides to retain after protein-level filtering,\n        and returns the necessary inputs for `_apply_rs_filter`.\n\n        Args:\n            original (pAnnData): Original pAnnData object before filtering.\n            updated_prot (AnnData): Updated protein AnnData object to filter against.\n            debug (bool, optional): If True, prints debugging information.\n\n        Returns:\n            tuple: Inputs needed for downstream `_apply_rs_filter` operation.\n        \"\"\"\n        if debug:\n            print(f\"{format_log_prefix('info')} Applying RS-based peptide sync-up on peptides after protein filtering...\")\n\n        # Get original axis names from unfiltered self\n        rs = original.rs\n        orig_prot_names = np.array(original.prot.var_names)\n        orig_pep_names = np.array(original.pep.var_names)\n        # Determine which protein rows to keep in RS\n        proteins_to_keep=updated_prot.var_names\n        keep_set = set(proteins_to_keep)\n        prot_mask = np.fromiter((p in keep_set for p in orig_prot_names), dtype=bool)\n        rs_filtered = rs[prot_mask, :]\n        # Keep peptides that are still linked to \u22651 protein\n        pep_mask = np.array(rs_filtered.sum(axis=0)).ravel() &gt; 0\n        peptides_to_keep = orig_pep_names[pep_mask]\n\n        return proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names\n\n    def filter_sample(self, values=None, exact_cases=False, condition=None, file_list=None, exclude_file_list=None, min_prot=None, cleanup=True, return_copy=True, debug=False, query_mode=False):\n        \"\"\"\n        Filter samples in a pAnnData object based on categorical, numeric, or identifier-based criteria.\n\n        You must specify **exactly one** of the following:\n\n        - `values`: Dictionary or list of dictionaries specifying class-based filters (e.g., treatment, cellline).\n        - `condition`: A string condition evaluated against summary-level numeric metadata (e.g., protein count).\n        - `file_list`: List of sample or file names to retain.\n\n        Args:\n            values (dict or list of dict, optional): Categorical metadata filter. Matches rows in `.summary` or `.obs` with those field values.\n                Examples: `{'treatment': 'kd', 'cellline': 'A'}`.\n            exact_cases (bool): If True, uses exact match across all class values when `values` is a list of dicts.\n            condition (str, optional): Logical condition string referencing summary columns. This should reference columns in `pdata.summary`.\n                Examples: `\"protein_count &gt; 1000\"`.\n            file_list (list of str, optional): List of sample names or file identifiers to keep. Filters to only those samples (must match obs_names).\n            exclude_file_list (list of str, optional): Similar to `file_list`, but excludes the specified files/samples instead of keeping them.\n            min_prot (int, optional): Minimum number of proteins required in a sample to retain it.\n            cleanup (bool): If True (default), remove proteins that become all-NaN or all-zero after sample filtering and synchronize RS/peptide matrices. Set to False to retain all proteins for consistent feature alignment (e.g. during DE analysis).\n            return_copy (bool): If True, returns a filtered pAnnData object; otherwise modifies in place.\n            debug (bool): If True, prints query strings and filter summaries.\n            query_mode (bool): If True, interprets `values` or `condition` as a raw pandas-style `.query()` string and evaluates it directly on `.obs` or `.summary` respectively.\n\n        Returns:\n            pAnnData: Filtered pAnnData object if `return_copy=True`; otherwise, modifies in place and returns None.\n\n        Raises:\n            ValueError: If more than one or none of `values`, `condition`, or `file_list` is specified.\n\n        Examples:\n            Filter by metadata values:\n                ```python\n                pdata.filter_sample(values={'treatment': 'kd', 'cellline': 'A'})\n                ```\n\n            Filter with multiple exact matching cases:\n                ```python\n                pdata.filter_sample(\n                    values=[\n                        {'treatment': 'kd', 'cellline': 'A'},\n                        {'treatment': 'sc', 'cellline': 'B'}\n                    ],\n                    exact_cases=True\n                )\n                ```\n\n            Filter by numeric condition on summary:\n                ```python\n                pdata.filter_sample(condition=\"protein_count &gt; 1000\")\n                ```\n\n            Filter samples with fewer than 1000 proteins:\n                ```python\n                pdata.filter_sample(min_prot=1000)\n                ```\n\n            Keep specific samples by name:\n                ```python\n                pdata.filter_sample(file_list=['Sample_001', 'Sample_007'])\n                ```\n\n            Exclude specific files from the dataset:\n                ```python\n                pdata.filter_sample(exclude_file_list=['Sample_001', 'Sample_007'])\n                ```\n\n            For advanced usage using query mode, see the note below.\n\n            !!! note \"Advanced Usage\"\n                To enable **advanced filtering**, set `query_mode=True` to evaluate raw pandas-style queries:\n\n                - Query `.obs` metadata:\n                    ```python\n                    pdata.filter_sample(values=\"cellline == 'AS' and treatment == 'kd'\", query_mode=True)\n                    ```\n\n                - Query `.summary` metadata:\n                    ```python\n                    pdata.filter_sample(condition=\"protein_count &gt; 1000 and missing_pct &lt; 0.2\", query_mode=True)\n                    ```            \n        \"\"\"\n        # Ensure exactly one of the filter modes is specified\n        provided = [values, condition, file_list, min_prot, exclude_file_list]\n        if sum(arg is not None for arg in provided) != 1:\n            raise ValueError(\n                \"Invalid filter input. You must specify exactly one of the following keyword arguments:\\n\"\n                \"- `values=...` for categorical metadata filtering,\\n\"\n                \"- `condition=...` for summary-level condition filtering, or\\n\"\n                \"- `min_prot=...` to filter by minimum protein count.\\n\"\n                \"- `file_list=...` to filter by sample IDs.\\n\"\n                \"- `exclude_file_list=...` to exclude specific sample IDs.\\n\\n\"\n                \"Examples:\\n\"\n                \"  pdata.filter_sample(condition='protein_quant &gt; 0.2')\"\n            )\n\n        if min_prot is not None:\n            condition = f\"protein_count &gt;= {min_prot}\"\n\n        if values is not None and not query_mode:\n            return self._filter_sample_values(\n                values=values,\n                exact_cases=exact_cases,\n                debug=debug,\n                return_copy=return_copy, \n                cleanup=cleanup\n            )\n\n        if (condition is not None or file_list is not None or exclude_file_list is not None) and not query_mode:\n            return self._filter_sample_condition(\n                condition=condition,\n                file_list=file_list,\n                exclude_file_list=exclude_file_list,\n                return_copy=return_copy,\n                debug=debug, \n                cleanup=cleanup\n            )\n\n        if values is not None and query_mode:\n            return self._filter_sample_query(query_string=values, source='obs', return_copy=return_copy, debug=debug, cleanup=cleanup)\n\n        if condition is not None and query_mode:\n            return self._filter_sample_query(query_string=condition, source='summary', return_copy=return_copy, debug=debug, cleanup=cleanup)\n\n    def _filter_sample_condition(self, condition = None, return_copy = True, file_list=None, exclude_file_list=None, cleanup=True, debug=False):\n        \"\"\"\n        Filter samples based on numeric metadata conditions or a list of sample identifiers.\n\n        This internal method supports two modes:\n\n        - A string `condition` evaluated against `.summary` (e.g., `\"protein_count &gt; 1000\"`).\n        - A `file_list` of sample names or identifiers to retain (e.g., filenames or `.obs_names`).\n\n        Args:\n            condition (str, optional): Logical condition string referencing columns in `.summary`.\n            file_list (list of str, optional): List of sample identifiers to keep.\n            exclude_file_list (list of str, optional): Same as file_list, but excludes specified sample identifiers.\n            cleanup (bool): If True (default), remove proteins that become all-NaN or all-zero\n                after sample filtering and synchronize RS/peptide matrices. Set to False to\n                retain all proteins for consistent feature alignment (e.g. during DE analysis).\n            return_copy (bool): If True, returns a filtered pAnnData object. If False, modifies in place.\n            debug (bool): If True, prints the query string or filtering summary.\n\n        Returns:\n            pAnnData: Filtered pAnnData object if `return_copy=True`; otherwise, modifies in place and returns None.\n\n        Note:\n            This method is intended for internal use by `filter_sample()`. For general-purpose filtering, \n            use `filter_sample()` with `condition=...` or `file_list=...`.\n\n        Examples:\n            Filter samples with more than 1000 proteins:\n                ```python\n                pdata.filter_sample_condition(condition=\"protein_count &gt; 1000\")\n                ```\n\n            Keep only specific sample files:\n                ```python\n                pdata.filter_sample_condition(file_list=['fileA', 'fileB'])\n                ```\n\n            Exclude specific files from the dataset:\n                ```python\n                pdata.filter_sample(exclude_file_list=['Sample_001', 'Sample_007'])\n                ```\n        \"\"\"\n        if not self._has_data(): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if self._summary is None: # type: ignore[attr-defined]\n            self.update_summary(recompute=True) # type: ignore[attr-defined], SummaryMixin\n\n        if file_list is not None and exclude_file_list is not None:\n            raise ValueError(\n                \"You cannot specify both `file_list` and `exclude_file_list` simultaneously.\\n\"\n                \"Please use only one mode per call:\\n\"\n                \"  \u2022 `file_list=[...]` to keep only these samples, or\\n\"\n                \"  \u2022 `exclude_file_list=[...]` to remove these samples.\\n\"\n                \"If you need both operations, call `filter_sample()` twice in sequence.\"\n            )\n\n        # Determine whether to operate on a copy or in-place\n        pdata = self.copy() if return_copy else self # type: ignore[attr-defined], EditingMixin\n        action = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n\n        orig_sample_count = len(pdata.prot.obs)\n\n        if debug:\n            print(\"self.prot id:\", id(self.prot))\n            print(\"pdata.prot id:\", id(pdata.prot))\n            print(\"Length of pdata.prot.obs_names:\", len(pdata.prot.obs_names))\n\n        # Determine sample indices to retain\n        index_filter = None\n        missing = []\n\n        if condition is not None:\n            formatted_condition = self._format_filter_query(condition, pdata._summary)  # type: ignore[attr-defined]\n            if debug:\n                print(formatted_condition)\n            index_filter = pdata._summary[pdata._summary.eval(formatted_condition)].index\n        elif file_list is not None or exclude_file_list is not None:\n            all_samples = set(pdata.prot.obs_names)\n\n            if file_list is not None:\n                requested = set(file_list)\n                missing = list(requested - all_samples)\n                index_filter = list(all_samples.intersection(requested))\n                mode_str = \"including\"\n                exclude_mode = False\n\n            else:  # exclude_file_list is not None\n                requested = set(exclude_file_list)\n                missing = list(requested - all_samples)\n                index_filter = list(all_samples - requested)\n                mode_str = \"excluding\"\n                exclude_mode = True\n\n            if missing:\n                warnings.warn(f\"Some sample IDs not found: {missing}\")\n\n        else:\n            # No filtering applied\n            message = \"No filtering applied. Returning original data.\"\n            return pdata if return_copy else None\n\n        if debug:\n            print(f\"Length of index_filter: {len(index_filter)}\")\n            print(f\"Length of pdata.prot.obs_names before filter: {len(pdata.prot.obs_names)}\")\n            print(f\"Number of shared samples: {len(pdata.prot.obs_names.intersection(index_filter))}\")\n\n        # Filter out selected samples from prot and pep\n        if pdata.prot is not None:\n            pdata.prot = pdata.prot[pdata.prot.obs.index.isin(index_filter)]\n\n        if pdata.pep is not None:\n            pdata.pep = pdata.pep[pdata.pep.obs.index.isin(index_filter)]\n\n        if cleanup:\n            cleanup_message = pdata._cleanup_proteins_after_sample_filter(verbose=True)\n        else:\n            cleanup_message = None\n        pdata.update_summary(recompute=False, verbose=False) # type: ignore[attr-defined], SummaryMixin\n\n        print(f\"Length of pdata.prot.obs_names after filter: {len(pdata.prot.obs_names)}\") if debug else None\n\n        # Construct formatted message\n        filter_type = \"condition\" if condition else \"file list\" if (file_list or exclude_file_list) else \"none\"\n        log_prefix = format_log_prefix(\"user\")\n\n        if len(index_filter) == 0:\n            message = f\"{log_prefix} Filtering samples [{filter_type}]:\\n    \u2192 No matching samples found. No filtering applied.\"\n        else:\n            message = f\"{log_prefix} Filtering samples [{filter_type}]:\\n\"\n            message += f\"    {action} sample data based on {filter_type}:\\n\"\n            if condition:\n                message += f\"{format_log_prefix('filter_conditions')}Condition: {condition}\\n\"\n            elif file_list or exclude_file_list:\n                flist = file_list if file_list is not None else exclude_file_list\n                message += f\"{format_log_prefix('filter_conditions')}Files requested ({mode_str}): {len(flist)}\\n\"\n                if missing:\n                    message += f\"{format_log_prefix('filter_conditions')}Missing samples ignored: {len(missing)}\\n\"\n\n            message += cleanup_message + \"\\n\" if cleanup_message else \"\"\n            message += f\"    \u2192 Samples kept: {len(pdata.prot.obs)}, Samples dropped: {orig_sample_count - len(pdata.prot.obs)}\"\n            message += f\"\\n    \u2192 Proteins kept: {len(pdata.prot.var)}\\n\"\n\n        # Logging and history updates\n        print(message)\n        pdata._append_history(message) # type: ignore[attr-defined], HistoryMixin\n\n        return pdata if return_copy else None\n\n    def _filter_sample_values(self, values, exact_cases, cleanup=True, verbose=True, debug=False, return_copy=True):\n        \"\"\"\n        Filter samples using dictionary-style categorical matching.\n\n        This internal method filters samples based on class-like annotations (e.g., treatment, cellline),\n        using either loose field-wise filtering or strict combination matching. It supports:\n\n        - Single dictionary (e.g., `{'cellline': 'A'}`)\n        - List of dictionaries (e.g., `[{...}, {...}]` for multiple matching cases)\n        - Exact matching (`exact_cases=True`) across all key\u2013value pairs\n\n        Args:\n            values (dict or list of dict): Filtering conditions.\n                - If `exact_cases=False`: A single dictionary with field: list of values. \n                Applies OR logic within fields and AND logic across fields.\n                - If `exact_cases=True`: A list of dictionaries, each representing an exact combination of field values.\n            exact_cases (bool): If True, performs exact match filtering using the provided list of dictionaries.\n            cleanup (bool): If True (default), remove proteins that become all-NaN or all-zero\n                after sample filtering and synchronize RS/peptide matrices. Set to False to\n                retain all proteins for consistent feature alignment (e.g. during DE analysis).\n            verbose (bool): If True, prints a summary of the filtering result.\n            debug (bool): If True, prints internal queries and matching logic.\n            return_copy (bool): If True, returns a filtered copy. Otherwise modifies in place.\n\n        Returns:\n            pAnnData: Filtered view of the input AnnData object if `return_copy=True`; otherwise modifies in place and returns None.\n\n        Note:\n            This method is used internally by `filter_sample()`. For general use, call `filter_sample()` directly.\n\n        Examples:\n            Loose field-wise match (OR within fields, AND across fields):\n                ```python\n                pdata.filter_sample_values(values={'treatment': ['kd', 'sc'], 'cellline': 'A'})\n                ```\n\n            Exact combination matching:\n                ```python\n                pdata.filter_sample_values(\n                    values=[\n                        {'treatment': 'kd', 'cellline': 'A'},\n                        {'treatment': 'sc', 'cellline': 'B'}\n                    ],\n                    exact_cases=True\n                )\n                ```\n        \"\"\"\n\n        pdata = self.copy() if return_copy else self # type: ignore[attr-defined], EditingMixin\n        obs_keys = pdata.summary.columns # type: ignore[attr-defined]\n        orig_sample_count = len(pdata.prot.obs)\n\n        if exact_cases:\n            if not isinstance(values, list) or not all(isinstance(v, dict) for v in values):\n                raise ValueError(\"When exact_cases=True, `values` must be a list of dictionaries.\")\n\n            for case in values:\n                if not case:\n                    raise ValueError(\"Empty dictionary found in values.\")\n                for key in case:\n                    if key not in obs_keys:\n                        raise ValueError(f\"Field '{key}' not found in adata.obs.\")\n\n            query = \" | \".join([\n                \" &amp; \".join([\n                    f\"(adata.obs['{k}'] == '{v}')\" for k, v in case.items()\n                ])\n                for case in values\n            ])\n\n        else:\n            if not isinstance(values, dict):\n                raise ValueError(\"When exact_cases=False, `values` must be a dictionary.\")\n\n            for key in values:\n                if key not in obs_keys:\n                    raise ValueError(f\"Field '{key}' not found in adata.obs.\")\n\n            query_parts = []\n            for k, v in values.items():\n                v_list = v if isinstance(v, list) else [v]\n                part = \" | \".join([f\"(adata.obs['{k}'] == '{val}')\" for val in v_list])\n                query_parts.append(f\"({part})\")\n            query = \" &amp; \".join(query_parts)\n\n        if debug:\n                print(f\"Filter query: {query}\")\n\n        if pdata.prot is not None:\n            adata = pdata.prot\n            pdata.prot = adata[eval(query)]\n        if pdata.pep is not None:\n            adata = pdata.pep\n            pdata.pep = adata[eval(query)]\n\n        if cleanup:\n            cleanup_message = pdata._cleanup_proteins_after_sample_filter(verbose=True)\n        else:\n            cleanup_message = None\n        pdata.update_summary(recompute=False, verbose=False) # type: ignore[attr-defined], SummaryMixin\n\n        n_samples = len(pdata.prot)\n        log_prefix = format_log_prefix(\"user\")\n        filter_mode = \"exact match\" if exact_cases else \"class match\"\n\n        if n_samples == 0:\n            message = (\n                f\"{log_prefix} Filtering samples [{filter_mode}]:\\n\"\n                f\"    \u2192 No matching samples found. No filtering applied.\"\n            )\n        else:\n            message = (\n                f\"{log_prefix} Filtering samples [{filter_mode}]:\\n\"\n                f\"    {'Returning a copy of' if return_copy else 'Filtered and modified'} sample data based on {filter_mode}:\\n\"\n            )\n\n            if exact_cases:\n                message += f\"{format_log_prefix('filter_conditions')}Matching any of the following cases:\\n\"\n                for i, case in enumerate(values, 1):\n                    message += f\"       {i}. {case}\\n\"\n            else:\n                message += \"   \ud83d\udd38 Match samples where:\\n\"\n                for k, v in values.items():\n                    valstr = v if isinstance(v, str) else \", \".join(map(str, v))\n                    message += f\"      - {k}: {valstr}\\n\"\n\n            message += cleanup_message + \"\\n\" if cleanup_message else \"\"\n            message += f\"    \u2192 Samples kept: {n_samples}, Samples dropped: {orig_sample_count - n_samples}\"\n            message += f\"\\n    \u2192 Proteins kept: {len(pdata.prot.var)}\\n\"\n\n        print(message) if verbose else None\n        pdata._append_history(message) # type: ignore[attr-defined], HistoryMixin\n\n        return pdata\n\n    def _filter_sample_query(self, query_string, source='obs', cleanup=True, return_copy=True, debug=False):\n        \"\"\"\n        Filter samples using a raw pandas-style query string on `.obs` or `.summary`.\n\n        This method allows advanced filtering of samples using logical expressions evaluated \n        directly on the sample metadata.\n\n        Args:\n            query_string (str): A pandas-style query string. \n                Examples: `\"cellline == 'AS' and treatment in ['kd', 'sc']\"`.\n            source (str): The metadata source to query \u2014 either `\"obs\"` or `\"summary\"`.\n            cleanup (bool): If True (default), remove proteins that become all-NaN or all-zero\n                after sample filtering and synchronize RS/peptide matrices. Set to False to\n                retain all proteins for consistent feature alignment (e.g. during DE analysis).\n            return_copy (bool): If True, returns a filtered pAnnData object; otherwise modifies in place.\n            debug (bool): If True, prints the parsed query and debug messages.\n\n        Returns:\n            pAnnData: Filtered pAnnData object if `return_copy=True`; otherwise, modifies in place and returns None.\n        \"\"\"\n        pdata = self.copy() if return_copy else self # type: ignore[attr-defined], EditingMixin\n        action = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n        orig_sample_count = len(pdata.prot.obs)\n\n        print(f\"{format_log_prefix('warn',indent=1)} Advanced query mode enabled \u2014 interpreting string as a pandas-style expression.\")\n\n        if source == 'obs':\n            df = pdata.prot.obs\n        elif source == 'summary':\n            if self._summary is None: # type: ignore[attr-defined]\n                self.update_summary(recompute=True) # type: ignore[attr-defined], SummaryMixin\n            df = pdata._summary # type: ignore[attr-defined]\n        else:\n            raise ValueError(\"source must be 'obs' or 'summary'\")\n\n        try:\n            filtered_df = df.query(query_string)\n        except Exception as e:\n            raise ValueError(f\"Failed to parse query string:\\n  {query_string}\\nError: {e}\")\n\n        index_filter = filtered_df.index\n\n        if pdata.prot is not None:\n            pdata.prot = pdata.prot[pdata.prot.obs_names.isin(index_filter)]\n        if pdata.pep is not None:\n            pdata.pep = pdata.pep[pdata.pep.obs_names.isin(index_filter)]\n\n        if cleanup:\n            cleanup_message = pdata._cleanup_proteins_after_sample_filter(verbose=True)\n        else:\n            cleanup_message = None\n        pdata.update_summary(recompute=False, verbose=False) # type: ignore[attr-defined], SummaryMixin\n\n        n_samples = len(pdata.prot)\n        log_prefix = format_log_prefix(\"user\")\n        action = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n\n        message = (\n            f\"{log_prefix} Filtering samples [query]:\\n\"\n            f\"    {action} sample data based on query string:\\n\"\n            f\"   \ud83d\udd38 Query: {query_string}\\n\"\n        )\n\n        if cleanup_message:\n            message += f\"{cleanup_message}\\n\"\n\n        message += (\n            f\"    \u2192 Samples kept: {n_samples}, Samples dropped: {orig_sample_count - n_samples}\\n\"\n            f\"    \u2192 Proteins kept: {len(pdata.prot.var)}\\n\"\n        )\n\n        print(message)\n\n        history_message = f\"{action} samples based on query string. Samples kept: {len(index_filter)}.\"\n        pdata._append_history(history_message) # type: ignore[attr-defined], HistoryMixin\n\n        return pdata if return_copy else None\n\n    def _cleanup_proteins_after_sample_filter(self, verbose=True, printout=False):\n        \"\"\"\n        Internal helper to remove proteins that became all-NaN or all-zero\n        after sample filtering. Called silently by `filter_sample()`.\n\n        Ensures the RS matrix and peptide table are synchronized after cleanup.\n        Called automatically by `filter_sample()` and during import.        \n\n        Args:\n            verbose (bool): If True, returns a cleanup summary message.\n                            If False, runs silently.\n            printout (bool): If True and verbose=True, directly prints the cleanup mesage.\n\n        Returns:\n            str or None: Cleanup message if verbose=True and any proteins removed,\n                        otherwise None.\n        \"\"\"\n        from scipy.sparse import issparse\n\n        if not self._check_data(\"protein\"):  # type: ignore[attr-defined]\n            return\n\n        X = self.prot.X.toarray() if issparse(self.prot.X) else self.prot.X\n        original_var_names = self.prot.var_names.copy()\n        all_nan = np.all(np.isnan(X), axis=0)\n        all_zero = np.all(X == 0, axis=0)\n        remove_mask = all_nan | all_zero\n\n        if not remove_mask.any():\n            if verbose:\n                return f\"{format_log_prefix('info_only',2)} Auto-cleanup: No empty proteins found (all-NaN or all-zero).\"\n            else:\n                return None\n\n        n_remove = int(remove_mask.sum())\n        keep_mask = ~remove_mask\n\n        # skip cleanup entirely if no samples or no protein data remain\n        if self.prot is None or self.prot.n_obs == 0 or self.prot.n_vars == 0:\n            if verbose:\n                print(f\"{format_log_prefix('warn_only',2)} No samples or proteins to clean up. Skipping RS sync.\")\n            return None\n\n        # Backup original for RS/peptide syncing, ensure summary and obs are aligned before making copy\n        if self._summary is not None and not self.prot.obs.index.equals(self._summary.index):\n            self._summary = self._summary.loc[self.prot.obs.index].copy()\n        original = self.copy()\n        # Filter protein data\n        self.prot = self.prot[:, keep_mask].copy()\n\n        if self.pep is not None and self.rs is not None:\n            proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = self._filter_sync_peptides_to_proteins(\n                original=original,\n                updated_prot=self.prot,\n                debug=False\n            )\n\n            self._apply_rs_filter(\n                keep_proteins=proteins_to_keep,\n                keep_peptides=peptides_to_keep,\n                orig_prot_names=orig_prot_names,\n                orig_pep_names=orig_pep_names,\n                debug=False\n            )\n\n        self.update_summary(recompute=True, verbose=False)\n\n        if verbose:            \n            removed_proteins = list(original_var_names[remove_mask])\n            preview = \", \".join(removed_proteins[:10])\n            if n_remove &gt; 10:\n                preview += \", ...\"\n\n            if printout and n_remove&gt;0:\n                # for startup\n                print(f\"{format_log_prefix('info_only',1)} Removed {n_remove} empty proteins (all-NaN or all-zero). Proteins: {preview}\")\n            else:\n                # for filter\n                return(f\"{format_log_prefix('info_only',2)} Auto-cleanup: Removed {n_remove} empty proteins (all-NaN or all-zero). Proteins: {preview}\")\n\n        return None\n\n    def filter_rs(\n        self,\n        min_peptides_per_protein=None,\n        min_unique_peptides_per_protein=2,\n        max_proteins_per_peptide=None,\n        return_copy=True,\n        preset=None,\n        validate_after=True\n    ):\n        \"\"\"\n        Filter the RS matrix and associated `.prot` and `.pep` data based on peptide-protein relationships.\n\n        This method applies rules for keeping proteins with sufficient peptide evidence and/or removing\n        ambiguous peptides. It also updates internal mappings accordingly.\n\n        Args:\n            min_peptides_per_protein (int, optional): Minimum number of total peptides required per protein.\n            min_unique_peptides_per_protein (int, optional): Minimum number of unique peptides required per protein \n                (default is 2).\n            max_proteins_per_peptide (int, optional): Maximum number of proteins a peptide can map to; peptides \n                exceeding this will be removed.\n            return_copy (bool): If True (default), returns a filtered pAnnData object. If False, modifies in place.\n            preset (str or dict, optional): Predefined filter presets:\n                - `\"default\"` \u2192 unique peptides \u2265 2\n                - `\"lenient\"` \u2192 total peptides \u2265 2\n                - A dictionary specifying filter thresholds manually.\n            validate_after (bool): If True (default), calls `self.validate()` after filtering.\n\n        Returns:\n            pAnnData: Filtered pAnnData object if `return_copy=True`; otherwise, modifies in place and returns None.\n\n        Note:\n            Stores filter metadata in `.prot.uns['filter_rs']`, including indices of proteins/peptides kept and \n            filtering summary.\n        \"\"\"\n        if self.rs is None: # type: ignore[attr-defined]\n            print(\"\u26a0\ufe0f No RS matrix to filter.\")\n            return self if return_copy else None\n\n        # --- Apply preset if given ---\n        if preset:\n            if preset == \"default\":\n                min_peptides_per_protein = None\n                min_unique_peptides_per_protein = 2\n                max_proteins_per_peptide = None\n            elif preset == \"lenient\":\n                min_peptides_per_protein = 2\n                min_unique_peptides_per_protein = None\n                max_proteins_per_peptide = None\n            elif isinstance(preset, dict):\n                min_peptides_per_protein = preset.get(\"min_peptides_per_protein\", min_peptides_per_protein)\n                min_unique_peptides_per_protein = preset.get(\"min_unique_peptides_per_protein\", min_unique_peptides_per_protein)\n                max_proteins_per_peptide = preset.get(\"max_proteins_per_peptide\", max_proteins_per_peptide)\n            else:\n                raise ValueError(f\"Unknown RS filtering preset: {preset}\")\n\n        pdata = self.copy() if return_copy else self # type: ignore[attr-defined], EditingMixin\n\n        rs = pdata.rs # type: ignore[attr-defined]\n\n        # --- Step 1: Peptide filter (max proteins per peptide) ---\n        if max_proteins_per_peptide is not None:\n            peptide_links = rs.getnnz(axis=0)\n            keep_peptides = peptide_links &lt;= max_proteins_per_peptide\n            rs = rs[:, keep_peptides]\n        else:\n            keep_peptides = np.ones(rs.shape[1], dtype=bool)\n\n        # --- Step 2: Protein filters ---\n        is_unique = rs.getnnz(axis=0) == 1\n        unique_counts = rs[:, is_unique].getnnz(axis=1)\n        peptide_counts = rs.getnnz(axis=1)\n\n        keep_proteins = np.ones(rs.shape[0], dtype=bool)\n        if min_peptides_per_protein is not None:\n            keep_proteins &amp;= (peptide_counts &gt;= min_peptides_per_protein)\n        if min_unique_peptides_per_protein is not None:\n            keep_proteins &amp;= (unique_counts &gt;= min_unique_peptides_per_protein)\n\n        rs_filtered = rs[keep_proteins, :]\n\n        # --- Step 3: Re-filter peptides now unmapped ---\n        keep_peptides_final = rs_filtered.getnnz(axis=0) &gt; 0\n        rs_filtered = rs_filtered[:, keep_peptides_final]\n\n        # --- Apply filtered RS ---\n        pdata._set_RS(rs_filtered, validate=False) # type: ignore[attr-defined], EditingMixin\n\n        # --- Filter .prot and .pep ---\n        if pdata.prot is not None:\n            pdata.prot = pdata.prot[:, keep_proteins]\n        if pdata.pep is not None:\n            original_peptides = keep_peptides.nonzero()[0]\n            final_peptides = original_peptides[keep_peptides_final]\n            pdata.pep = pdata.pep[:, final_peptides]\n\n        # --- History and summary ---\n        n_prot_before = self.prot.shape[1] if self.prot is not None else rs.shape[0]\n        n_pep_before = self.pep.shape[1] if self.pep is not None else rs.shape[1]\n        n_prot_after = rs_filtered.shape[0]\n        n_pep_after = rs_filtered.shape[1]\n\n        n_prot_dropped = n_prot_before - n_prot_after\n        n_pep_dropped = n_pep_before - n_pep_after\n\n        msg = \"\ud83e\uddea Filtered RS\"\n        if preset:\n            msg += f\" using preset '{preset}'\"\n        if min_peptides_per_protein is not None:\n            msg += f\", min peptides per protein: {min_peptides_per_protein}\"\n        if min_unique_peptides_per_protein is not None:\n            msg += f\", min unique peptides: {min_unique_peptides_per_protein}\"\n        if max_proteins_per_peptide is not None:\n            msg += f\", max proteins per peptide: {max_proteins_per_peptide}\"\n        msg += (\n            f\". Proteins: {n_prot_before} \u2192 {n_prot_after} (dropped {n_prot_dropped}), \"\n            f\"Peptides: {n_pep_before} \u2192 {n_pep_after} (dropped {n_pep_dropped}).\"\n        )\n\n        pdata._append_history(msg) # type: ignore[attr-defined], HistoryMixin\n        print(msg)\n        pdata.update_summary() # type: ignore[attr-defined], SummaryMixin\n\n        # --- Save filter indices to .uns ---\n        protein_indices = list(pdata.prot.var_names) if pdata.prot is not None else []\n        peptide_indices = list(pdata.pep.var_names) if pdata.pep is not None else []\n        pdata.prot.uns['filter_rs'] = {\n            \"kept_proteins\": protein_indices,\n            \"kept_peptides\": peptide_indices,\n            \"n_proteins\": len(protein_indices),\n            \"n_peptides\": len(peptide_indices),\n            \"description\": msg\n        }\n\n        if validate_after:\n            pdata.validate(verbose=True) # type: ignore[attr-defined], ValidationMixin\n\n        return pdata if return_copy else None\n\n    def _apply_rs_filter(\n        self,\n        keep_proteins=None,\n        keep_peptides=None,\n        orig_prot_names=None,\n        orig_pep_names=None,\n        debug=True\n    ):\n        \"\"\"\n        Apply filtering to `.prot`, `.pep`, and `.rs` based on protein/peptide masks or name lists.\n\n        This method filters the relational structure (RS matrix) and associated data objects by retaining\n        only the specified proteins and/or peptides. Original axis names can be provided to ensure correct\n        alignment after prior filtering steps.\n\n        Args:\n            keep_proteins (list or np.ndarray or bool array, optional): List of protein names or boolean mask \n                indicating which proteins (RS matrix rows) to keep.\n            keep_peptides (list or np.ndarray or bool array, optional): List of peptide names or boolean mask \n                indicating which peptides (RS matrix columns) to keep.\n            orig_prot_names (list or np.ndarray, optional): Original protein names corresponding to RS matrix rows.\n            orig_pep_names (list or np.ndarray, optional): Original peptide names corresponding to RS matrix columns.\n            debug (bool): If True, prints filtering details and index alignment diagnostics.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.rs is None: # type: ignore[attr-defined]\n            raise ValueError(\"No RS matrix to filter.\")\n\n        from scipy.sparse import issparse\n\n        rs = self.rs # type: ignore[attr-defined]\n\n        # Use provided names or fallback to current .prot/.pep\n        prot_names = np.array(orig_prot_names) if orig_prot_names is not None else np.array(self.prot.var_names)\n        pep_names = np.array(orig_pep_names) if orig_pep_names is not None else np.array(self.pep.var_names)\n\n        if rs.shape[0] != len(prot_names) or rs.shape[1] != len(pep_names):\n            raise ValueError(\n                f\"RS shape {rs.shape} does not match provided protein/peptide names \"\n                f\"({len(prot_names)} proteins, {len(pep_names)} peptides). \"\n                \"Did you forget to pass the original names?\"\n            )\n\n        # --- Normalize protein mask ---\n        if keep_proteins is None:\n            prot_mask = np.ones(rs.shape[0], dtype=bool)\n        elif isinstance(keep_proteins, (list, np.ndarray, pd.Index)) and isinstance(keep_proteins[0], str):\n            keep_set = set(keep_proteins)\n            prot_mask = np.fromiter((p in keep_set for p in prot_names), dtype=bool)\n        elif isinstance(keep_proteins, (list, np.ndarray)) and isinstance(keep_proteins[0], (bool, np.bool_)):\n            prot_mask = np.asarray(keep_proteins)\n        else:\n            raise TypeError(\"keep_proteins must be a list of str or a boolean mask.\")\n\n        # --- Normalize peptide mask ---\n        if keep_peptides is None:\n            pep_mask = np.ones(rs.shape[1], dtype=bool)\n        elif isinstance(keep_peptides, (list, np.ndarray, pd.Index)) and isinstance(keep_peptides[0], str):\n            keep_set = set(keep_peptides)\n            pep_mask = np.fromiter((p in keep_set for p in pep_names), dtype=bool)\n        elif isinstance(keep_peptides, (list, np.ndarray)) and isinstance(keep_peptides[0], (bool, np.bool_)):\n            pep_mask = np.asarray(keep_peptides)\n        else:\n            raise TypeError(\"keep_peptides must be a list of str or a boolean mask.\")\n\n        # --- Final safety check ---\n        if len(prot_mask) != rs.shape[0] or len(pep_mask) != rs.shape[1]:\n            raise ValueError(\"Mismatch between mask lengths and RS matrix dimensions.\")\n\n        # --- Apply to RS ---\n        self._set_RS(rs[prot_mask, :][:, pep_mask], validate=False) # type: ignore[attr-defined], EditingMixin\n\n        # --- Apply to .prot and .pep ---\n        kept_prot_names = np.array(orig_prot_names)[prot_mask]\n        kept_pep_names = np.array(orig_pep_names)[pep_mask]\n\n        if self.prot is not None:\n            self.prot = self.prot[:, self.prot.var_names.isin(kept_prot_names)]\n\n        if self.pep is not None:\n            self.pep = self.pep[:, self.pep.var_names.isin(kept_pep_names)]\n\n        if debug:\n            print(f\"{format_log_prefix('result')} RS matrix filtered: {prot_mask.sum()} proteins, {pep_mask.sum()} peptides retained.\")\n\n    def _format_filter_query(self, condition, dataframe):\n        \"\"\"\n        Format a condition string for safe evaluation on a DataFrame with complex column names.\n\n        This method prepares a query string for use with `pandas.eval()` or `.query()` by:\n        - Wrapping column names containing spaces or special characters in backticks.\n        - Converting custom `includes` syntax (for substring matching) into `.str.contains(...)` expressions.\n        - Quoting unquoted string values automatically for object/category columns.\n\n        Args:\n            condition (str): The condition string to parse and format.\n            dataframe (pd.DataFrame): The DataFrame whose column names and dtypes are used for formatting.\n\n        Returns:\n            str: A cleaned and properly formatted condition string suitable for `.eval()` or `.query()`.\n\n        Note:\n            This is an internal helper used by methods such as `filter_sample_condition()` and `filter_prot()` \n            to support user-friendly string-based queries.\n        \"\"\"\n\n        # Wrap column names with backticks if needed\n        column_names = dataframe.columns.tolist()\n        column_names.sort(key=len, reverse=True) # Avoid partial matches\n\n        for col in column_names:\n            if re.search(r'[^\\\\w]', col):  # Non-alphanumeric characters\n                condition = re.sub(fr'(?&lt;!`)({re.escape(col)})(?!`)', f'`{col}`', condition)\n\n        # Handle 'includes' syntax for substring matching\n        match = re.search(r'`?(\\w[\\w\\s:.-]*)`?\\s+includes\\s+[\\'\"]([^\\'\"]+)[\\'\"]', condition)\n        if match:\n            col_name = match.group(1)\n            substring = match.group(2)\n            condition = f\"{col_name}.str.contains('{substring}', case=False, na=False)\"\n\n        # Auto-quote string values for categorical/text columns\n        for col in dataframe.columns:\n            if dataframe[col].dtype.name in [\"object\", \"category\"]:\n                for op in [\"==\", \"!=\"]:\n                    pattern = fr\"(?&lt;![&gt;&lt;=!])\\b{re.escape(col)}\\s*{op}\\s*([^\\s'\\\"()]+)\"\n                    matches = re.findall(pattern, condition)\n                    for match_val in matches:\n                        quoted_val = f'\"{match_val}\"'\n                        condition = re.sub(fr\"({re.escape(col)}\\s*{op}\\s*){match_val}\\b\", r\"\\1\" + quoted_val, condition)\n\n        return condition\n\n    def _annotate_found_samples(self, threshold=0.0, layer='X'):\n        \"\"\"\n        Annotate proteins and peptides with per-sample 'Found In' flags.\n\n        For each sample, this method adds boolean indicators to `.prot.var` and `.pep.var` \n        indicating whether the feature was detected (i.e. exceeds the given threshold).\n\n        Args:\n            threshold (float): Minimum value to consider a feature as \"found\" (default is 0.0).\n            layer (str): Name of the data layer to evaluate (e.g., \"X\", \"imputed\", etc.).\n\n        Returns:\n            None\n\n        Note:\n            This is an internal helper used to support downstream grouping-based detection \n            (e.g., in `annotate_found_in()`).\n            Adds new columns to `.var` of the form: `'Found In: &lt;sample&gt;' \u2192 bool`.\n        \"\"\"\n        for level in ['prot', 'pep']:\n            adata = getattr(self, level)\n            # Skip if the level doesn't exist\n            if adata is None:\n                continue\n\n            # Handle layer selection\n            if layer == 'X':\n                data = pd.DataFrame(adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X,\n                                    index=adata.obs_names,\n                                    columns=adata.var_names).T\n            elif layer in adata.layers:\n                data = pd.DataFrame(adata.layers[layer].toarray() if hasattr(adata.layers[layer], 'toarray') else adata.layers[layer],\n                                    index=adata.obs_names,\n                                    columns=adata.var_names).T\n            else:\n                raise KeyError(f\"Layer '{layer}' not found in {level}.layers and is not 'X'.\")\n\n            found = data &gt; threshold\n            for sample in found.columns:\n                adata.var[f\"Found In: {sample}\"] = found[sample]\n\n    def annotate_found(self, classes=None, on='protein', layer='X', threshold=0.0, indent=1, verbose=True):\n        \"\"\"\n        Add group-level 'Found In' annotations for proteins or peptides.\n\n        This method computes detection flags across groups of samples, based on a minimum intensity \n        threshold, and stores them in `.prot.var` or `.pep.var` as new boolean columns.\n\n        Args:\n            classes (str or list of str): Sample-level class/grouping column(s) in `.sample.obs`.\n            on (str): Whether to annotate 'protein' or 'peptide' level features.\n            layer (str): Data layer to use for evaluation (default is \"X\").\n            threshold (float): Minimum intensity value to be considered \"found\".\n            indent (int): Indentation level for printed messages.\n            verbose (bool): If True, prints a summary message after annotation.\n\n        Returns:\n            None\n\n        Examples:\n            Annotate proteins by group using sample-level metadata:\n                ```python\n                pdata.annotate_found(classes=[\"group\", \"condition\"], on=\"protein\")\n                ```\n\n            Filter for proteins found in at least 20% of samples from a given group:\n                ```python\n                pdata_filtered = pdata.filter_prot_found(group=\"Group_A\", min_ratio=0.2)\n                pdata_filtered.prot.var\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            return\n\n        adata = self.prot if on == 'protein' else self.pep\n        var = adata.var\n\n        # Handle layer correctly (supports 'X' or adata.layers keys)\n        if layer == 'X':\n            data = pd.DataFrame(adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X,\n                                index=adata.obs_names,\n                                columns=adata.var_names).T\n        elif layer in adata.layers:\n            raw = adata.layers[layer]\n            data = pd.DataFrame(raw.toarray() if hasattr(raw, 'toarray') else raw,\n                                index=adata.obs_names,\n                                columns=adata.var_names).T\n        else:\n            raise KeyError(f\"Layer '{layer}' not found in {on}.layers and is not 'X'.\")\n\n        found_df = data &gt; threshold\n\n        # Prepare or retrieve existing numeric storage in .uns\n        metrics_key = f\"found_metrics_{on}\"\n        metrics_df = adata.uns.get(metrics_key, pd.DataFrame(index=adata.var_names))\n\n        if classes is not None:\n            classes_list = utils.get_classlist(adata, classes=classes)\n\n            for class_value in classes_list:\n                class_data = utils.resolve_class_filter(adata, classes, class_value)\n                class_samples = class_data.obs_names\n\n                if len(class_samples) == 0:\n                    continue\n\n                sub_found = found_df[class_samples]\n                count = sub_found.sum(axis=1)\n                ratio = count / len(class_samples)\n\n                # Store display-friendly annotations in .var\n                var[f\"Found In: {class_value}\"] = sub_found.any(axis=1)\n                var[f\"Found In: {class_value} ratio\"] = sub_found.sum(axis=1).astype(str) + \"/\" + str(len(class_samples))\n\n                # Store numeric data in .uns\n                metrics_df[(class_value, \"count\")] = count\n                metrics_df[(class_value, \"ratio\")] = ratio\n\n            # Store updated versions back into .uns\n            metrics_df.columns = pd.MultiIndex.from_tuples(metrics_df.columns)\n            metrics_df = metrics_df.sort_index(axis=1)\n            adata.uns[metrics_key] = metrics_df\n\n        self._history.append(  # type: ignore[attr-defined], HistoryMixin\n            f\"{on}: Annotated features 'found in' class combinations {classes} using threshold {threshold}.\"\n        )\n        if verbose:\n            print(\n                f\"{format_log_prefix('user', indent=indent)} Annotated 'found in' features by group: \"\n                f\"{classes} using threshold {threshold}.\"\n            )\n\n    def annotate_significant(self, classes=None, on='protein', fdr_threshold=0.01, indent=1, verbose=True):\n        \"\"\"\n        Add group-level 'Significant In' annotations for proteins or peptides.\n\n        This method computes significance flags (e.g., X_qval &lt; threshold) across groups \n        of samples and stores them in `.prot.var` or `.pep.var` as new boolean columns.\n\n        DIA-NN: protein X_qval originates from PG.Q.Value, peptide X_qval originates from Q.Value.\n\n        Args:\n            classes (str or list of str, optional): Sample-level grouping column(s) for group-based annotations.\n            fdr_threshold (float): Significance threshold (default 0.01).\n            on (str): Level to annotate ('protein' or 'peptide').\n            indent (int): Indentation level for printed messages.\n            verbose (bool): If True, prints a summary message after annotation.\n\n        Returns:\n            None\n\n        Examples:\n            Annotate proteins by group using sample-level metadata:\n                ```python\n                pdata.annotate_significant(classes=\"celltype\", on=\"protein\", fdr_threshold=0.01)\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            return\n\n        adata = self.prot if on == 'protein' else self.pep\n        var = adata.var\n\n        # Check if significance layer exists\n        sig_layer = 'X_qval'\n        if sig_layer not in adata.layers:\n            # Try global q-values as fallback\n            if \"Global_Q_value\" in var.columns:\n                global_mask = var[\"Global_Q_value\"] &lt; fdr_threshold\n                var[\"Significant In: Global\"] = global_mask\n                adata.uns[f\"significance_metrics_{on}\"] = pd.DataFrame(\n                    {\"Global_count\": global_mask.sum(), \"Global_ratio\": global_mask.mean()},\n                    index=[\"Global\"]\n                )\n                if verbose:\n                    print(f\"{format_log_prefix('user', indent=indent)} Annotated global significant features using FDR threshold {fdr_threshold}.\")\n                return\n            else:\n                raise ValueError(\n                    f\"No per-sample layer ('{sig_layer}') or 'Global_Q_value' column found for {on}-level significance.\"\n                )\n\n        data = pd.DataFrame(\n            adata.layers[sig_layer].toarray() if hasattr(adata.layers[sig_layer], 'toarray') else adata.layers[sig_layer],\n            index=adata.obs_names,\n            columns=adata.var_names).T  # shape: features \u00d7 samples\n\n        # Group-level summary\n        metrics_key = f\"significance_metrics_{on}\"\n        metrics_df = adata.uns.get(metrics_key, pd.DataFrame(index=adata.var_names))\n\n        if classes is not None:\n            classes_list = utils.get_classlist(adata, classes=classes)\n\n            for class_value in classes_list:\n                class_data = utils.resolve_class_filter(adata, classes, class_value)\n                class_samples = class_data.obs_names\n\n                if len(class_samples) == 0:\n                    continue\n\n                sub_df = data[class_samples]\n                count = (sub_df &lt; fdr_threshold).sum(axis=1)\n                ratio = count / len(class_samples)\n\n                var[f\"Significant In: {class_value}\"] = (sub_df &lt; fdr_threshold).any(axis=1)\n                var[f\"Significant In: {class_value} ratio\"] = count.astype(str) + \"/\" + str(len(class_samples))\n\n                metrics_df[(class_value, \"count\")] = count\n                metrics_df[(class_value, \"ratio\")] = ratio\n\n            metrics_df.columns = pd.MultiIndex.from_tuples(metrics_df.columns)\n            metrics_df = metrics_df.sort_index(axis=1)\n            adata.uns[metrics_key] = metrics_df\n\n        self._history.append(  # type: ignore[attr-defined], HistoryMixin\n            f\"{on}: Annotated significance across classes {classes} using FDR threshold {fdr_threshold}.\"\n        )\n        if verbose:\n            print(\n                f\"{format_log_prefix('user', indent=indent)} Annotated significant features by group: {classes} using FDR threshold {fdr_threshold}.\"\n            )\n\n    def _annotate_significant_samples(self, fdr_threshold=0.01):\n        \"\"\"\n        Annotate proteins and peptides with per-sample 'Significant In' flags.\n\n        For each sample, this method adds boolean indicators to `.prot.var` and `.pep.var`\n        indicating whether the feature is significantly detected (i.e. FDR &lt; threshold).\n\n        Args:\n            fdr_threshold (float): FDR threshold for significance (default: 0.01).\n\n        Returns:\n            None\n\n        Note:\n            This is an internal helper used to support downstream grouping-based annotation.\n            Adds new columns to `.var` of the form: `'Significant In: &lt;sample&gt;' \u2192 bool`.\n        \"\"\"\n        for level in ['prot', 'pep']:\n            adata = getattr(self, level)\n            if adata is None:\n                continue\n\n            if \"Global_Q_value\" in adata.var.columns:\n                adata.var[\"Significant In: Global\"] = adata.var[\"Global_Q_value\"] &lt; fdr_threshold\n\n            sig_layer = 'X_qval'\n            if sig_layer not in adata.layers:\n                # Fallback to global q-values if available\n                print(f\"{format_log_prefix('info', 2)} Using global q-values for '{level}' significance annotation.\")\n                continue  # Skip if significance data not available\n\n            print(f\"{format_log_prefix('info', 2)} Using sample-specific q-values for '{level}' significance annotation.\")\n\n            data = pd.DataFrame(\n                adata.layers[sig_layer].toarray() if hasattr(adata.layers[sig_layer], 'toarray') else adata.layers[sig_layer],\n                index=adata.obs_names,\n                columns=adata.var_names).T  # features \u00d7 samples\n\n            significant = data &lt; fdr_threshold\n            sig_df = significant.add_prefix(\"Significant In: \")\n            adata.var = pd.concat([adata.var, sig_df], axis=1)\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.filtering.FilterMixin.annotate_found","title":"annotate_found","text":"<pre><code>annotate_found(classes=None, on='protein', layer='X', threshold=0.0, indent=1, verbose=True)\n</code></pre> <p>Add group-level 'Found In' annotations for proteins or peptides.</p> <p>This method computes detection flags across groups of samples, based on a minimum intensity  threshold, and stores them in <code>.prot.var</code> or <code>.pep.var</code> as new boolean columns.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>str or list of str</code> <p>Sample-level class/grouping column(s) in <code>.sample.obs</code>.</p> <code>None</code> <code>on</code> <code>str</code> <p>Whether to annotate 'protein' or 'peptide' level features.</p> <code>'protein'</code> <code>layer</code> <code>str</code> <p>Data layer to use for evaluation (default is \"X\").</p> <code>'X'</code> <code>threshold</code> <code>float</code> <p>Minimum intensity value to be considered \"found\".</p> <code>0.0</code> <code>indent</code> <code>int</code> <p>Indentation level for printed messages.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>If True, prints a summary message after annotation.</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <p>Annotate proteins by group using sample-level metadata:     <pre><code>pdata.annotate_found(classes=[\"group\", \"condition\"], on=\"protein\")\n</code></pre></p> <p>Filter for proteins found in at least 20% of samples from a given group:     <pre><code>pdata_filtered = pdata.filter_prot_found(group=\"Group_A\", min_ratio=0.2)\npdata_filtered.prot.var\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>def annotate_found(self, classes=None, on='protein', layer='X', threshold=0.0, indent=1, verbose=True):\n    \"\"\"\n    Add group-level 'Found In' annotations for proteins or peptides.\n\n    This method computes detection flags across groups of samples, based on a minimum intensity \n    threshold, and stores them in `.prot.var` or `.pep.var` as new boolean columns.\n\n    Args:\n        classes (str or list of str): Sample-level class/grouping column(s) in `.sample.obs`.\n        on (str): Whether to annotate 'protein' or 'peptide' level features.\n        layer (str): Data layer to use for evaluation (default is \"X\").\n        threshold (float): Minimum intensity value to be considered \"found\".\n        indent (int): Indentation level for printed messages.\n        verbose (bool): If True, prints a summary message after annotation.\n\n    Returns:\n        None\n\n    Examples:\n        Annotate proteins by group using sample-level metadata:\n            ```python\n            pdata.annotate_found(classes=[\"group\", \"condition\"], on=\"protein\")\n            ```\n\n        Filter for proteins found in at least 20% of samples from a given group:\n            ```python\n            pdata_filtered = pdata.filter_prot_found(group=\"Group_A\", min_ratio=0.2)\n            pdata_filtered.prot.var\n            ```\n    \"\"\"\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        return\n\n    adata = self.prot if on == 'protein' else self.pep\n    var = adata.var\n\n    # Handle layer correctly (supports 'X' or adata.layers keys)\n    if layer == 'X':\n        data = pd.DataFrame(adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X,\n                            index=adata.obs_names,\n                            columns=adata.var_names).T\n    elif layer in adata.layers:\n        raw = adata.layers[layer]\n        data = pd.DataFrame(raw.toarray() if hasattr(raw, 'toarray') else raw,\n                            index=adata.obs_names,\n                            columns=adata.var_names).T\n    else:\n        raise KeyError(f\"Layer '{layer}' not found in {on}.layers and is not 'X'.\")\n\n    found_df = data &gt; threshold\n\n    # Prepare or retrieve existing numeric storage in .uns\n    metrics_key = f\"found_metrics_{on}\"\n    metrics_df = adata.uns.get(metrics_key, pd.DataFrame(index=adata.var_names))\n\n    if classes is not None:\n        classes_list = utils.get_classlist(adata, classes=classes)\n\n        for class_value in classes_list:\n            class_data = utils.resolve_class_filter(adata, classes, class_value)\n            class_samples = class_data.obs_names\n\n            if len(class_samples) == 0:\n                continue\n\n            sub_found = found_df[class_samples]\n            count = sub_found.sum(axis=1)\n            ratio = count / len(class_samples)\n\n            # Store display-friendly annotations in .var\n            var[f\"Found In: {class_value}\"] = sub_found.any(axis=1)\n            var[f\"Found In: {class_value} ratio\"] = sub_found.sum(axis=1).astype(str) + \"/\" + str(len(class_samples))\n\n            # Store numeric data in .uns\n            metrics_df[(class_value, \"count\")] = count\n            metrics_df[(class_value, \"ratio\")] = ratio\n\n        # Store updated versions back into .uns\n        metrics_df.columns = pd.MultiIndex.from_tuples(metrics_df.columns)\n        metrics_df = metrics_df.sort_index(axis=1)\n        adata.uns[metrics_key] = metrics_df\n\n    self._history.append(  # type: ignore[attr-defined], HistoryMixin\n        f\"{on}: Annotated features 'found in' class combinations {classes} using threshold {threshold}.\"\n    )\n    if verbose:\n        print(\n            f\"{format_log_prefix('user', indent=indent)} Annotated 'found in' features by group: \"\n            f\"{classes} using threshold {threshold}.\"\n        )\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.filtering.FilterMixin.annotate_significant","title":"annotate_significant","text":"<pre><code>annotate_significant(classes=None, on='protein', fdr_threshold=0.01, indent=1, verbose=True)\n</code></pre> <p>Add group-level 'Significant In' annotations for proteins or peptides.</p> <p>This method computes significance flags (e.g., X_qval &lt; threshold) across groups  of samples and stores them in <code>.prot.var</code> or <code>.pep.var</code> as new boolean columns.</p> <p>DIA-NN: protein X_qval originates from PG.Q.Value, peptide X_qval originates from Q.Value.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>str or list of str</code> <p>Sample-level grouping column(s) for group-based annotations.</p> <code>None</code> <code>fdr_threshold</code> <code>float</code> <p>Significance threshold (default 0.01).</p> <code>0.01</code> <code>on</code> <code>str</code> <p>Level to annotate ('protein' or 'peptide').</p> <code>'protein'</code> <code>indent</code> <code>int</code> <p>Indentation level for printed messages.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>If True, prints a summary message after annotation.</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> <p>Examples:</p> <p>Annotate proteins by group using sample-level metadata:     <pre><code>pdata.annotate_significant(classes=\"celltype\", on=\"protein\", fdr_threshold=0.01)\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>def annotate_significant(self, classes=None, on='protein', fdr_threshold=0.01, indent=1, verbose=True):\n    \"\"\"\n    Add group-level 'Significant In' annotations for proteins or peptides.\n\n    This method computes significance flags (e.g., X_qval &lt; threshold) across groups \n    of samples and stores them in `.prot.var` or `.pep.var` as new boolean columns.\n\n    DIA-NN: protein X_qval originates from PG.Q.Value, peptide X_qval originates from Q.Value.\n\n    Args:\n        classes (str or list of str, optional): Sample-level grouping column(s) for group-based annotations.\n        fdr_threshold (float): Significance threshold (default 0.01).\n        on (str): Level to annotate ('protein' or 'peptide').\n        indent (int): Indentation level for printed messages.\n        verbose (bool): If True, prints a summary message after annotation.\n\n    Returns:\n        None\n\n    Examples:\n        Annotate proteins by group using sample-level metadata:\n            ```python\n            pdata.annotate_significant(classes=\"celltype\", on=\"protein\", fdr_threshold=0.01)\n            ```\n    \"\"\"\n    if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n        return\n\n    adata = self.prot if on == 'protein' else self.pep\n    var = adata.var\n\n    # Check if significance layer exists\n    sig_layer = 'X_qval'\n    if sig_layer not in adata.layers:\n        # Try global q-values as fallback\n        if \"Global_Q_value\" in var.columns:\n            global_mask = var[\"Global_Q_value\"] &lt; fdr_threshold\n            var[\"Significant In: Global\"] = global_mask\n            adata.uns[f\"significance_metrics_{on}\"] = pd.DataFrame(\n                {\"Global_count\": global_mask.sum(), \"Global_ratio\": global_mask.mean()},\n                index=[\"Global\"]\n            )\n            if verbose:\n                print(f\"{format_log_prefix('user', indent=indent)} Annotated global significant features using FDR threshold {fdr_threshold}.\")\n            return\n        else:\n            raise ValueError(\n                f\"No per-sample layer ('{sig_layer}') or 'Global_Q_value' column found for {on}-level significance.\"\n            )\n\n    data = pd.DataFrame(\n        adata.layers[sig_layer].toarray() if hasattr(adata.layers[sig_layer], 'toarray') else adata.layers[sig_layer],\n        index=adata.obs_names,\n        columns=adata.var_names).T  # shape: features \u00d7 samples\n\n    # Group-level summary\n    metrics_key = f\"significance_metrics_{on}\"\n    metrics_df = adata.uns.get(metrics_key, pd.DataFrame(index=adata.var_names))\n\n    if classes is not None:\n        classes_list = utils.get_classlist(adata, classes=classes)\n\n        for class_value in classes_list:\n            class_data = utils.resolve_class_filter(adata, classes, class_value)\n            class_samples = class_data.obs_names\n\n            if len(class_samples) == 0:\n                continue\n\n            sub_df = data[class_samples]\n            count = (sub_df &lt; fdr_threshold).sum(axis=1)\n            ratio = count / len(class_samples)\n\n            var[f\"Significant In: {class_value}\"] = (sub_df &lt; fdr_threshold).any(axis=1)\n            var[f\"Significant In: {class_value} ratio\"] = count.astype(str) + \"/\" + str(len(class_samples))\n\n            metrics_df[(class_value, \"count\")] = count\n            metrics_df[(class_value, \"ratio\")] = ratio\n\n        metrics_df.columns = pd.MultiIndex.from_tuples(metrics_df.columns)\n        metrics_df = metrics_df.sort_index(axis=1)\n        adata.uns[metrics_key] = metrics_df\n\n    self._history.append(  # type: ignore[attr-defined], HistoryMixin\n        f\"{on}: Annotated significance across classes {classes} using FDR threshold {fdr_threshold}.\"\n    )\n    if verbose:\n        print(\n            f\"{format_log_prefix('user', indent=indent)} Annotated significant features by group: {classes} using FDR threshold {fdr_threshold}.\"\n        )\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.filtering.FilterMixin.filter_prot","title":"filter_prot","text":"<pre><code>filter_prot(condition=None, accessions=None, valid_genes=False, unique_profiles=False, return_copy=True, debug=False)\n</code></pre> <p>Filter protein data based on metadata conditions or accession list (protein name and gene name).</p> <p>This method filters the protein-level data either by evaluating a string condition on the protein metadata, or by providing a list of protein accession numbers (or gene names) to keep. Peptides that are exclusively linked to removed proteins are also removed, and the RS matrix is updated accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>str</code> <p>A condition string to filter protein metadata. Supports:</p> <ul> <li>Standard comparisons, e.g. <code>\"Protein FDR Confidence: Combined == 'High'\"</code></li> <li>Substring queries using <code>includes</code>, e.g. <code>\"Description includes 'p97'\"</code></li> </ul> <code>None</code> <code>accessions</code> <code>list of str</code> <p>List of accession numbers (var_names) to keep.</p> <code>None</code> <code>valid_genes</code> <code>bool</code> <p>If True, removes rows with missing gene names and resolves duplicate gene names by appending numeric suffixes.</p> <code>False</code> <code>unique_profiles</code> <code>bool</code> <p>If True, remove rows with duplicate abundance profiles across samples.</p> <code>False</code> <code>return_copy</code> <code>bool</code> <p>If True, returns a filtered copy. If False, modifies in place.</p> <code>True</code> <code>debug</code> <code>bool</code> <p>If True, prints debugging information.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <code>pAnnData</code> <p>Returns a filtered pAnnData object if <code>return_copy=True</code>. </p> <code>None</code> <code>None</code> <p>Otherwise, modifies in-place and returns None.</p> <p>Examples:</p> <p>Filter by metadata condition:     <pre><code>condition = \"Protein FDR Confidence: Combined == 'High'\"\npdata.filter_prot(condition=condition)\n</code></pre></p> <p>Substring match on protein description:     <pre><code>condition = \"Description includes 'p97'\"\npdata.filter_prot(condition=condition)\n</code></pre></p> <p>Numerical condition on metadata:     <pre><code>condition = \"Score &gt; 0.75\"\npdata.filter_prot(condition=condition)\n</code></pre></p> <p>Filter by specific protein accessions:     <pre><code>accessions = ['GAPDH', 'P53']\npdata.filter_prot(accessions=accessions)\n</code></pre></p> <p>Filter out all that have no valid genes (potentially artefacts):     <pre><code>pdata.filter_prot(valid_genes=True)\n</code></pre></p> <p>Tip</p> <p>Multiple filters can be combined in a single call. For example, to filger by condition and valid genes: ```python condition = \"Score &gt; 0.75\" pdata.filter_prot(condition=condition, valid_genes=True)</p> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>def filter_prot(self, condition = None, accessions=None, valid_genes=False, unique_profiles=False, return_copy = True, debug=False):\n    \"\"\"\n    Filter protein data based on metadata conditions or accession list (protein name and gene name).\n\n    This method filters the protein-level data either by evaluating a string condition on the protein metadata,\n    or by providing a list of protein accession numbers (or gene names) to keep. Peptides that are exclusively\n    linked to removed proteins are also removed, and the RS matrix is updated accordingly.\n\n    Args:\n        condition (str): A condition string to filter protein metadata. Supports:\n\n            - Standard comparisons, e.g. `\"Protein FDR Confidence: Combined == 'High'\"`\n            - Substring queries using `includes`, e.g. `\"Description includes 'p97'\"`\n        accessions (list of str, optional): List of accession numbers (var_names) to keep.\n        valid_genes (bool): If True, removes rows with missing gene names and resolves duplicate gene names by appending numeric suffixes.\n        unique_profiles (bool): If True, remove rows with duplicate abundance profiles across samples.\n        return_copy (bool): If True, returns a filtered copy. If False, modifies in place.\n        debug (bool): If True, prints debugging information.\n\n    Returns:\n        pAnnData (pAnnData): Returns a filtered pAnnData object if `return_copy=True`. \n        None (None): Otherwise, modifies in-place and returns None.\n\n    Examples:\n        Filter by metadata condition:\n            ```python\n            condition = \"Protein FDR Confidence: Combined == 'High'\"\n            pdata.filter_prot(condition=condition)\n            ```\n\n        Substring match on protein description:\n            ```python\n            condition = \"Description includes 'p97'\"\n            pdata.filter_prot(condition=condition)\n            ```\n\n        Numerical condition on metadata:\n            ```python\n            condition = \"Score &gt; 0.75\"\n            pdata.filter_prot(condition=condition)\n            ```\n\n        Filter by specific protein accessions:\n            ```python\n            accessions = ['GAPDH', 'P53']\n            pdata.filter_prot(accessions=accessions)\n            ```\n\n        Filter out all that have no valid genes (potentially artefacts):\n            ```python\n            pdata.filter_prot(valid_genes=True)\n            ```\n\n        !!! tip\n            Multiple filters can be combined in a single call. For example, to filger by condition and valid genes:\n            ```python\n            condition = \"Score &gt; 0.75\"\n            pdata.filter_prot(condition=condition, valid_genes=True)\n    \"\"\"\n    from scipy.sparse import issparse\n\n    if not self._check_data('protein'): # type: ignore[attr-defined]\n        raise ValueError(f\"No protein data found. Check that protein data was imported.\")\n\n    pdata = self.copy() if return_copy else self # type: ignore[attr-defined]\n    action = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n\n    message_parts = []\n\n    # 1. Filter by condition\n    if condition is not None:\n        formatted_condition = self._format_filter_query(condition, pdata.prot.var)\n        if debug:\n            print(f\"Formatted condition: {formatted_condition}\")\n        filtered_proteins = pdata.prot.var[pdata.prot.var.eval(formatted_condition)]\n        pdata.prot = pdata.prot[:, filtered_proteins.index]\n        message_parts.append(f\"condition: {condition}\")\n\n    # 2. Filter by accession list or gene names\n    if accessions is not None:\n        gene_map, _ = pdata.get_gene_maps(on='protein') # type: ignore[attr-defined]\n\n        resolved, unmatched = [], []\n        var_names = pdata.prot.var_names.astype(str)\n\n        for name in accessions:\n            name = str(name)\n            if name in var_names:\n                resolved.append(name)\n            elif name in gene_map:\n                resolved.append(gene_map[name])\n            else:\n                unmatched.append(name)\n\n        if unmatched:\n            warnings.warn(\n                f\"The following accession(s) or gene name(s) were not found and will be ignored: {unmatched}\"\n            )\n\n        if not resolved:\n            warnings.warn(\"No matching accessions found. No proteins will be retained.\")\n            pdata.prot = pdata.prot[:, []]\n            message_parts.append(\"accessions: 0 matched\")\n        else:\n            pdata.prot = pdata.prot[:, pdata.prot.var_names.isin(resolved)]\n            message_parts.append(f\"accessions: {len(resolved)} matched / {len(accessions)} requested\")\n\n    # 3. Valid genes\n    if valid_genes:\n        # A. Remove invalid gene entries\n        var = pdata.prot.var\n\n        mask_missing_gene = var[\"Genes\"].isna() | (var[\"Genes\"].astype(str).str.strip() == \"\")\n        keep_mask = ~mask_missing_gene\n\n        if debug:\n            print(f\"Missing genes: {mask_missing_gene.sum()}\")\n            missing_names = pdata.prot.var_names[mask_missing_gene]\n            print(f\"Examples of proteins missing names: {missing_names[:5].tolist()}\")\n\n        pdata.prot = pdata.prot[:, keep_mask].copy()\n        message_parts.append(f\"valid_genes: removed {int(mask_missing_gene.sum())} proteins with invalid gene names\")            \n\n        # B. Resolve duplicate gene names\n        var = pdata.prot.var  # refresh after filtering\n        var_genes = var[\"Genes\"].astype(str).str.strip()\n        gene_counts = var_genes.value_counts()\n        duplicates = gene_counts[gene_counts &gt; 1].index.tolist()\n\n        if len(duplicates) &gt; 0:\n            if debug:\n                print(f\"Found {len(duplicates)} duplicate gene names.\")\n\n            # Track how many times each duplicate has appeared\n            seen = {}\n            new_names = []\n            for gene in var[\"Genes\"]:\n                if gene in duplicates:\n                    seen[gene] = seen.get(gene, 0) + 1\n                    if seen[gene] &gt; 1:\n                        gene = f\"{gene}-{seen[gene]}\"\n                new_names.append(gene)\n\n            # Assign back to var\n            pdata.prot.var[\"Genes\"] = new_names\n\n            message_parts.append(f\"valid_genes: resolved {len(duplicates)} duplicate gene names by appending numeric suffixes\")\n            if debug:\n                example_dupes = [d for d in duplicates[:5]]\n                print(f\"Examples of duplicate genes resolved: {example_dupes}\")\n\n    # 4. Remove duplicate profiles\n    if unique_profiles:\n        X = pdata.prot.X.toarray() if issparse(pdata.prot.X) else pdata.prot.X\n        df_X = pd.DataFrame(X.T, index=pdata.prot.var_names)\n\n        all_nan = np.all(np.isnan(X), axis=0)\n        all_zero = np.all(X == 0, axis=0)\n        empty_mask = all_nan | all_zero\n\n        duplicated_mask = df_X.duplicated(keep=\"first\").values  # mark duplicates\n\n        # Combine removal conditions\n        remove_mask = duplicated_mask | empty_mask\n        keep_mask = ~remove_mask\n\n        # Counts for each type\n        n_dup = int(duplicated_mask.sum())\n        n_empty = int(empty_mask.sum())\n        n_total = int(remove_mask.sum())\n\n        if debug:\n            dup_names = pdata.prot.var_names[duplicated_mask]\n            print(f\"Duplicate abundance profiles detected: {n_dup} proteins\")\n            if len(dup_names) &gt; 0:\n                print(f\"Examples of duplicates: {dup_names[:5].tolist()}\")\n            print(f\"Empty (all-zero or all-NaN) proteins detected: {n_empty}\")\n\n        # Apply filter\n        pdata.prot = pdata.prot[:, keep_mask].copy()\n\n        # Add summary message\n        message_parts.append(\n            f\"unique_profiles: removed {n_dup} duplicate and {n_empty} empty abundance profiles \"\n            f\"({n_total} total)\"\n        )\n\n    if not message_parts:\n        # no filters were applied\n        message = f\"{format_log_prefix('user')} Filtering proteins [failed]: {action} protein data.\\n    \u2192 No filters applied.\"\n    else:\n        # at least 1 filter applied\n        # PEPTIDES: also filter out peptides that belonged only to the filtered proteins\n        if pdata.pep is not None and pdata.rs is not None: # type: ignore[attr-defined]\n            proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = pdata._filter_sync_peptides_to_proteins(\n                original=self, \n                updated_prot=pdata.prot, \n                debug=debug)\n\n            # Apply filtered RS and update .prot and .pep using the helper\n            pdata._apply_rs_filter(\n                keep_proteins=proteins_to_keep,\n                keep_peptides=peptides_to_keep,\n                orig_prot_names=orig_prot_names,\n                orig_pep_names=orig_pep_names,\n                debug=debug\n            )\n\n        # detect which filters were applied\n        active_filters = []\n        if condition is not None:\n            active_filters.append(\"condition\")\n        if accessions is not None:\n            active_filters.append(\"accession\")\n        if valid_genes:\n            active_filters.append(\"valid genes\")\n        if unique_profiles:\n            active_filters.append(\"unique profiles\")\n\n        # build the header, joining multiple filters nicely\n        joined_filters = \", \".join(active_filters) if active_filters else \"unspecified\"\n        message = (\n            f\"{format_log_prefix('user')} Filtering proteins [{joined_filters}]:\\n\"\n            f\"    {action} protein data based on {joined_filters}:\"\n        )\n\n        for part in message_parts:\n                message += f\"\\n    \u2192 {part}\"\n\n        # Protein and peptide counts summary\n        message += f\"\\n    \u2192 Proteins kept: {pdata.prot.shape[1]}\"\n        if pdata.pep is not None:\n            message += f\"\\n    \u2192 Peptides kept (linked): {pdata.pep.shape[1]}\\n\"\n\n    print(message)\n    pdata._append_history(message) # type: ignore[attr-defined]\n    pdata.update_summary(recompute=True) # type: ignore[attr-defined]\n    return pdata if return_copy else None\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.filtering.FilterMixin.filter_prot_found","title":"filter_prot_found","text":"<pre><code>filter_prot_found(group, min_ratio=None, min_count=None, on='protein', return_copy=True, verbose=True, match_any=False)\n</code></pre> <p>Filter proteins or peptides based on 'Found In' detection across samples or groups.</p> <p>This method filters features by checking whether they are found in a minimum number or proportion  of samples, either at the group level (e.g., biological condition) or based on individual files.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str or list of str</code> <p>Group name(s) corresponding to 'Found In: {group} ratio'  (e.g., \"HCT116_DMSO\") or a list of filenames (e.g., [\"F1\", \"F2\"]). If this argument matches one or more <code>.obs</code> columns, the function automatically  interprets it as a class name, expands it to all class values, and annotates the necessary <code>'Found In:'</code> features.</p> required <code>min_ratio</code> <code>float</code> <p>Minimum proportion (0.0\u20131.0) of samples the feature must be  found in. Ignored for file-based filtering.</p> <code>None</code> <code>min_count</code> <code>int</code> <p>Minimum number of samples the feature must be found in. Alternative  to <code>min_ratio</code>. Ignored for file-based filtering.</p> <code>None</code> <code>on</code> <code>str</code> <p>Feature level to filter: either \"protein\" or \"peptide\".</p> <code>'protein'</code> <code>return_copy</code> <code>bool</code> <p>If True, returns a filtered copy. If False, modifies in place.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, prints verbose summary information.</p> <code>True</code> <code>match_any</code> <code>bool</code> <p>Defaults to False, for a AND search condition. If True, matches features found in any of the specified groups/files (i.e. union).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>A filtered pAnnData object if <code>return_copy=True</code>; otherwise, modifies in place and returns None.</p> Note <ul> <li>If <code>group</code> matches <code>.obs</code> column names, the method automatically annotates found    features by class before filtering.</li> <li>For file-based filtering, use the file identifiers from <code>.prot.obs_names</code>.            </li> </ul> <p>Examples:</p> <p>Filter proteins found in all \"cellline\" groups (e.g. Cellline A, and cellline B), with at least 2 samples each:     <pre><code>pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_count=2, match_any=False)\n</code></pre></p> <p>Filter proteins found in any \"cellline\" groups (e.g. Cellline A, and cellline B), as long as they meet a minimum ratio of 0.4:     <pre><code>pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_ratio=0.4, match_any=True)\n</code></pre></p> <p>Filter proteins found in all three input files:     <pre><code>pdata.filter_prot_found(group=[\"F1\", \"F2\", \"F3\"])\n</code></pre></p> <p>Filter proteins found in files of a specific sub-group:     <pre><code>pdata.annotate_found(classes=['group','treatment'])\npdata.filter_prot_found(group=[\"groupA_control\", \"groupB_treated\"])\n</code></pre></p> <p>If a single class column (e.g., <code>\"cellline\"</code>) is given, filter proteins based on each of its unique values (e.g. Line A, Line B):     <pre><code>pdata.filter_prot_found(group=\"cellline\", min_ratio=0.5)\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>def filter_prot_found(self, group, min_ratio=None, min_count=None, on='protein', return_copy=True, verbose=True, match_any=False):\n    \"\"\"\n    Filter proteins or peptides based on 'Found In' detection across samples or groups.\n\n    This method filters features by checking whether they are found in a minimum number or proportion \n    of samples, either at the group level (e.g., biological condition) or based on individual files.\n\n    Args:\n        group (str or list of str): Group name(s) corresponding to 'Found In: {group} ratio' \n            (e.g., \"HCT116_DMSO\") or a list of filenames (e.g., [\"F1\", \"F2\"]). If this argument matches one or more `.obs` columns, the function automatically \n            interprets it as a class name, expands it to all class values, and annotates the\n            necessary `'Found In:'` features.\n        min_ratio (float, optional): Minimum proportion (0.0\u20131.0) of samples the feature must be \n            found in. Ignored for file-based filtering.\n        min_count (int, optional): Minimum number of samples the feature must be found in. Alternative \n            to `min_ratio`. Ignored for file-based filtering.\n        on (str): Feature level to filter: either \"protein\" or \"peptide\".\n        return_copy (bool): If True, returns a filtered copy. If False, modifies in place.\n        verbose (bool): If True, prints verbose summary information.\n        match_any (bool): Defaults to False, for a AND search condition. If True, matches features found in any of the specified groups/files (i.e. union).\n\n    Returns:\n        pAnnData: A filtered pAnnData object if `return_copy=True`; otherwise, modifies in place and returns None.\n\n    Note:\n        - If `group` matches `.obs` column names, the method automatically annotates found \n          features by class before filtering.\n        - For file-based filtering, use the file identifiers from `.prot.obs_names`.            \n\n    Examples:\n        Filter proteins found in all \"cellline\" groups (e.g. Cellline A, and cellline B), with at least 2 samples each:\n            ```python\n            pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_count=2, match_any=False)\n            ```\n\n        Filter proteins found in any \"cellline\" groups (e.g. Cellline A, and cellline B), as long as they meet a minimum ratio of 0.4:\n            ```python\n            pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_ratio=0.4, match_any=True)\n            ```                \n\n        Filter proteins found in all three input files:\n            ```python\n            pdata.filter_prot_found(group=[\"F1\", \"F2\", \"F3\"])\n            ```\n\n        Filter proteins found in files of a specific sub-group:\n            ```python\n            pdata.annotate_found(classes=['group','treatment'])\n            pdata.filter_prot_found(group=[\"groupA_control\", \"groupB_treated\"])\n            ```\n\n        If a single class column (e.g., `\"cellline\"`) is given, filter proteins based on each of its unique values (e.g. Line A, Line B):\n            ```python\n            pdata.filter_prot_found(group=\"cellline\", min_ratio=0.5)\n            ```\n    \"\"\"\n    if not self._check_data(on): # type: ignore[attr-defined]\n        return\n\n    adata = self.prot if on == 'protein' else self.pep\n    var = adata.var\n\n    # Normalize group to list\n    if isinstance(group, str):\n        group = [group]\n    if not isinstance(group, (list, tuple)):\n        raise TypeError(\"`group` must be a string or list of strings.\")\n\n    # Auto-resolve obs columns passed instead of group values\n    auto_value_msg = None\n    if all(g in adata.obs.columns for g in group):\n        if len(group) == 1:\n            obs_col = group[0]\n            expanded_groups = adata.obs[obs_col].unique().tolist()\n        else:\n            expanded_groups = (\n                adata.obs[group].astype(str)\n                    .agg(\"_\".join, axis=1)\n                    .unique()\n                    .tolist()\n            )\n        # auto-annotate found features by these obs columns\n        self.annotate_found(classes=group, on=on, verbose=False)\n        group = expanded_groups\n        auto_value_msg = (\n            f\"{format_log_prefix('info', 2)} Found matching obs column(s): {group}. \"\n            \"Automatically annotating detection by group values.\"\n        )\n\n    if verbose and auto_value_msg:\n        print(auto_value_msg)\n\n    # Determine filtering mode: group vs file or handle ambiguity/missing\n    group_metrics = adata.uns.get(f\"found_metrics_{on}\")\n\n    mode = None\n    all_file_cols = all(f\"Found In: {g}\" in var.columns for g in group)\n    all_group_cols = (\n        group_metrics is not None\n        and all((g, \"count\") in group_metrics.columns for g in group)\n    )\n\n    # --- 1\ufe0f\u20e3 Explicit ambiguity: both file- and group-level indicators exist ---\n    is_ambiguous, annotated_files, annotated_groups = _detect_ambiguous_input(group, var, group_metrics)\n    if is_ambiguous:\n        raise ValueError(\n            f\"Ambiguous input: items in {group} include both file identifiers {annotated_files} \"\n            f\"and group values {annotated_groups}.\\n\"\n            \"Please separate group-based and file-based filters into separate calls.\"\n        )\n\n    # --- 2\ufe0f\u20e3 Group-based mode ---\n    elif all_group_cols:\n        mode = \"group\"\n\n    # --- 3\ufe0f\u20e3 File-based mode ---\n    elif all_file_cols:\n        mode = \"file\"\n\n    # --- 4\ufe0f\u20e3 Mixed or unresolved case (fallback) ---\n    else:\n        missing = []\n        for g in group:\n            group_missing = (\n                group_metrics is None\n                or (g, \"count\") not in group_metrics.columns\n                or (g, \"ratio\") not in group_metrics.columns\n            )\n            file_missing = f\"Found In: {g}\" not in var.columns\n\n            if group_missing and file_missing:\n                missing.append(g)\n\n        # Consistent, readable user message\n        msg = [f\"The following group(s)/file(s) could not be found: {missing or '\u2014'}\"]\n        msg.append(\"\u2192 If these are group names, make sure you ran:\")\n        msg.append(f\"   pdata.annotate_found(classes={group})\")\n        msg.append(\"\u2192 If these are file names, ensure 'Found In: &lt;file&gt;' columns exist.\\n\")\n        raise ValueError(\"\\n\".join(msg))\n\n    # ---------------\n    # Apply filtering\n    mask = np.ones(len(var), dtype=bool)\n\n    if mode == \"file\":\n        if match_any: # OR logic\n            mask = np.zeros(len(var), dtype=bool)\n            for g in group:\n                col = f\"Found In: {g}\"\n                mask |= var[col]  \n            if verbose:\n                print(f\"{format_log_prefix('user')} Filtering proteins [Found|File-mode|ANY]: keeping {mask.sum()} / {len(mask)} features found in ANY of files: {group}\")\n        else: # AND logic (default)\n            for g in group:\n                col = f\"Found In: {g}\"\n                mask &amp;= var[col]\n            if verbose:\n                print(f\"{format_log_prefix('user')} Filtering proteins [Found|File-mode|ALL]: keeping {mask.sum()} / {len(mask)} features found in ALL files: {group}\")\n\n    elif mode == \"group\":\n        if min_ratio is None and min_count is None:\n            raise ValueError(\"You must specify either `min_ratio` or `min_count` when filtering by group.\")\n\n        if match_any: # ANY logic\n            mask = np.zeros(len(var), dtype=bool)\n            for g in group:\n                count_series = group_metrics[(g, \"count\")]\n                ratio_series = group_metrics[(g, \"ratio\")]\n\n                if min_ratio is not None:\n                    this_mask = ratio_series &gt;= min_ratio\n                else:\n                    this_mask = count_series &gt;= min_count\n\n                mask |= this_mask\n            if verbose:\n                print(f\"{format_log_prefix('user')} Filtering proteins [Found|Group-mode|ANY]: keeping {mask.sum()} / {len(mask)} features passing threshold in ANY of groups: {group}\")\n\n        else:\n            for g in group:\n                count_series = group_metrics[(g, \"count\")]\n                ratio_series = group_metrics[(g, \"ratio\")]\n\n                if min_ratio is not None:\n                    this_mask = ratio_series &gt;= min_ratio\n                else:\n                    this_mask = count_series &gt;= min_count\n\n                mask &amp;= this_mask\n\n            if verbose:\n                print(f\"{format_log_prefix('user')} Filtering proteins [Found|Group-mode|ALL]: keeping {mask.sum()} / {len(mask)} features passing threshold {min_ratio if min_ratio is not None else min_count} across groups: {group}\")\n\n    # Apply filtering\n    filtered = self.copy() if return_copy else self # type: ignore[attr-defined], EditingMixin\n    adata_filtered = adata[:, mask.values]\n\n    if on == 'protein':\n        filtered.prot = adata_filtered\n\n        # Optional: filter peptides + rs as well\n        if filtered.pep is not None and filtered.rs is not None:\n            proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = filtered._filter_sync_peptides_to_proteins(\n                original=self,\n                updated_prot=filtered.prot,\n                debug=verbose\n            )\n\n            filtered._apply_rs_filter(\n                keep_proteins=proteins_to_keep,\n                keep_peptides=peptides_to_keep,\n                orig_prot_names=orig_prot_names,\n                orig_pep_names=orig_pep_names,\n                debug=verbose\n            )\n\n    else:\n        filtered.pep = adata_filtered\n        # Optionally, we could also remove proteins no longer linked to any peptides,\n        # but that's less common and we can leave it out unless requested.\n\n    criteria_str = (\n        f\"min_ratio={min_ratio}\" if mode == \"group\" and min_ratio is not None else\n        f\"min_count={min_count}\" if mode == \"group\" else\n        (\"ANY files\" if match_any else \"ALL files\")\n    )\n\n    logic_str = \"ANY\" if match_any else \"ALL\"\n\n    filtered._append_history(  # type: ignore[attr-defined], HistoryMixin\n        f\"{on}: Filtered by detection in {mode} group(s) {group} using {criteria_str} (match_{logic_str}).\"\n    )\n    filtered.update_summary(recompute=True) # type: ignore[attr-defined], SummaryMixin\n\n    return filtered if return_copy else None\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.filtering.FilterMixin.filter_prot_significant","title":"filter_prot_significant","text":"<pre><code>filter_prot_significant(group=None, min_ratio=None, min_count=None, fdr_threshold=0.01, return_copy=True, verbose=True, match_any=True)\n</code></pre> <p>Filter proteins based on significance across samples or groups using FDR thresholds.</p> <p>This method filters proteins by checking whether they are significant (e.g. PG.Q.Value &lt; 0.01) in a minimum number or proportion of samples, either per file or grouped.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str, list, or None</code> <p>Group name(s) (e.g., sample classes or filenames). If None, uses all files.</p> <code>None</code> <code>min_ratio</code> <code>float</code> <p>Minimum proportion of samples to be significant.</p> <code>None</code> <code>min_count</code> <code>int</code> <p>Minimum number of samples to be significant.</p> <code>None</code> <code>fdr_threshold</code> <code>float</code> <p>Significance threshold (default = 0.01).</p> <code>0.01</code> <code>return_copy</code> <code>bool</code> <p>Whether to return a filtered copy or modify in-place.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to print summary.</p> <code>True</code> <code>match_any</code> <code>bool</code> <p>If True, retain proteins significant in any group/file (OR logic). If False, require all groups/files to be significant (AND logic).</p> <code>True</code> <p>Returns:</p> Type Description <p>pAnnData or None: Filtered object (if <code>return_copy=True</code>) or modifies in-place.</p> <p>Examples:</p> <p>Filter proteins significant by their global significance (e.g. PD-based imports):     <pre><code>pdata.filter_prot_significant()\n</code></pre></p> <p>Filter proteins significant in the \"cellline\" group containing e.g. \"groupA\" and \"groupB\" groups, FDR of 0.01 (default):     <pre><code>pdata.filter_prot_significant(group=[\"cellline\"], min_count=2)\n</code></pre></p> <p>Filter proteins significant in all three input files:     <pre><code>pdata.filter_prot_significant(group=[\"F1\", \"F2\", \"F3\"])\n</code></pre></p> <p>Filter proteins significant in files of a specific sub-group:     <pre><code>pdata.annotate_significant(classes=['group','treatment'])\npdata.filter_prot_significant(group=[\"groupA_control\", \"groupB_treated\"])            \n</code></pre></p> Todo <p>Implement peptide then protein filter</p> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>def filter_prot_significant(self, group=None, min_ratio=None, min_count=None, fdr_threshold=0.01, return_copy=True, verbose=True, match_any=True):\n    \"\"\"\n    Filter proteins based on significance across samples or groups using FDR thresholds.\n\n    This method filters proteins by checking whether they are significant (e.g. PG.Q.Value &lt; 0.01)\n    in a minimum number or proportion of samples, either per file or grouped.\n\n    Args:\n        group (str, list, or None): Group name(s) (e.g., sample classes or filenames). If None, uses all files.\n        min_ratio (float, optional): Minimum proportion of samples to be significant.\n        min_count (int, optional): Minimum number of samples to be significant.\n        fdr_threshold (float): Significance threshold (default = 0.01).\n        return_copy (bool): Whether to return a filtered copy or modify in-place.\n        verbose (bool): Whether to print summary.\n        match_any (bool): If True, retain proteins significant in *any* group/file (OR logic). If False, require *all* groups/files to be significant (AND logic).\n\n    Returns:\n        pAnnData or None: Filtered object (if `return_copy=True`) or modifies in-place.\n\n    Examples:\n        Filter proteins significant by their global significance (e.g. PD-based imports):\n            ```python\n            pdata.filter_prot_significant()\n            ```\n\n        Filter proteins significant in the \"cellline\" group containing e.g. \"groupA\" and \"groupB\" groups, FDR of 0.01 (default):\n            ```python\n            pdata.filter_prot_significant(group=[\"cellline\"], min_count=2)\n            ```\n\n        Filter proteins significant in all three input files:\n            ```\n            pdata.filter_prot_significant(group=[\"F1\", \"F2\", \"F3\"])\n            ```\n\n        Filter proteins significant in files of a specific sub-group:\n            ```python\n            pdata.annotate_significant(classes=['group','treatment'])\n            pdata.filter_prot_significant(group=[\"groupA_control\", \"groupB_treated\"])            \n            ```\n\n    Todo:\n        Implement peptide then protein filter\n    \"\"\"\n    if not self._check_data(\"protein\"): # type: ignore[attr-defined]\n        return\n\n    adata = self.prot \n    var = adata.var\n\n    # Detect per-sample significance layer\n    has_protein_level_significance = any(\n        k.lower().endswith(\"_qval\") or k.lower().endswith(\"_fdr\") for k in adata.layers.keys()\n    )\n\n    # --- Handle missing significance data entirely ---\n    if not has_protein_level_significance and \"Global_Q_value\" not in adata.var.columns:\n        raise ValueError(\n            \"No per-sample layer (e.g., *_qval) or global significance column ('Global_Q_value') \"\n            \"found in .prot. Please ensure your data includes q-values or run annotate_significant().\"\n        )\n\n    # --- 1\ufe0f\u20e3 Global fallback mode (e.g. PD-based imports) ---\n    if not has_protein_level_significance and \"Global_Q_value\" in adata.var.columns:\n        if group is not None:\n            raise ValueError(\n                f\"Cannot filter by group {group}: per-sample significance data missing \"\n                \"and only global q-values available.\"\n            )\n\n        global_mask = adata.var[\"Global_Q_value\"] &lt; fdr_threshold\n\n        n_total = len(global_mask)\n        n_kept = int(global_mask.sum())\n        n_dropped = n_total - n_kept\n\n        filtered = self.copy() if return_copy else self\n        filtered.prot = adata[:, global_mask]\n\n        if filtered.pep is not None and filtered.rs is not None:\n            proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = filtered._filter_sync_peptides_to_proteins(\n                original=self, updated_prot=filtered.prot, debug=verbose\n            )\n            filtered._apply_rs_filter(\n                keep_proteins=proteins_to_keep,\n                keep_peptides=peptides_to_keep,\n                orig_prot_names=orig_prot_names,\n                orig_pep_names=orig_pep_names,\n                debug=verbose,\n            )\n\n        filtered.update_summary(recompute=True)\n        filtered._append_history(\n            f\"Filtered by global significance (Global_Q_value &lt; {fdr_threshold}); \"\n            f\"{n_kept}/{n_total} proteins retained.\"\n        )\n\n        if verbose:\n            print(f\"{format_log_prefix('user')} Filtering proteins by significance [Global-mode]:\")\n            print(f\"{format_log_prefix('info', 2)} Using global protein-level q-values (no per-sample significance available).\")\n            return_copy_str = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n            print(f\"    {return_copy_str} protein data based on significance thresholds:\")\n            print(f\"{format_log_prefix('filter_conditions')}Files requested: All\")\n            print(f\"{format_log_prefix('filter_conditions')}FDR threshold: {fdr_threshold}\")\n            print(f\"    \u2192 Proteins kept: {n_kept}, Proteins dropped: {n_dropped}\\n\")\n\n        return filtered if return_copy else None\n\n    # --- 2\ufe0f\u20e3 Per-sample significance data available ---\n    no_group_msg = None\n    auto_group_msg = None\n    auto_value_msg = None\n\n    if group is None:\n        group_list = list(adata.obs_names)\n        if verbose:\n            no_group_msg = f\"{format_log_prefix('info', 2)} No group provided. Defaulting to sample-level significance filtering.\"\n    else:\n        group_list = [group] if isinstance(group, str) else group\n\n    # Ensure annotations exist or auto-generate\n    missing_cols = [f\"Significant In: {g}\" for g in group_list]\n    if all(col in var.columns for col in missing_cols):\n        # Case A: user passed actual group values, already annotated\n        pass\n    else:\n        # Case B: need to resolve automatically\n        if all(g in adata.obs.columns for g in group_list):\n            # User passed obs column(s)\n            if len(group_list) == 1:\n                obs_col = group_list[0]\n                expanded_groups = adata.obs[obs_col].unique().tolist()\n            else:\n                expanded_groups = (\n                    adata.obs[group_list].astype(str)\n                        .agg(\"_\".join, axis=1)\n                        .unique()\n                        .tolist()\n                )\n            self.annotate_significant(classes=group_list,\n                                    fdr_threshold=fdr_threshold,\n                                    on=\"protein\", verbose=False)\n            group_list = expanded_groups\n            auto_group_msg = f\"{format_log_prefix('info', 2)} Found matching obs column '{group_list}'. Automatically annotating significance by group: {group_list} using FDR threshold {fdr_threshold}.\"\n\n        else:\n            # User passed group values, but not annotated yet\n            found_obs_col = None\n            for obs_col in adata.obs.columns:\n                if set(group_list).issubset(set(adata.obs[obs_col].unique())):\n                    found_obs_col = obs_col\n                    break\n\n            if found_obs_col is not None:\n                self.annotate_significant(classes=[found_obs_col],\n                                        fdr_threshold=fdr_threshold,\n                                        on=\"protein\", indent=2, verbose=False)\n                auto_value_msg = (f\"{format_log_prefix('info', 2)} Found matching obs column '{found_obs_col}'\"\n                f\"for groups {group_list}. Automatically annotating significant features by group {found_obs_col} \"\n                f\"using FDR threshold {fdr_threshold}.\")    \n            else:\n                raise ValueError(\n                    f\"Could not find existing significance annotations for groups {group_list}. \"\n                    \"Please either pass valid obs column(s), provide values from a valid `.obs` column or run `annotate_significant()` first.\"\n                )\n\n    # --- 3\ufe0f\u20e3 Mode detection and ambiguity handling ---\n    metrics_key = \"significance_metrics_protein\"\n    metrics_df = adata.uns.get(metrics_key, pd.DataFrame())\n\n    is_ambiguous, annotated_files, annotated_groups = _detect_ambiguous_input(group_list, var, metrics_df)\n    if is_ambiguous:\n        raise ValueError(\n            f\"Ambiguous input: items in {group_list} include both file identifiers {annotated_files} \"\n            f\"and group values {annotated_groups}.\\n\"\n            \"Please separate group-based and file-based filters into separate calls.\"\n        )\n\n    all_group_cols = (\n        metrics_df is not None\n        and all((g, \"count\") in metrics_df.columns for g in group_list)\n    )\n    all_file_cols = all(f\"Significant In: {g}\" in var.columns for g in group_list)\n    mode = \"group\" if all_group_cols else \"file\"\n\n    # Build filtering mask\n    mask = np.zeros(len(var), dtype=bool) if match_any else np.ones(len(var), dtype=bool)\n\n    if mode == \"group\":\n        if min_ratio is None and min_count is None:\n            raise ValueError(\"Specify `min_ratio` or `min_count` for group-based filtering.\")\n        for g in group_list:\n            count = metrics_df[(g, \"count\")]\n            ratio = metrics_df[(g, \"ratio\")]\n            this_mask = ratio &gt;= min_ratio if min_ratio is not None else count &gt;= min_count\n            mask = mask | this_mask if match_any else mask &amp; this_mask\n    else:  # file mode\n        for g in group_list:\n            col = f\"Significant In: {g}\"\n            this_mask = var[col].values\n            mask = mask | this_mask if match_any else mask &amp; this_mask\n\n    # --- 4\ufe0f\u20e3 Apply filtering and sync ---\n    filtered = self.copy() if return_copy else self\n    filtered.prot = adata[:, mask]\n\n    # Sync peptides and RS\n    if filtered.pep is not None and filtered.rs is not None:\n        proteins_to_keep, peptides_to_keep, orig_prot_names, orig_pep_names = filtered._filter_sync_peptides_to_proteins(\n            original=self, updated_prot=filtered.prot, debug=verbose\n        )\n        filtered._apply_rs_filter(\n            keep_proteins=proteins_to_keep,\n            keep_peptides=peptides_to_keep,\n            orig_prot_names=orig_prot_names,\n            orig_pep_names=orig_pep_names,\n            debug=verbose\n        )\n\n    filtered.update_summary(recompute=True)\n    filtered._append_history(\n        f\"Filtered by significance (FDR &lt; {fdr_threshold}) in group(s): {group_list}, \"\n        f\"using min_ratio={min_ratio} / min_count={min_count}, match_any={match_any}\"\n    )\n\n    if verbose:\n        logic = \"any\" if match_any else \"all\"\n        mode_str = \"Group-mode\" if mode == \"group\" else \"File-mode\"\n\n        print(f\"{format_log_prefix('user')} Filtering proteins [Significance|{mode_str}]:\")\n\n        if no_group_msg:\n            print(no_group_msg)\n        if auto_group_msg:\n            print(auto_group_msg)\n        if auto_value_msg:\n            print(auto_value_msg)\n\n        return_copy_str = \"Returning a copy of\" if return_copy else \"Filtered and modified\"\n        print(f\"    {return_copy_str} protein data based on significance thresholds:\")\n\n        if mode == \"group\":\n            # Case A: obs column(s) expanded \u2192 show expanded_groups and add note\n            if auto_group_msg:\n                group_note = f\" (all values of obs column(s))\"\n                print(f\"{format_log_prefix('filter_conditions')}Groups requested: {group_list}{group_note}\")\n            else:\n                print(f\"{format_log_prefix('filter_conditions')}Groups requested: {group_list}\")\n            print(f\"{format_log_prefix('filter_conditions')}FDR threshold: {fdr_threshold}\")\n            if min_ratio is not None:\n                print(f\"{format_log_prefix('filter_conditions')}Minimum ratio: {min_ratio} (match_{logic} = {match_any})\")\n            if min_count is not None:\n                print(f\"{format_log_prefix('filter_conditions')}Minimum count: {min_count} (match_{logic} = {match_any})\")\n        else:\n            print(f\"{format_log_prefix('filter_conditions')}Files requested: All\")\n            print(f\"{format_log_prefix('filter_conditions')}FDR threshold: {fdr_threshold}\")\n            print(f\"{format_log_prefix('filter_conditions')}Logic: {logic} \"\n                f\"(protein must be significant in {'\u22651' if match_any else 'all'} file(s))\")\n\n        n_kept = int(mask.sum())\n        n_total = len(mask)\n        n_dropped = n_total - n_kept\n        print(f\"    \u2192 Proteins kept: {n_kept}, Proteins dropped: {n_dropped}\\n\")\n\n    return filtered if return_copy else None\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.filtering.FilterMixin.filter_rs","title":"filter_rs","text":"<pre><code>filter_rs(min_peptides_per_protein=None, min_unique_peptides_per_protein=2, max_proteins_per_peptide=None, return_copy=True, preset=None, validate_after=True)\n</code></pre> <p>Filter the RS matrix and associated <code>.prot</code> and <code>.pep</code> data based on peptide-protein relationships.</p> <p>This method applies rules for keeping proteins with sufficient peptide evidence and/or removing ambiguous peptides. It also updates internal mappings accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>min_peptides_per_protein</code> <code>int</code> <p>Minimum number of total peptides required per protein.</p> <code>None</code> <code>min_unique_peptides_per_protein</code> <code>int</code> <p>Minimum number of unique peptides required per protein  (default is 2).</p> <code>2</code> <code>max_proteins_per_peptide</code> <code>int</code> <p>Maximum number of proteins a peptide can map to; peptides  exceeding this will be removed.</p> <code>None</code> <code>return_copy</code> <code>bool</code> <p>If True (default), returns a filtered pAnnData object. If False, modifies in place.</p> <code>True</code> <code>preset</code> <code>str or dict</code> <p>Predefined filter presets: - <code>\"default\"</code> \u2192 unique peptides \u2265 2 - <code>\"lenient\"</code> \u2192 total peptides \u2265 2 - A dictionary specifying filter thresholds manually.</p> <code>None</code> <code>validate_after</code> <code>bool</code> <p>If True (default), calls <code>self.validate()</code> after filtering.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>Filtered pAnnData object if <code>return_copy=True</code>; otherwise, modifies in place and returns None.</p> Note <p>Stores filter metadata in <code>.prot.uns['filter_rs']</code>, including indices of proteins/peptides kept and  filtering summary.</p> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>def filter_rs(\n    self,\n    min_peptides_per_protein=None,\n    min_unique_peptides_per_protein=2,\n    max_proteins_per_peptide=None,\n    return_copy=True,\n    preset=None,\n    validate_after=True\n):\n    \"\"\"\n    Filter the RS matrix and associated `.prot` and `.pep` data based on peptide-protein relationships.\n\n    This method applies rules for keeping proteins with sufficient peptide evidence and/or removing\n    ambiguous peptides. It also updates internal mappings accordingly.\n\n    Args:\n        min_peptides_per_protein (int, optional): Minimum number of total peptides required per protein.\n        min_unique_peptides_per_protein (int, optional): Minimum number of unique peptides required per protein \n            (default is 2).\n        max_proteins_per_peptide (int, optional): Maximum number of proteins a peptide can map to; peptides \n            exceeding this will be removed.\n        return_copy (bool): If True (default), returns a filtered pAnnData object. If False, modifies in place.\n        preset (str or dict, optional): Predefined filter presets:\n            - `\"default\"` \u2192 unique peptides \u2265 2\n            - `\"lenient\"` \u2192 total peptides \u2265 2\n            - A dictionary specifying filter thresholds manually.\n        validate_after (bool): If True (default), calls `self.validate()` after filtering.\n\n    Returns:\n        pAnnData: Filtered pAnnData object if `return_copy=True`; otherwise, modifies in place and returns None.\n\n    Note:\n        Stores filter metadata in `.prot.uns['filter_rs']`, including indices of proteins/peptides kept and \n        filtering summary.\n    \"\"\"\n    if self.rs is None: # type: ignore[attr-defined]\n        print(\"\u26a0\ufe0f No RS matrix to filter.\")\n        return self if return_copy else None\n\n    # --- Apply preset if given ---\n    if preset:\n        if preset == \"default\":\n            min_peptides_per_protein = None\n            min_unique_peptides_per_protein = 2\n            max_proteins_per_peptide = None\n        elif preset == \"lenient\":\n            min_peptides_per_protein = 2\n            min_unique_peptides_per_protein = None\n            max_proteins_per_peptide = None\n        elif isinstance(preset, dict):\n            min_peptides_per_protein = preset.get(\"min_peptides_per_protein\", min_peptides_per_protein)\n            min_unique_peptides_per_protein = preset.get(\"min_unique_peptides_per_protein\", min_unique_peptides_per_protein)\n            max_proteins_per_peptide = preset.get(\"max_proteins_per_peptide\", max_proteins_per_peptide)\n        else:\n            raise ValueError(f\"Unknown RS filtering preset: {preset}\")\n\n    pdata = self.copy() if return_copy else self # type: ignore[attr-defined], EditingMixin\n\n    rs = pdata.rs # type: ignore[attr-defined]\n\n    # --- Step 1: Peptide filter (max proteins per peptide) ---\n    if max_proteins_per_peptide is not None:\n        peptide_links = rs.getnnz(axis=0)\n        keep_peptides = peptide_links &lt;= max_proteins_per_peptide\n        rs = rs[:, keep_peptides]\n    else:\n        keep_peptides = np.ones(rs.shape[1], dtype=bool)\n\n    # --- Step 2: Protein filters ---\n    is_unique = rs.getnnz(axis=0) == 1\n    unique_counts = rs[:, is_unique].getnnz(axis=1)\n    peptide_counts = rs.getnnz(axis=1)\n\n    keep_proteins = np.ones(rs.shape[0], dtype=bool)\n    if min_peptides_per_protein is not None:\n        keep_proteins &amp;= (peptide_counts &gt;= min_peptides_per_protein)\n    if min_unique_peptides_per_protein is not None:\n        keep_proteins &amp;= (unique_counts &gt;= min_unique_peptides_per_protein)\n\n    rs_filtered = rs[keep_proteins, :]\n\n    # --- Step 3: Re-filter peptides now unmapped ---\n    keep_peptides_final = rs_filtered.getnnz(axis=0) &gt; 0\n    rs_filtered = rs_filtered[:, keep_peptides_final]\n\n    # --- Apply filtered RS ---\n    pdata._set_RS(rs_filtered, validate=False) # type: ignore[attr-defined], EditingMixin\n\n    # --- Filter .prot and .pep ---\n    if pdata.prot is not None:\n        pdata.prot = pdata.prot[:, keep_proteins]\n    if pdata.pep is not None:\n        original_peptides = keep_peptides.nonzero()[0]\n        final_peptides = original_peptides[keep_peptides_final]\n        pdata.pep = pdata.pep[:, final_peptides]\n\n    # --- History and summary ---\n    n_prot_before = self.prot.shape[1] if self.prot is not None else rs.shape[0]\n    n_pep_before = self.pep.shape[1] if self.pep is not None else rs.shape[1]\n    n_prot_after = rs_filtered.shape[0]\n    n_pep_after = rs_filtered.shape[1]\n\n    n_prot_dropped = n_prot_before - n_prot_after\n    n_pep_dropped = n_pep_before - n_pep_after\n\n    msg = \"\ud83e\uddea Filtered RS\"\n    if preset:\n        msg += f\" using preset '{preset}'\"\n    if min_peptides_per_protein is not None:\n        msg += f\", min peptides per protein: {min_peptides_per_protein}\"\n    if min_unique_peptides_per_protein is not None:\n        msg += f\", min unique peptides: {min_unique_peptides_per_protein}\"\n    if max_proteins_per_peptide is not None:\n        msg += f\", max proteins per peptide: {max_proteins_per_peptide}\"\n    msg += (\n        f\". Proteins: {n_prot_before} \u2192 {n_prot_after} (dropped {n_prot_dropped}), \"\n        f\"Peptides: {n_pep_before} \u2192 {n_pep_after} (dropped {n_pep_dropped}).\"\n    )\n\n    pdata._append_history(msg) # type: ignore[attr-defined], HistoryMixin\n    print(msg)\n    pdata.update_summary() # type: ignore[attr-defined], SummaryMixin\n\n    # --- Save filter indices to .uns ---\n    protein_indices = list(pdata.prot.var_names) if pdata.prot is not None else []\n    peptide_indices = list(pdata.pep.var_names) if pdata.pep is not None else []\n    pdata.prot.uns['filter_rs'] = {\n        \"kept_proteins\": protein_indices,\n        \"kept_peptides\": peptide_indices,\n        \"n_proteins\": len(protein_indices),\n        \"n_peptides\": len(peptide_indices),\n        \"description\": msg\n    }\n\n    if validate_after:\n        pdata.validate(verbose=True) # type: ignore[attr-defined], ValidationMixin\n\n    return pdata if return_copy else None\n</code></pre>"},{"location":"reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.filtering.FilterMixin.filter_sample","title":"filter_sample","text":"<pre><code>filter_sample(values=None, exact_cases=False, condition=None, file_list=None, exclude_file_list=None, min_prot=None, cleanup=True, return_copy=True, debug=False, query_mode=False)\n</code></pre> <p>Filter samples in a pAnnData object based on categorical, numeric, or identifier-based criteria.</p> <p>You must specify exactly one of the following:</p> <ul> <li><code>values</code>: Dictionary or list of dictionaries specifying class-based filters (e.g., treatment, cellline).</li> <li><code>condition</code>: A string condition evaluated against summary-level numeric metadata (e.g., protein count).</li> <li><code>file_list</code>: List of sample or file names to retain.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>dict or list of dict</code> <p>Categorical metadata filter. Matches rows in <code>.summary</code> or <code>.obs</code> with those field values. Examples: <code>{'treatment': 'kd', 'cellline': 'A'}</code>.</p> <code>None</code> <code>exact_cases</code> <code>bool</code> <p>If True, uses exact match across all class values when <code>values</code> is a list of dicts.</p> <code>False</code> <code>condition</code> <code>str</code> <p>Logical condition string referencing summary columns. This should reference columns in <code>pdata.summary</code>. Examples: <code>\"protein_count &gt; 1000\"</code>.</p> <code>None</code> <code>file_list</code> <code>list of str</code> <p>List of sample names or file identifiers to keep. Filters to only those samples (must match obs_names).</p> <code>None</code> <code>exclude_file_list</code> <code>list of str</code> <p>Similar to <code>file_list</code>, but excludes the specified files/samples instead of keeping them.</p> <code>None</code> <code>min_prot</code> <code>int</code> <p>Minimum number of proteins required in a sample to retain it.</p> <code>None</code> <code>cleanup</code> <code>bool</code> <p>If True (default), remove proteins that become all-NaN or all-zero after sample filtering and synchronize RS/peptide matrices. Set to False to retain all proteins for consistent feature alignment (e.g. during DE analysis).</p> <code>True</code> <code>return_copy</code> <code>bool</code> <p>If True, returns a filtered pAnnData object; otherwise modifies in place.</p> <code>True</code> <code>debug</code> <code>bool</code> <p>If True, prints query strings and filter summaries.</p> <code>False</code> <code>query_mode</code> <code>bool</code> <p>If True, interprets <code>values</code> or <code>condition</code> as a raw pandas-style <code>.query()</code> string and evaluates it directly on <code>.obs</code> or <code>.summary</code> respectively.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>Filtered pAnnData object if <code>return_copy=True</code>; otherwise, modifies in place and returns None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If more than one or none of <code>values</code>, <code>condition</code>, or <code>file_list</code> is specified.</p> <p>Examples:</p> <p>Filter by metadata values:     <pre><code>pdata.filter_sample(values={'treatment': 'kd', 'cellline': 'A'})\n</code></pre></p> <p>Filter with multiple exact matching cases:     <pre><code>pdata.filter_sample(\n    values=[\n        {'treatment': 'kd', 'cellline': 'A'},\n        {'treatment': 'sc', 'cellline': 'B'}\n    ],\n    exact_cases=True\n)\n</code></pre></p> <p>Filter by numeric condition on summary:     <pre><code>pdata.filter_sample(condition=\"protein_count &gt; 1000\")\n</code></pre></p> <p>Filter samples with fewer than 1000 proteins:     <pre><code>pdata.filter_sample(min_prot=1000)\n</code></pre></p> <p>Keep specific samples by name:     <pre><code>pdata.filter_sample(file_list=['Sample_001', 'Sample_007'])\n</code></pre></p> <p>Exclude specific files from the dataset:     <pre><code>pdata.filter_sample(exclude_file_list=['Sample_001', 'Sample_007'])\n</code></pre></p> <p>For advanced usage using query mode, see the note below.</p> <p>Advanced Usage</p> <p>To enable advanced filtering, set <code>query_mode=True</code> to evaluate raw pandas-style queries:</p> <ul> <li> <p>Query <code>.obs</code> metadata:     <pre><code>pdata.filter_sample(values=\"cellline == 'AS' and treatment == 'kd'\", query_mode=True)\n</code></pre></p> </li> <li> <p>Query <code>.summary</code> metadata:     <pre><code>pdata.filter_sample(condition=\"protein_count &gt; 1000 and missing_pct &lt; 0.2\", query_mode=True)\n</code></pre></p> </li> </ul> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>def filter_sample(self, values=None, exact_cases=False, condition=None, file_list=None, exclude_file_list=None, min_prot=None, cleanup=True, return_copy=True, debug=False, query_mode=False):\n    \"\"\"\n    Filter samples in a pAnnData object based on categorical, numeric, or identifier-based criteria.\n\n    You must specify **exactly one** of the following:\n\n    - `values`: Dictionary or list of dictionaries specifying class-based filters (e.g., treatment, cellline).\n    - `condition`: A string condition evaluated against summary-level numeric metadata (e.g., protein count).\n    - `file_list`: List of sample or file names to retain.\n\n    Args:\n        values (dict or list of dict, optional): Categorical metadata filter. Matches rows in `.summary` or `.obs` with those field values.\n            Examples: `{'treatment': 'kd', 'cellline': 'A'}`.\n        exact_cases (bool): If True, uses exact match across all class values when `values` is a list of dicts.\n        condition (str, optional): Logical condition string referencing summary columns. This should reference columns in `pdata.summary`.\n            Examples: `\"protein_count &gt; 1000\"`.\n        file_list (list of str, optional): List of sample names or file identifiers to keep. Filters to only those samples (must match obs_names).\n        exclude_file_list (list of str, optional): Similar to `file_list`, but excludes the specified files/samples instead of keeping them.\n        min_prot (int, optional): Minimum number of proteins required in a sample to retain it.\n        cleanup (bool): If True (default), remove proteins that become all-NaN or all-zero after sample filtering and synchronize RS/peptide matrices. Set to False to retain all proteins for consistent feature alignment (e.g. during DE analysis).\n        return_copy (bool): If True, returns a filtered pAnnData object; otherwise modifies in place.\n        debug (bool): If True, prints query strings and filter summaries.\n        query_mode (bool): If True, interprets `values` or `condition` as a raw pandas-style `.query()` string and evaluates it directly on `.obs` or `.summary` respectively.\n\n    Returns:\n        pAnnData: Filtered pAnnData object if `return_copy=True`; otherwise, modifies in place and returns None.\n\n    Raises:\n        ValueError: If more than one or none of `values`, `condition`, or `file_list` is specified.\n\n    Examples:\n        Filter by metadata values:\n            ```python\n            pdata.filter_sample(values={'treatment': 'kd', 'cellline': 'A'})\n            ```\n\n        Filter with multiple exact matching cases:\n            ```python\n            pdata.filter_sample(\n                values=[\n                    {'treatment': 'kd', 'cellline': 'A'},\n                    {'treatment': 'sc', 'cellline': 'B'}\n                ],\n                exact_cases=True\n            )\n            ```\n\n        Filter by numeric condition on summary:\n            ```python\n            pdata.filter_sample(condition=\"protein_count &gt; 1000\")\n            ```\n\n        Filter samples with fewer than 1000 proteins:\n            ```python\n            pdata.filter_sample(min_prot=1000)\n            ```\n\n        Keep specific samples by name:\n            ```python\n            pdata.filter_sample(file_list=['Sample_001', 'Sample_007'])\n            ```\n\n        Exclude specific files from the dataset:\n            ```python\n            pdata.filter_sample(exclude_file_list=['Sample_001', 'Sample_007'])\n            ```\n\n        For advanced usage using query mode, see the note below.\n\n        !!! note \"Advanced Usage\"\n            To enable **advanced filtering**, set `query_mode=True` to evaluate raw pandas-style queries:\n\n            - Query `.obs` metadata:\n                ```python\n                pdata.filter_sample(values=\"cellline == 'AS' and treatment == 'kd'\", query_mode=True)\n                ```\n\n            - Query `.summary` metadata:\n                ```python\n                pdata.filter_sample(condition=\"protein_count &gt; 1000 and missing_pct &lt; 0.2\", query_mode=True)\n                ```            \n    \"\"\"\n    # Ensure exactly one of the filter modes is specified\n    provided = [values, condition, file_list, min_prot, exclude_file_list]\n    if sum(arg is not None for arg in provided) != 1:\n        raise ValueError(\n            \"Invalid filter input. You must specify exactly one of the following keyword arguments:\\n\"\n            \"- `values=...` for categorical metadata filtering,\\n\"\n            \"- `condition=...` for summary-level condition filtering, or\\n\"\n            \"- `min_prot=...` to filter by minimum protein count.\\n\"\n            \"- `file_list=...` to filter by sample IDs.\\n\"\n            \"- `exclude_file_list=...` to exclude specific sample IDs.\\n\\n\"\n            \"Examples:\\n\"\n            \"  pdata.filter_sample(condition='protein_quant &gt; 0.2')\"\n        )\n\n    if min_prot is not None:\n        condition = f\"protein_count &gt;= {min_prot}\"\n\n    if values is not None and not query_mode:\n        return self._filter_sample_values(\n            values=values,\n            exact_cases=exact_cases,\n            debug=debug,\n            return_copy=return_copy, \n            cleanup=cleanup\n        )\n\n    if (condition is not None or file_list is not None or exclude_file_list is not None) and not query_mode:\n        return self._filter_sample_condition(\n            condition=condition,\n            file_list=file_list,\n            exclude_file_list=exclude_file_list,\n            return_copy=return_copy,\n            debug=debug, \n            cleanup=cleanup\n        )\n\n    if values is not None and query_mode:\n        return self._filter_sample_query(query_string=values, source='obs', return_copy=return_copy, debug=debug, cleanup=cleanup)\n\n    if condition is not None and query_mode:\n        return self._filter_sample_query(query_string=condition, source='summary', return_copy=return_copy, debug=debug, cleanup=cleanup)\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/","title":"Hidden Functions","text":"<p>Hidden functions for all MixIns.</p> <p>Advanced / Internal</p> <p>The functions in this section are internal utilities. They may change  without notice and are not guaranteed to remain stable across releases.  Use only if you understand the internal architecture of <code>pAnnData</code>.</p>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.analysis.AnalysisMixin","title":"src.scpviz.pAnnData.analysis.AnalysisMixin","text":"<p>Provides core statistical and dimensionality reduction tools for analyzing single-cell proteomics data.</p> <p>This mixin includes functionality for:</p> <ul> <li>Differential expression (DE) analysis using t-tests, Mann\u2013Whitney U, or Wilcoxon signed-rank tests  </li> <li>Ranking proteins or peptides by abundance within groups  </li> <li>Coefficient of Variation (CV) computation  </li> <li>Missing value imputation (global or group-wise) using statistical or KNN-based methods  </li> <li>Dimensionality reduction and clustering using PCA, UMAP, and Leiden  </li> <li>Neighbor graph construction for downstream manifold learning  </li> <li>Cleaning <code>.X</code> matrices by replacing NaNs  </li> <li>Row-wise normalization across multiple strategies  </li> </ul> <p>All functions are compatible with both protein- and peptide-level data and support use of AnnData layers.</p> <p>Methods:</p> Name Description <code>cv</code> <p>Compute coefficient of variation (CV) for each feature across or within sample groups.</p> <code>de</code> <p>Perform differential expression analysis between two sample groups.</p> <code>rank</code> <p>Rank features by mean abundance, compute standard deviation and numeric rank.</p> <code>impute</code> <p>Impute missing values globally or within groups using mean, median, min, or KNN.</p> <code>neighbor</code> <p>Compute neighborhood graph using PCA (or another embedding) for clustering or UMAP.</p> <code>leiden</code> <p>Run Leiden clustering on neighborhood graph, storing labels in <code>.obs['leiden']</code>.</p> <code>umap</code> <p>Perform UMAP dimensionality reduction using previously computed neighbors.</p> <code>pca</code> <p>Run PCA on normalized expression matrix, handling NaN exclusion and reinsertion of features.</p> <code>clean_X</code> <p>Replace NaNs in <code>.X</code> or a specified layer, optionally backing up the original.</p> <code>_normalize_helper</code> <p>Internal helper to compute per-sample scaling across multiple normalization methods.</p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>class AnalysisMixin:\n    \"\"\"\n    Provides core statistical and dimensionality reduction tools for analyzing single-cell proteomics data.\n\n    This mixin includes functionality for:\n\n    - Differential expression (DE) analysis using t-tests, Mann\u2013Whitney U, or Wilcoxon signed-rank tests  \n    - Ranking proteins or peptides by abundance within groups  \n    - Coefficient of Variation (CV) computation  \n    - Missing value imputation (global or group-wise) using statistical or KNN-based methods  \n    - Dimensionality reduction and clustering using PCA, UMAP, and Leiden  \n    - Neighbor graph construction for downstream manifold learning  \n    - Cleaning `.X` matrices by replacing NaNs  \n    - Row-wise normalization across multiple strategies  \n\n    All functions are compatible with both protein- and peptide-level data and support use of AnnData layers.\n\n    Functions:\n        cv: Compute coefficient of variation (CV) for each feature across or within sample groups.\n        de: Perform differential expression analysis between two sample groups.\n        rank: Rank features by mean abundance, compute standard deviation and numeric rank.\n        impute: Impute missing values globally or within groups using mean, median, min, or KNN.\n        neighbor: Compute neighborhood graph using PCA (or another embedding) for clustering or UMAP.\n        leiden: Run Leiden clustering on neighborhood graph, storing labels in `.obs['leiden']`.\n        umap: Perform UMAP dimensionality reduction using previously computed neighbors.\n        pca: Run PCA on normalized expression matrix, handling NaN exclusion and reinsertion of features.\n        clean_X: Replace NaNs in `.X` or a specified layer, optionally backing up the original.\n        _normalize_helper: Internal helper to compute per-sample scaling across multiple normalization methods.\n    \"\"\"\n\n    def cv(self, classes = None, on = 'protein', layer = \"X\", debug = False):\n        \"\"\"\n        Compute the coefficient of variation (CV) for each feature across sample groups.\n\n        This method calculates CV for each protein or peptide across all samples in each group,\n        storing the result as new columns in `.var`, one per group.\n\n        Args:\n            classes (str or list of str, optional): Sample-level class or list of classes used to define groups.\n            on (str): Whether to compute CV on \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use for computation (default is \"X\").\n            debug (bool): If True, prints debug information while filtering groups.\n\n        Returns:\n            None\n\n        Example:\n            Compute per-group CV for proteins using a custom normalization layer:\n                ```python\n                pdata.cv(classes=[\"group\", \"condition\"], on=\"protein\", layer=\"X_norm\")\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        adata = self.prot if on == 'protein' else self.pep\n        classes_list = utils.get_classlist(adata, classes)\n\n        for j, class_value in enumerate(classes_list):\n            data_filtered = utils.resolve_class_filter(adata, classes, class_value)\n\n            cv_data = data_filtered.X.toarray() if layer == \"X\" else data_filtered.layers[layer].toarray() if layer in data_filtered.layers else None\n            if cv_data is None:\n                raise ValueError(f\"Layer '{layer}' not found in adata.layers.\")\n\n            adata.var['CV: '+ class_value] = variation(cv_data, axis=0)\n\n        self._history.append(f\"{on}: Coefficient of Variation (CV) calculated for {layer} data by {classes}. CV stored in var['CV: {class_value}'].\") # type: ignore[attr-defined]\n\n    # TODO: implement methods for calculdating fold change, 1. mean, 2. prot pairwise median, or 3. pep pairwise median (will need to refer to RS)\n    def de(self, values=None, class_type=None, method='ttest', layer='X', pval=0.05, log2fc=1.0, fold_change_mode='mean'):\n        \"\"\"\n        Perform differential expression (DE) analysis on proteins across sample groups.\n\n        This method compares protein abundance between two sample groups using a specified\n        statistical test and fold change method. Input groups can be defined using either\n        legacy-style (`class_type` + `values`) or dictionary-style filters.\n\n        Args:\n            values (list of dict or list of list): Sample group filters to compare.\n\n                - Dictionary-style (recommended): [{'cellline': 'HCT116', 'treatment': 'DMSO'}, {...}]\n                - Legacy-style (if `class_type` is provided): [['HCT116', 'DMSO'], ['HCT116', 'DrugX']]\n\n            class_type (str or list of str, optional): Legacy-style class label(s) to interpret `values`.\n\n            method (str): Statistical test to use. Options: \"ttest\", \"mannwhitneyu\", \"wilcoxon\".\n\n            layer (str): Name of the data layer to use (default is \"X\").\n\n            pval (float): P-value cutoff used for labeling significance.\n\n            log2fc (float): Minimum log2 fold change threshold for significance labeling.\n\n            fold_change_mode (str): Strategy for computing fold change. Options:\n\n                - \"mean\": log2(mean(group1) / mean(group2))\n                - \"pairwise_median\": median of all pairwise log2 ratios\n                - \"pep_pairwise_median\": median of peptide-level pairwise log2 ratios, aggregated per protein\n\n        Returns:\n            pd.DataFrame: DataFrame with DE statistics including log2 fold change, p-values, and significance labels.\n\n        Example:\n            Legacy-style DE comparison using class types and value combinations:\n                ```python\n                pdata.de(\n                    class_type=[\"cellline\", \"treatment\"],\n                    values=[[\"HCT116\", \"DMSO\"], [\"HCT116\", \"DrugX\"]]\n                )\n                ```\n\n            Dictionary-style (recommended) DE comparison:\n                ```python\n                pdata.de(\n                    values=[\n                        {\"cellline\": \"HCT116\", \"treatment\": \"DMSO\"},\n                        {\"cellline\": \"HCT116\", \"treatment\": \"DrugX\"}\n                    ]\n                )\n                ```\n        \"\"\"\n\n        # --- Handle legacy input ---\n        if values is None:\n            raise ValueError(\"Please provide `values` (new format) or both `class_type` and `values` (legacy format).\")\n\n        if class_type is not None:\n            values = utils.format_class_filter(class_type, values, exact_cases=True)\n\n        if not isinstance(values, list) or len(values) != 2:\n            raise ValueError(\"`values` must be a list of two group dictionaries (or legacy value pairs).\")\n\n        if values[0] == values[1]:\n            raise ValueError(\"Both groups in `values` refer to the same condition. Please provide two distinct groups.\")\n\n        group1_dict, group2_dict = (\n            [values[0]] if not isinstance(values[0], list) else values[0],\n            [values[1]] if not isinstance(values[1], list) else values[1]\n        )\n\n\n        # --- Sample filtering ---\n        pdata_case1 = self._filter_sample_values(values=group1_dict, exact_cases=True, return_copy=True, verbose=False, cleanup=False) # type: ignore[attr-defined], FilteringMixin\n        pdata_case2 = self._filter_sample_values(values=group2_dict, exact_cases=True, return_copy=True, verbose=False, cleanup=False) # type: ignore[attr-defined], FilteringMixin\n\n        def _label(d):\n            if isinstance(d, dict):\n                return '_'.join(str(v) for v in d.values())\n            return str(d)\n\n        group1_string = _label(group1_dict)\n        group2_string = _label(group2_dict)\n        comparison_string = f'{group1_string} vs {group2_string}'\n\n        log_prefix = format_log_prefix(\"user\")\n        n1, n2 = len(pdata_case1.prot), len(pdata_case2.prot)\n        print(f\"{log_prefix} Running differential expression [protein]\")\n        print(f\"   \ud83d\udd38 Comparing groups: {comparison_string}\")\n        print(f\"   \ud83d\udd38 Group sizes: {n1} vs {n2} samples\")\n        print(f\"   \ud83d\udd38 Method: {method} | Fold Change: {fold_change_mode} | Layer: {layer}\")\n        print(f\"   \ud83d\udd38 P-value threshold: {pval} | Log2FC threshold: {log2fc}\")\n\n        # --- Get layer data ---\n        data1 = utils.get_adata_layer(pdata_case1.prot, layer)\n        data2 = utils.get_adata_layer(pdata_case2.prot, layer)\n\n        # Shape: (samples, features)\n        data1 = np.asarray(data1)\n        data2 = np.asarray(data2)\n\n        # --- Compute fold change ---\n        if fold_change_mode == 'mean':\n            with np.errstate(all='ignore'):\n                group1_mean = np.nanmean(data1, axis=0)\n                group2_mean = np.nanmean(data2, axis=0)\n\n                # Identify zeros or NaNs in either group\n                mask_invalid = (group1_mean == 0) | (group2_mean == 0) | np.isnan(group1_mean) | np.isnan(group2_mean)\n                log2fc_vals = np.log2(group1_mean / group2_mean)\n                log2fc_vals[mask_invalid] = np.nan\n\n                n_invalid = np.sum(mask_invalid)\n                if n_invalid &gt; 0:\n                    print(f\"{format_log_prefix('info',2)} {n_invalid} proteins were not comparable (zero or NaN mean in one group).\")\n\n        elif fold_change_mode == 'pairwise_median':\n            mask_invalid = ( # Detect invalid features (any 0 or NaN in either group)\n                np.any((data1 == 0) | np.isnan(data1), axis=0) |\n                np.any((data2 == 0) | np.isnan(data2), axis=0)\n            )\n            # Compute median pairwise log2FC\n            log2fc_vals = utils.pairwise_log2fc(data1, data2)\n            log2fc_vals[mask_invalid] = np.nan # Mark invalid features as NaN\n            n_invalid = np.sum(mask_invalid)\n            if n_invalid &gt; 0:\n                print(f\"{format_log_prefix('info',2)} {n_invalid} proteins were not comparable (zero or NaN mean in one group).\")\n\n        elif fold_change_mode == 'pep_pairwise_median':\n            # --- Validate .pep presence ---\n            if self.pep is None:\n                raise ValueError(\"Peptide-level data (.pep) is required for fold_change_mode='pep_pairwise_median', but self.pep is None.\")\n\n            # --- Handle peptide layer fallback ---\n            actual_layer = layer\n            if layer != 'X' and not (hasattr(self.pep, \"layers\") and layer in self.pep.layers):\n                warnings.warn(\n                    f\"Layer '{layer}' not found in .pep.layers. Falling back to 'X'.\",\n                    UserWarning\n                )\n                actual_layer = 'X'\n\n            # Get peptide data\n            pep_data1 = np.asarray(utils.get_adata_layer(pdata_case1.pep, actual_layer))\n            pep_data2 = np.asarray(utils.get_adata_layer(pdata_case2.pep, actual_layer))\n\n            # Detect invalid peptides (any 0 or NaN in either group)\n            mask_invalid_pep = (\n                np.any((pep_data1 == 0) | np.isnan(pep_data1), axis=0) |\n                np.any((pep_data2 == 0) | np.isnan(pep_data2), axis=0)\n            )\n\n            # Compute per-peptide pairwise log2FCs\n            pep_log2fc = utils.pairwise_log2fc(pep_data1, pep_data2)\n            pep_log2fc[mask_invalid_pep] = np.nan  # mark invalids\n\n            n_invalid_pep = np.sum(mask_invalid_pep)\n            if n_invalid_pep &gt; 0:\n                print(f\"{format_log_prefix('info',2)} {n_invalid_pep} peptides were not comparable (zero or NaN mean in one group).\")\n\n            # Map peptides to proteins\n            pep_to_prot = utils.get_pep_prot_mapping(self, return_series=True)\n\n            # Aggregate peptide log2FCs into protein-level log2FCs\n            prot_log2fc = pd.Series(index=self.prot.var_names, dtype=float)\n            not_comparable_prot = []\n\n            for prot in self.prot.var_names:\n                matching_peptides = pep_to_prot[pep_to_prot == prot].index\n                if len(matching_peptides) == 0:\n                    continue\n\n                idxs = self.pep.var_names.get_indexer(matching_peptides)\n                valid_idxs = idxs[idxs &gt;= 0]\n                if len(valid_idxs) == 0:\n                    continue\n\n                valid_log2fc = pep_log2fc[valid_idxs]\n\n                if np.all(np.isnan(valid_log2fc)):\n                    prot_log2fc[prot] = np.nan\n                    not_comparable_prot.append(prot)\n                else:\n                    prot_log2fc[prot] = np.nanmedian(pep_log2fc[valid_idxs])\n\n            log2fc_vals = prot_log2fc.values\n            if len(not_comparable_prot) &gt; 0:\n                print(f\"{format_log_prefix('info',2)} {len(not_comparable_prot)} proteins were not comparable (all peptides invalid or missing).\")\n\n        else:\n            raise ValueError(f\"Unsupported fold_change_mode: {fold_change_mode}\")\n\n        # --- Statistical test ---\n        pvals = []\n        stats = []\n        for i in range(data1.shape[1]):\n            x1, x2 = data1[:, i], data2[:, i]\n            try:\n                if method == 'ttest':\n                    res = ttest_ind(x1, x2, nan_policy='omit')\n                elif method == 'mannwhitneyu':\n                    res = mannwhitneyu(x1, x2, alternative='two-sided')\n                elif method == 'wilcoxon':\n                    res = wilcoxon(x1, x2)\n                else:\n                    raise ValueError(f\"Unsupported test method: {method}\")\n                pvals.append(res.pvalue)\n                stats.append(res.statistic)\n            except Exception as e:\n                pvals.append(np.nan)\n                stats.append(np.nan)\n\n        # --- Compile results ---\n        var = self.prot.var.copy()\n        df_stats = pd.DataFrame(index=self.prot.var_names)\n        df_stats['Genes'] = var['Genes'] if 'Genes' in var.columns else var.index\n        df_stats[group1_string] = np.nanmean(data1, axis=0)\n        df_stats[group2_string] = np.nanmean(data2, axis=0)\n        df_stats['log2fc'] = log2fc_vals\n        df_stats['p_value'] = pvals\n        df_stats['test_statistic'] = stats\n\n        df_stats['-log10(p_value)'] = -np.log10(df_stats['p_value'].replace(0, np.nan).astype(float))\n        df_stats['significance_score'] = df_stats['-log10(p_value)'] * df_stats['log2fc']\n        df_stats['significance'] = 'not significant'\n        mask_not_comparable = df_stats['log2fc'].isna()\n        df_stats.loc[mask_not_comparable, 'significance'] = 'not comparable'\n        df_stats.loc[(df_stats['p_value'] &lt; pval) &amp; (df_stats['log2fc'] &gt; log2fc), 'significance'] = 'upregulated'\n        df_stats.loc[(df_stats['p_value'] &lt; pval) &amp; (df_stats['log2fc'] &lt; -log2fc), 'significance'] = 'downregulated'\n        df_stats['significance'] = pd.Categorical(df_stats['significance'], categories=['upregulated', 'downregulated', 'not significant', 'not comparable'], ordered=True)\n\n        df_stats = df_stats.sort_values(by='significance')\n\n        # --- Store and return ---\n        self._stats[comparison_string] = df_stats # type: ignore[attr-defined]\n        self._append_history(f\"prot: DE for {class_type} {values} using {method} and fold_change_mode='{fold_change_mode}'. Stored in .stats['{comparison_string}'].\") # type: ignore[attr-defined], HistoryMixin\n\n        sig_counts = df_stats['significance'].value_counts().to_dict()\n        n_up = sig_counts.get('upregulated', 0)\n        n_down = sig_counts.get('downregulated', 0)\n        n_ns = sig_counts.get('not significant', 0)\n\n        print(f\"{format_log_prefix('result_only', indent=2)} DE complete. Results stored in:\")\n        print(f'       \u2022 .stats[\"{comparison_string}\"]')\n        print(f\"       \u2022 Columns: log2fc, p_value, significance, etc.\")\n        print(f\"       \u2022 Upregulated: {n_up} | Downregulated: {n_down} | Not significant: {n_ns}\")\n\n        return df_stats\n\n    # TODO: Need to figure out how to make this interface with plot functions, probably do reordering by each class_value within the loop?\n    def rank(self, classes = None, on = 'protein', layer = \"X\"):\n        \"\"\"\n        Rank proteins or peptides by average abundance across sample groups.\n\n        This method computes the average and standard deviation for each feature within \n        each group and assigns a rank (highest to lowest) based on the group-level mean.\n        The results are stored in `.var` with one set of columns per group.\n\n        Args:\n            classes (str or list of str, optional): Sample-level class/grouping column(s) in `.obs`.\n            on (str): Whether to compute ranks on \"protein\" or \"peptide\" data.\n            layer (str): Name of the data layer to use (default is \"X\").\n\n        Returns:\n            None\n\n        Example:\n            Rank proteins by average abundance across treatment groups:\n                ```python\n                pdata.rank(classes=\"treatment\", on=\"protein\", layer=\"X_norm\")\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        adata = self.prot if on == 'protein' else self.pep\n        classes_list = utils.get_classlist(adata, classes)\n\n        for class_value in classes_list:\n            rank_data = utils.resolve_class_filter(adata, classes, class_value)\n            if layer == \"X\":\n                layer_data = rank_data.X.toarray()\n            elif layer in rank_data.layers:\n                layer_data = rank_data.layers[layer].toarray()\n            else:\n                raise ValueError(f\"Layer '{layer}' not found in layers.\")\n\n            # Convert sparse to dense if needed\n            if hasattr(layer_data, 'toarray'):\n                layer_data = layer_data.toarray()\n\n            # Transpose to get DataFrame of shape (features, samples)\n            rank_df = pd.DataFrame(layer_data.T, index=rank_data.var.index, columns=rank_data.obs_names)\n\n            # Compute stats\n            avg_col = f\"Average: {class_value}\"\n            std_col = f\"Stdev: {class_value}\"\n            rank_col = f\"Rank: {class_value}\"\n\n            with np.errstate(invalid='ignore', divide='ignore'):\n                rank_df[avg_col] = np.nanmean(layer_data, axis=0)\n                rank_df[std_col] = np.nanstd(layer_data, axis=0)\n\n            # Sort by average (descending), assign rank\n            rank_df.sort_values(by=avg_col, ascending=False, inplace=True)\n            rank_df[rank_col] = np.where(rank_df[avg_col].isna(), np.nan, np.arange(1, len(rank_df) + 1))\n\n            # Reindex back to original order in adata.var\n            rank_df = rank_df.reindex(adata.var.index)\n\n            adata.var[avg_col] = rank_df[avg_col]\n            adata.var[std_col] = rank_df[std_col]\n            adata.var[rank_col] = rank_df[rank_col]\n\n        self._history.append(f\"{on}: Ranked {layer} data. Ranking, average and stdev stored in var.\") # type: ignore[attr-defined], HistoryMixin\n\n    def impute(self, classes=None, layer=\"X\", method='mean', on='protein', min_scale=1, set_X=True, **kwargs):\n        \"\"\"\n        Impute missing values across samples globally or within groups.\n\n        This method imputes missing values in the specified data layer using one of several strategies.\n        It supports both global (across all samples) and group-wise imputation based on sample classes.\n\n        Args:\n            classes (str or list of str, optional): Sample-level class/grouping column(s). If None, imputation is global.\n            layer (str): Data layer to impute from (default is \"X\").\n            method (str): Imputation strategy to use. Options include:\n\n                - \"mean\": Fill missing values with the mean of each feature.\n                - \"median\": Fill missing values with the median of each feature.\n                - \"min\": Fill with the minimum observed value (0 if all missing).\n                - \"knn\": Use K-nearest neighbors (only supported for global imputation).\n\n            on (str): Whether to impute \"protein\" or \"peptide\" data.\n            min_scale (float): Scaled multiplication of minimum value for imputation, i.e. 0.2 would be 20% of minimum value (default is 1).\n            set_X (bool): If True, updates `.X` to use the imputed result.\n            **kwargs: Additional arguments passed to the imputer (e.g., `n_neighbors` for KNN).\n\n        Returns:\n            None\n\n        Example:\n            Globally impute missing values using the median strategy:\n                ```python\n                pdata.impute(method=\"median\", on=\"protein\")\n                ```\n\n            Group-wise imputation based on treatment:\n                ```python\n                pdata.impute(classes=\"treatment\", method=\"mean\", on=\"protein\")\n                ```\n\n        Note:\n            - KNN imputation is only supported for global (non-grouped) mode.\n            - Features that are entirely missing within a group or across all samples are skipped and preserved as NaN.\n            - Imputed results are stored in a new layer named `\"X_impute_&lt;method&gt;\"`.\n            - Imputation summaries are printed to the console by group or overall.\n        \"\"\"\n        from sklearn.impute import SimpleImputer, KNNImputer\n        from scipy import sparse\n        from scpviz import utils\n\n\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            return\n\n        adata = self.prot if on == 'protein' else self.pep\n        if layer != \"X\" and layer not in adata.layers:\n            raise ValueError(f\"Layer '{layer}' not found in .{on}.\")\n\n        impute_data = adata.layers[layer] if layer != \"X\" else adata.X\n        was_sparse = sparse.issparse(impute_data)\n        impute_data = impute_data.toarray() if was_sparse else impute_data.copy()\n        original_data = impute_data.copy()\n\n        layer_name = f\"X_impute_{method}\"\n\n        if method not in {\"mean\", \"median\", \"min\",\"knn\"}:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        if classes is None:\n            # Global imputation\n            if method == 'min':\n                min_vals = np.nanmin(impute_data, axis=0)\n                min_vals = np.where(np.isnan(min_vals), 0, min_vals)\n                min_vals = min_vals * min_scale\n                mask = np.isnan(impute_data)\n                impute_data[mask] = np.take(min_vals, np.where(mask)[1])\n            elif method == 'knn':\n                n_neighbors = kwargs.get('n_neighbors', 3)\n                imputer = KNNImputer(n_neighbors=n_neighbors)\n                impute_data = imputer.fit_transform(impute_data)\n            else:\n                imputer = SimpleImputer(strategy=method, keep_empty_features=True)\n                nan_columns = np.isnan(impute_data).all(axis=0)  # features fully missing in this group\n                impute_data = imputer.fit_transform(impute_data)\n                impute_data[:, nan_columns] = np.nan\n\n            min_message = \"\" if method != 'min' else f\"Minimum scaled by {min_scale}.\"\n            print(f\"{format_log_prefix('user')} Global imputation using '{method}'. Layer saved as '{layer_name}'. {min_message}\")\n            skipped_features = np.sum(np.isnan(impute_data).all(axis=0))\n\n        else:\n            # Group-wise imputation\n            if method == 'knn':\n                raise ValueError(\"KNN imputation is not supported for group-wise imputation.\")\n\n            sample_names = utils.get_samplenames(adata, classes)\n            sample_names = np.array(sample_names)\n            unique_groups = np.unique(sample_names)\n\n            for group in unique_groups:\n                idx = np.where(sample_names == group)[0]\n                group_data = impute_data[idx, :]\n\n                if method == 'min':\n                    min_vals = np.nanmin(group_data, axis=0)\n                    min_vals = np.where(np.isnan(min_vals), 0, min_vals)\n                    min_vals = min_vals * min_scale\n                    mask = np.isnan(group_data)\n                    group_data[mask] = np.take(min_vals, np.where(mask)[1])\n                    imputed_group = group_data\n                else:\n                    imputer = SimpleImputer(strategy=method, keep_empty_features=True)\n                    nan_columns = np.isnan(group_data).all(axis=0)  # features fully missing in this group\n                    imputed_group = imputer.fit_transform(group_data)\n                    imputed_group[:, nan_columns] = np.nan # restore fully missing features\n\n                impute_data[idx, :] = imputed_group\n\n            min_message = \"\" if method != 'min' else f\"Minimum scaled by {min_scale}.\"\n            print(f\"{format_log_prefix('user')} Group-wise imputation using '{method}' on class(es): {classes}. Layer saved as '{layer_name}'. {min_message}\")\n\n        summary_lines = []\n        if classes is None:\n            num_imputed = np.sum(np.isnan(original_data) &amp; ~np.isnan(impute_data))\n            # Row-wise missingness\n            was_missing = np.isnan(original_data).any(axis=1)\n            now_complete = ~np.isnan(impute_data).any(axis=1)\n            now_incomplete = np.isnan(impute_data).any(axis=1)\n\n            fully_imputed_samples = np.sum(was_missing &amp; now_complete)\n            partially_imputed_samples = np.sum(was_missing &amp; now_incomplete)\n            skipped_features = np.sum(np.isnan(impute_data).all(axis=0))\n\n            summary_lines.append(\n                f\"{format_log_prefix('result_only', indent=2)} {num_imputed} values imputed.\"\n            )\n            summary_lines.append(\n                f\"{format_log_prefix('info_only', indent=2)} {fully_imputed_samples} samples fully imputed, {partially_imputed_samples} samples partially imputed, {skipped_features} skipped feature(s) with all missing values.\"\n            )\n\n        else:\n            sample_names = utils.get_samplenames(adata, classes)\n            sample_names = np.array(sample_names)\n            unique_groups = np.unique(sample_names)\n\n            counts_by_group = {}\n            fully_by_group = {}\n            partial_by_group = {}\n            missing_features_by_group = {}\n            total_samples_by_group = {}\n\n            for group in unique_groups:\n                idx = np.where(sample_names == group)[0]\n                before = original_data[idx, :]\n                after = impute_data[idx, :]\n\n                # count imputed values\n                mask = np.isnan(before) &amp; ~np.isnan(after)\n                counts_by_group[group] = np.sum(mask)\n\n                # count fully and partially imputed samples\n                was_missing = np.isnan(before).any(axis=1)\n                now_complete = ~np.isnan(after).any(axis=1)\n                now_incomplete = np.isnan(after).any(axis=1)\n                now_missing = np.sum(np.isnan(before).all(axis=0))\n\n                fully_by_group[group] = np.sum(was_missing &amp; now_complete)\n                partial_by_group[group] = np.sum(was_missing &amp; now_incomplete)\n                missing_features_by_group[group] = now_missing\n                total_samples_by_group[group] = len(idx)\n\n            # Compute dynamic width based on longest group name\n            group_width = max(max(len(str(g)) for g in unique_groups), 20)\n\n            # Summary totals\n            total = sum(counts_by_group.values())\n            summary_lines.append(f\"{format_log_prefix('result_only', indent=2)} {total} values imputed total.\")\n            summary_lines.append(f\"{format_log_prefix('info_only', indent=2)} Group-wise summary:\")\n\n            # Header row (aligned with computed width)\n            header = (f\"{'Group':&lt;{group_width}} | Values Imputed | Skipped Features | Samples Imputed (Partial,Fully)/Total\")\n            divider = \"-\" * len(header)\n            summary_lines.append(f\"{' ' * 5}{header}\")\n            summary_lines.append(f\"{' ' * 5}{divider}\")\n\n            # Data rows\n            for group in unique_groups:\n                count = counts_by_group[group]\n                fully = fully_by_group[group]\n                partial = partial_by_group[group]\n                skipped = missing_features_by_group[group]\n                total_samples = total_samples_by_group[group]\n                summary_lines.append(\n                    f\"{' ' * 5}{group:&lt;{group_width}} | {count:&gt;14} | {skipped:&gt;16} | {partial:&gt;7}, {fully:&gt;5} / {total_samples:&lt;3}\"\n                )\n\n        print(\"\\n\".join(summary_lines))\n\n        adata.layers[layer_name] = sparse.csr_matrix(impute_data) if was_sparse else impute_data\n\n        if set_X:\n            self.set_X(layer=layer_name, on=on) # type: ignore[attr-defined], EditingMixin\n\n        self._history.append( # type: ignore[attr-defined]\n            f\"{on}: Imputed layer '{layer}' using '{method}' (grouped by {classes if classes else 'ALL'}). Stored in '{layer_name}'.\"\n        )\n\n    def neighbor(self, on = 'protein', layer = \"X\", use_rep='X_pca', user_indent=0,**kwargs):\n        \"\"\"\n        Compute a neighbor graph based on protein or peptide data.\n\n        This method builds a nearest-neighbors graph for downstream analysis using \n        `scanpy.pp.neighbors`. It optionally performs PCA before constructing the graph \n        if a valid representation is not already available.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use (default is \"X\").\n            use_rep (str): Key in `.obsm` to use for computing neighbors. Default is `\"X_pca\"`.\n                If `\"X_pca\"` is requested but not found, PCA will be run automatically.\n            **kwargs: Additional keyword arguments passed to `scanpy.pp.neighbors()`.\n\n        Returns:\n            None\n\n        Example:\n            Compute neighbors using default PCA representation:\n                ```python\n                pdata.neighbor(on=\"protein\", layer=\"X\")\n                ```\n\n            Use a custom representation stored in `.obsm[\"X_umap\"]`:\n                ```python\n                pdata.neighbor(on=\"protein\", use_rep=\"X_umap\", n_neighbors=15)\n                ```\n\n        Note:\n            - The neighbor graph is stored in `.obs[\"distances\"]` and `.obs[\"connectivities\"]`.\n            - Neighbor metadata is stored in `.uns[\"neighbors\"]`.\n            - Automatically calls `self.set_X()` if a non-default layer is specified.\n            - PCA is computed automatically if `use_rep='X_pca'` and not already present.\n\n        Todo:\n            Allow users to supply a custom `KNeighborsTransformer` or precomputed neighbor graph.\n                ```python\n                from sklearn.neighbors import KNeighborsTransformer\n                transformer = KNeighborsTransformer(n_neighbors=10, metric='manhattan', algorithm='kd_tree')\n                ```\n        \"\"\"\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on.lower() in [\"prot\", \"protein\"]:\n            adata = self.prot\n        elif on.lower() in [\"pep\", \"peptide\"]:\n            adata = self.pep\n\n        if layer == \"X\":\n            # do nothing\n            pass\n        elif layer in adata.layers.keys():\n            self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n        log_prefix = format_log_prefix(\"user\") if user_indent == 0 else format_log_prefix(\"user_only\",2)\n        print(f\"{log_prefix} Computing neighbors [{on}] using layer: {layer}\")\n\n        if use_rep == 'X_pca':\n            if 'pca' not in adata.uns:\n                print(f\"{format_log_prefix('info_only',indent=2)} PCA not found in AnnData object. Running PCA with default settings.\")\n                self.pca(on = on, layer = layer)\n        else:\n            if use_rep not in adata.obsm:\n                raise ValueError(f\"PCA key '{use_rep}' not found in obsm. Please run PCA first and specify a valid key.\")\n            print(f\"{format_log_prefix('info_only',indent=2)} Using '{use_rep}' found in obsm for neighbor graph.\")\n\n        if use_rep == 'X_pca':\n            sc.pp.neighbors(adata, **kwargs)\n        else:\n            sc.pp.neighbors(adata, use_rep=use_rep, **kwargs)\n\n        self._append_history(f'{on}: Neighbors fitted on {layer}, using {use_rep}, stored in obs[\"distances\"] and obs[\"connectivities\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only',indent=2)} Neighbors computed on {layer}, using {use_rep}. Results stored in:\")\n        print(f\"       \u2022 obs['distances'] (pairwise distances)\")\n        print(f\"       \u2022 obs['connectivities'] (connectivity graph)\")\n        print(f\"       \u2022 uns['neighbors'] (neighbor graph metadata)\")\n\n    def leiden(self, on = 'protein', layer = \"X\", **kwargs):\n        \"\"\"\n        Perform Leiden clustering on protein or peptide data.\n\n        This method runs community detection using the Leiden algorithm based on a precomputed\n        neighbor graph using `scanpy.tl.leiden()`. If neighbors are not already computed, they will be generated automatically.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use for clustering (default is \"X\").\n            **kwargs: Additional keyword arguments passed to `scanpy.tl.leiden()`.\n\n        Returns:\n            None\n\n        Example:\n            Perform Leiden clustering using the default PCA-based neighbors:\n                ```python\n                pdata.leiden(on=\"protein\", layer=\"X\", resolution=0.25)\n                ```\n\n        Note:\n            - Cluster labels are stored in `.obs[\"leiden\"]`.\n            - Neighbor graphs are automatically computed if not present in `.uns[\"neighbors\"]`.\n            - Automatically sets `.X` to the specified layer if it is not already active.\n        \"\"\"\n        # uses sc.tl.leiden with default resolution of 0.25\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein':\n            adata = self.prot\n        elif on == 'peptide':\n            adata = self.pep\n\n        log_prefix = format_log_prefix(\"user\")\n        print(f\"{log_prefix} Performing Leiden clustering [{on}] using layer: {layer}\")\n\n        if 'resolution' in kwargs:\n            resolution = kwargs.pop(\"resolution\", 0.25)\n\n        if 'neighbors' not in adata.uns:\n            print(f\"{format_log_prefix('info_only', indent=2)} Neighbors not found in AnnData object. Running neighbors with default settings.\")\n            self.neighbor(on = on, layer = layer, **kwargs)\n\n        if layer == \"X\":\n            # do nothing\n            pass\n        elif layer in adata.layers.keys():\n            self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n        sc.tl.leiden(adata, resolution)\n\n        self._append_history(f'{on}: Leiden clustering fitted on {layer}, stored in obs[\"leiden\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only', indent=2)} Leiden clustering complete. Results stored in:\")\n        print(f\"       \u2022 obs['leiden'] (cluster labels)\")\n\n    def umap(self, on = 'protein', layer = \"X\", **kwargs):\n        \"\"\"\n        Compute UMAP dimensionality reduction on protein or peptide data.\n\n        This method runs UMAP (Uniform Manifold Approximation and Projection) on the selected data layer using `scanpy.tl.umap()`.\n        If neighbor graphs are not already computed, they will be generated automatically.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use for UMAP (default is \"X\").\n            **kwargs: Additional keyword arguments passed to `scanpy.tl.umap()`, `scanpy.tl.neighbor()` or the scpviz `pca` function.\n                Example:\n                    \"n_neighbors\": neighbor argument\n                    \"min_dist\": umap argument\n                    \"metric\": neighbor argument\n                    \"spread\": umap argument\n                    \"random_state\": umap argument\n                    \"n_pcs\": neighbor argument\n\n        Returns:\n            None\n\n        Example:\n            Run UMAP using default settings:\n                ```python\n                pdata.umap(on=\"protein\", layer=\"X\")\n                ```\n        Note:\n            - UMAP coordinates are stored in `.obsm[\"X_umap\"]`.\n            - UMAP settings are stored in `.uns[\"umap\"]`.\n            - Automatically computes neighbor graphs if not already available.\n            - Will call `.set_X()` if a non-default layer is used.\n        \"\"\"\n        # uses sc.tl.umap\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein':\n            adata = self.prot\n        elif on == 'peptide':\n            adata = self.pep\n\n        log_prefix = format_log_prefix(\"user\")\n        print(f\"{log_prefix} Computing UMAP [{on}] using layer: {layer}\")\n\n        if \"n_neighbors\" in kwargs or \"metric\" in kwargs or \"n_pcs\" in kwargs:\n                    n_neighbors = kwargs.pop(\"n_neighbors\", None)\n                    metric = kwargs.pop(\"metric\", None)\n                    n_pcs = kwargs.pop(\"n_pcs\", None)\n\n                    # Prepare a readable message\n                    neighbor_args = []\n                    if n_neighbors is not None:\n                        neighbor_args.append(f\"n_neighbors={n_neighbors}\")\n                    else:\n                        n_neighbors = 15  # default value\n                    if metric is not None:\n                        neighbor_args.append(f\"metric='{metric}'\")\n                    else:\n                        metric = \"euclidean\"  # default value\n                    if n_pcs is not None:\n                        neighbor_args.append(f\"n_pcs={n_pcs}\")\n                    else:\n                        n_pcs = 50\n                    arg_str = \", \".join(neighbor_args)\n\n                    print(f\"{format_log_prefix('info_only', indent=2)} {arg_str} provided. \"\n                        f\"Re-running neighbors with these settings before UMAP.\")\n\n                    self.neighbor(on=on, layer=layer, n_neighbors=n_neighbors, metric=metric, user_indent=2)\n                    self._append_history(f\"{on}: Neighbors re-computed with {arg_str} before UMAP\")  # type: ignore[attr-defined], HistoryMixin\n        else:\n            # check if neighbor has been run before, look for distances and connectivities in obsp\n            if 'neighbors' not in adata.uns:\n                print(f\"{format_log_prefix('info_only', indent=2)} Neighbors not found in AnnData object. Running neighbors with default settings.\")\n                self.neighbor(on = on, layer = layer)\n                self._append_history(f\"{on}: Neighbors computed with default settings before UMAP\")  # type: ignore[attr-defined], HistoryMixin\n            else:\n                print(f\"{format_log_prefix('info_only', indent=2)} Using existing neighbors found in AnnData object.\")\n\n        if layer == \"X\":\n            # do nothing\n            pass\n        elif layer in adata.layers.keys():\n            self.set_X(layer = layer, on = on) # type: ignore[attr-defined], EditingMixin\n\n        sc.tl.umap(adata, **kwargs)\n\n        self._append_history(f'{on}: UMAP fitted on {layer}, stored in obsm[\"X_umap\"] and uns[\"umap\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only', indent=2)} UMAP complete. Results stored in:\")\n        print(f\"       \u2022 obsm['X_umap'] (UMAP coordinates)\")\n        print(f\"       \u2022 uns['umap'] (UMAP settings)\")\n\n    def pca(self, on = 'protein', layer = \"X\", **kwargs):\n        \"\"\"\n        Perform PCA (Principal Component Analysis) on protein or peptide data.\n\n        This method performs PCA on the selected data layer, after z-score normalization and removal of\n        NaN-containing features. The results are stored in `.obsm[\"X_pca\"]` and `.uns[\"pca\"]`.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data.\n            layer (str): Data layer to use for PCA (default is \"X\").\n            **kwargs: Additional keyword arguments passed to `scanpy.tl.pca()`. For example,\n                `key_added` to store PCA in a different key.\n\n        Returns:\n            None\n\n        Note:\n            - Features (columns) with NaN values are excluded before PCA and then padded with zeros.\n            - PCA scores are stored in `.obsm['X_pca']`.\n            - Principal component loadings, variance ratios, and total variances are stored in `.uns['pca']`.\n            - If you store PCs under a custom key using `key_added`, remember to set `use_rep` when calling `.neighbor()` or `.umap()`.\n        \"\"\"\n\n        # uses sc.tl.pca\n        # for kwargs can use key_added to store PCA in a different key - then for neighbors need to specify key by use_rep\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein':\n            adata = self.prot\n        elif on == 'peptide':\n            adata = self.pep\n\n        # make sample array\n        if layer == \"X\":\n            X = adata.X.toarray()\n        elif layer in adata.layers.keys():\n            X = adata.layers[layer].toarray()\n\n        log_prefix = format_log_prefix(\"user\")\n        print(f\"{log_prefix} Performing PCA [{on}] using layer: {layer}, removing NaN features.\")\n        print(f\"   \ud83d\udd38 BEFORE (samples \u00d7 proteins): {X.shape}\")\n        Xnorm = (X - X.mean(axis=0)) / X.std(axis=0)\n        nan_cols = np.isnan(Xnorm).any(axis=0)\n        Xnorm = Xnorm[:, ~nan_cols]\n        print(f\"   \ud83d\udd38 AFTER  (samples \u00d7 proteins): {Xnorm.shape}\")\n\n        # TODO: fix bug here (ValueError: n_components=59 must be between 1 and min(n_samples, n_features)=31 with svd_solver='arpack')\n        pca_data = sc.tl.pca(Xnorm, return_info=True, **kwargs)\n        adata.obsm['X_pca'] = pca_data[0]\n        PCs = np.zeros((pca_data[1].shape[0], nan_cols.shape[0]))\n\n        # fill back the 0s where column was NaN in the original data, and thus not used in PCA\n        counter = 0\n        for i in range(PCs.shape[1]):\n            if not nan_cols[i]:\n                PCs[:, i] = pca_data[1][:, counter]\n                counter += 1\n\n        adata.uns['pca'] = {'PCs': PCs, 'variance_ratio': pca_data[2], 'variance': pca_data[3]}\n\n        subpdata = \"prot\" if on == 'protein' else \"pep\"\n\n        self._append_history(f'{on}: PCA fitted on {layer}, stored in obsm[\"X_pca\"] and varm[\"PCs\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only',indent=2)} PCA complete, fitted on {layer}. Results stored in:\")\n        print(f\"       \u2022 .{subpdata}.obsm['X_pca']\")\n        print(f\"       \u2022 .{subpdata}.uns['pca'] (includes PCs, variance, variance ratio)\")\n        var_pc1, var_pc2 = pca_data[2][:2]\n        print(f\"       \u2022 Variance explained by PC1/PC2: {var_pc1*100:.2f}% , {var_pc2*100:.2f}%\") \n\n    def harmony(self, key, on = 'protein'):\n        \"\"\"\n        Perform batch correction using Harmony integration.\n\n        This method applies Harmony-based batch correction (via `scanpy.external.pp.harmony_integrate`)\n        on PCA-reduced protein or peptide data to mitigate batch effects across samples.\n\n        Args:\n            key (str): Column name in `.obs` representing the batch variable to correct.\n            on (str): Whether to use \"protein\" or \"peptide\" data. Accepts \"prot\"/\"protein\" or \"pep\"/\"peptide\" (default: \"protein\").\n\n        Returns:\n            None\n\n        Example:\n            Perform Harmony integration on protein-level PCA embeddings:\n                ```python\n                pdata.harmony(key=\"batch\", on=\"protein\")\n                ```\n\n            Apply Harmony on peptide-level data instead:\n                ```python\n                pdata.harmony(key=\"run_id\", on=\"peptide\")\n                ```\n\n        Note:\n            - Harmony requires prior PCA computation. If PCA is missing, it will be computed automatically.\n            - The Harmony-corrected coordinates are stored in `.obsm[\"X_pca_harmony\"]`.\n            - Updates the processing history via `.history`.\n\n        Todo:\n            Add optional arguments for controlling Harmony parameters (e.g., `max_iter_harmony`, `theta`, `lambda`).\n        \"\"\"\n\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein' or on == 'prot':\n            adata = self.prot\n        elif on == 'peptide' or on == 'pep':\n            adata = self.pep\n\n        log_prefix = format_log_prefix(\"user\")\n        print(f\"{log_prefix} Performing Harmony batch correction on [{on}] PCA.\")\n\n        # check if pca has been run before, look for distances and connectivities in obsp\n        if 'pca' not in adata.uns:\n            print(f\"{format_log_prefix('info_only', indent=2)} PCA not found in AnnData object. Running PCA with default settings.\")\n            self.pca(on = on, layer = \"X\")\n\n        # check that key is valid column in adata.obs\n        if key not in adata.obs.columns:\n            raise ValueError(f\"Batch key '{key}' not found in adata.obs.\")\n\n        sc.external.pp.harmony_integrate(adata, key)\n\n        self._append_history(f'{on}: Harmony batch correction applied on key {key}, stored in obsm[\"X_pca_harmony\"] and uns[\"umap\"]') # type: ignore[attr-defined], HistoryMixin\n        print(f\"{format_log_prefix('result_only', indent=2)} Harmony batch correction complete. Results stored in:\")\n        print(f\"       \u2022 obsm['X_pca_harmony'] (PCA coordinates)\")\n\n    def nanmissingvalues(self, on = 'protein', limit = 0.5):\n        \"\"\"\n        Set columns (proteins or peptides) with excessive missing values to NaN.\n\n        This method scans all features and replaces their corresponding columns with NaN\n        if the fraction of missing values exceeds the given threshold. It helps ensure\n        downstream normalization and imputation steps are applied to meaningful features only.\n\n        Args:\n            on (str): Whether to use \"protein\" or \"peptide\" data. Accepts \"prot\"/\"protein\" or \"pep\"/\"peptide\" (default: \"protein\").\n            limit (float): Proportion threshold for missing values (default: 0.5). \n                Features with more than `limit \u00d7 100%` missing values are set entirely to NaN.\n\n        Returns:\n            None\n\n        !!! warning \"Deprecation Notice\"\n            This function may be deprecated in future releases.  \n            Use [`annotate_found`](reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing_mixins.annotate_found)  \n            and [`filter_prot_found`](reference/pAnnData/editing_mixins/#src.scpviz.pAnnData.editing_mixins.filter_prot_found)  \n            for more robust and configurable detection-based filtering.\n\n        Example:\n            Mask proteins with more than 50% missing values:\n                ```python\n                pdata.nanmissingvalues(on=\"protein\", limit=0.5)\n                ```\n\n            Apply the same filter for peptide-level data:\n                ```python\n                pdata.nanmissingvalues(on=\"peptide\", limit=0.3)\n                ```\n\n        Note:\n            - The missing-value fraction is computed per feature across all samples.\n            - This operation modifies the `.X` matrix in-place.\n            - The updated data are stored back into `.prot` or `.pep`.\n        \"\"\"\n        import scipy.sparse\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            pass\n\n        if on == 'protein':\n            adata = self.prot\n\n        elif on == 'peptide':\n            adata = self.pep\n\n        if scipy.sparse.issparse(adata.X):\n            X = adata.X.toarray()\n        else:\n            X = adata.X\n        missing_proportion = np.isnan(X).mean(axis=0)\n        columns_to_nan = missing_proportion &gt; limit\n        X[:, columns_to_nan] = np.nan\n        adata.X = scipy.sparse.csr_matrix(X) if scipy.sparse.issparse(adata.X) else X\n\n        if on == 'protein':\n            self.prot = adata\n        elif on == 'peptide':\n            self.pep = adata\n\n    def normalize(self, classes = None, layer = \"X\", method = 'sum', on = 'protein', set_X = True, force = False, use_nonmissing = False, **kwargs):  \n        \"\"\"\n        Normalize sample intensities across protein or peptide data.\n\n        This method performs global or group-wise normalization of the selected data layer.\n        It supports multiple normalization strategies ranging from simple scaling\n        (e.g., sum, median) to advanced approaches such as `reference_feature` and\n        [`directlfq`]((https://doi.org/10.1016/j.mcpro.2023.100581)).\n\n        Args:\n            classes (str or list, optional): Sample-level grouping column(s) in `.obs` to\n                perform group-wise normalization. If None, normalization is applied globally.\n            layer (str, optional): Data layer to normalize from (default: `\"X\"`).\n            method (str, optional): Normalization strategy to apply. Options include:\n                `'sum'`, `'median'`, `'mean'`, `'max'`, `'reference_feature'`,\n                `'robust_scale'`, `'quantile_transform'`, `'directlfq'`.\n            on (str, optional): Whether to use `\"protein\"` or `\"peptide\"` data.\n            set_X (bool, optional): Whether to set `.X` to the normalized result (default: True).\n            force (bool, optional): Proceed with normalization even if samples exceed the\n                allowed fraction of missing values (default: False).\n            use_nonmissing (bool, optional): If True, only use columns with no missing values\n                across all samples when computing scaling factors (default: False).\n            **kwargs: Additional keyword arguments for normalization methods.\n                - `reference_columns` (list): For `'reference_feature'`, specify columns or\n                gene names to normalize against.\n                - `max_missing_fraction` (float): Maximum allowed fraction of missing values\n                per sample (default: 0.5).\n                - `n_neighbors` (int): For methods requiring neighbor-based computations.\n                - `input_type_to_use` (str): For `'directlfq'`, specify `'pAnnData'`,\n                `'diann_precursor_ms1'`, or `'diann_precursor_ms1_and_ms2'`.\n                - `path` (str): For `'directlfq'`, path to the `report.tsv` or `report.parquet`\n                file from DIA-NN output.\n\n        Returns:\n            None\n\n        Example:\n            Perform global normalization using the median intensity:\n                ```python\n                pdata.normalize(on=\"protein\", method=\"median\")\n                ```\n\n            Apply group-wise normalization by treatment class using sum-scaling:\n                ```python\n                pdata.normalize(classes=\"treatment\", method=\"sum\", on=\"protein\")\n                ```\n\n            Run reference-feature normalization using specific genes:\n                ```python\n                pdata.normalize(\n                    on=\"protein\",\n                    method=\"reference_feature\",\n                    reference_columns=[\"ACTB\", \"GAPDH\"]\n                )\n                ```\n\n        !!! tip \"About `directlfq` normalization\"\n            - The `directlfq` method aggregates peptide-level data to protein-level intensities\n            and stores results in a new protein-layer (e.g. `'X_norm_directlfq'`).\n            - It does not support group-wise normalization.\n            - Processing time may scale with dataset size.\n            - For algorithmic and benchmarking details, see:  \n            **Ammar, Constantin et al. (2023)**  \n            *Accurate Label-Free Quantification by directLFQ to Compare Unlimited Numbers of Proteomes.*  \n            *Molecular &amp; Cellular Proteomics*, 22(7):100581.  \n            [https://doi.org/10.1016/j.mcpro.2023.100581](https://doi.org/10.1016/j.mcpro.2023.100581)\n\n\n\n        Note:\n            - Results are stored in a new layer named `'X_norm_&lt;method&gt;'`.\n            - The normalized layer replaces `.X` if `set_X=True`.\n            - Normalization operations are recorded in `.history`.\n            - For consistency across runs, consider running `.impute()` before normalization.\n\n        Todo:\n            - Add optional z-score and percentile normalization modes.\n            - Add support for specifying external scaling factors.\n        \"\"\"\n\n\n        if not self._check_data(on): # type: ignore[attr-defined], ValidationMixin\n            return\n\n        adata = self.prot if on == 'protein' else self.pep\n        if layer != \"X\" and layer not in adata.layers:\n            raise ValueError(f\"Layer {layer} not found in .{on}.\")\n\n        normalize_data = adata.layers[layer] if layer != \"X\" else adata.X\n        was_sparse = sparse.issparse(normalize_data)\n        normalize_data = normalize_data.toarray() if was_sparse else normalize_data.copy()\n        original_data = normalize_data.copy()\n\n        layer_name = 'X_norm_' + method\n        normalize_funcs = ['sum', 'median', 'mean', 'max', 'reference_feature', 'robust_scale', 'quantile_transform','directlfq']\n\n        if method not in normalize_funcs:\n            raise ValueError(f\"Unsupported normalization method: {method}\")\n\n        # Special handling for directlfq\n        if method == \"directlfq\":\n            if classes is not None:\n                print(f\"{format_log_prefix('warn')} 'directlfq' does not support group-wise normalization. Proceeding with global normalization.\")\n                classes = None\n\n            print(f\"{format_log_prefix('user')} Running directlfq normalization on peptide-level data.\")\n            print(f\"{format_log_prefix('info_only', indent=2)} Note: please be patient, directlfq can take a minute to run depending on data size. Output files will be produced.\")\n            normalize_data = self._normalize_helper_directlfq(**kwargs)\n\n            adata = self.prot  # directlfq always outputs protein-level intensities\n            adata.layers[layer_name] = sparse.csr_matrix(normalize_data) if was_sparse else normalize_data\n\n            if set_X:\n                self.set_X(layer=layer_name, on=\"protein\")  # type: ignore[attr-defined]\n\n            self._history.append(  # type: ignore[attr-defined]\n                f\"protein: Normalized layer using directlfq (input_type={kwargs.get('input_type_to_use', 'default')}). Stored in `{layer_name}`.\"\n            )\n            print(f\"{format_log_prefix('result_only', indent=2)} directlfq normalization complete. Results are stored in layer '{layer_name}'.\")\n            return\n\n        # --- standard normalization ---\n        # Build the header message early\n        if classes is None:\n            msg = f\"{format_log_prefix('user')} Global normalization using '{method}'\"\n        else:\n            msg = f\"{format_log_prefix('info_only')} Group-wise normalization using '{method}' on class(es): {classes}\"\n\n        if use_nonmissing and method in {'sum', 'mean', 'median', 'max'}:\n            msg += \" (using only fully observed columns)\"\n        msg += f\". Layer will be saved as '{layer_name}'.\"\n\n        # \u2705 Print message before checking for missing values\n        print(msg)\n\n        # Check for bad rows (too many missing values)\n        missing_fraction = np.isnan(normalize_data).sum(axis=1) / normalize_data.shape[1]\n        max_missing_fraction = kwargs.pop(\"max_missing_fraction\", 0.5)\n        bad_rows_mask = missing_fraction &gt; max_missing_fraction\n\n        if np.any(bad_rows_mask):\n            n_bad = np.sum(bad_rows_mask)\n            print(f\"{format_log_prefix('warn',2)} {n_bad} sample(s) have &gt;{int(max_missing_fraction*100)}% missing values.\")\n            print(\"     Try running `.impute()` before normalization. Suggest to use the flag `use_nonmissing=True` to normalize using only consistently observed proteins.\")\n            if not force:\n                print(\"     \u27a1\ufe0f Use `force=True` to proceed anyway.\")\n                return\n            print(f\"{format_log_prefix('warn',2)} Proceeding with normalization despite bad rows (force=True).\")\n\n        if classes is None:\n            normalize_data = self._normalize_helper(normalize_data, method, use_nonmissing=use_nonmissing, **kwargs)\n        else:\n            # Group-wise normalization\n            sample_names = utils.get_samplenames(adata, classes)\n            sample_names = np.array(sample_names)\n            unique_groups = np.unique(sample_names)\n\n            for group in unique_groups:\n                idx = np.where(sample_names == group)[0]\n                group_data = normalize_data[idx, :]\n\n                normalized_group = self._normalize_helper(group_data, method=method, use_nonmissing=use_nonmissing, **kwargs)\n                normalize_data[idx, :] = normalized_group\n\n        # summary printout\n        summary_lines = []\n        if classes is None:\n            summary_lines.append(f\"{format_log_prefix('result_only', indent=2)} Normalized all {normalize_data.shape[0]} samples.\")\n        else:\n            for group in unique_groups:\n                count = np.sum(sample_names == group)\n                summary_lines.append(f\"   - {group}: {count} samples normalized\")\n            summary_lines.insert(0, f\"{format_log_prefix('result_only', indent=2)} Normalized {normalize_data.shape[0]} samples total.\")\n        print(\"\\n\".join(summary_lines))\n\n        adata.layers[layer_name] = sparse.csr_matrix(normalize_data) if was_sparse else normalize_data\n\n        if set_X:\n            self.set_X(layer = layer_name, on = on) # type: ignore[attr-defined], EditingMixin\n\n        # Determine if use_nonmissing note should be added\n        note = \"\"\n        if use_nonmissing and method in {'sum', 'mean', 'median', 'max'}:\n            note = \" (using only fully observed columns)\"\n\n        self._history.append( # type: ignore[attr-defined], HistoryMixin\n            f\"{on}: Normalized layer {layer} using {method}{note} (grouped by {classes}). Stored in `{layer_name}`.\"\n            )\n\n    def _normalize_helper(self, data, method, use_nonmissing, **kwargs):\n        \"\"\"\n        Perform row-wise normalization using a selected method.\n\n        Used internally by `normalize()` to compute per-sample scaling.\n        Supports reference feature scaling, robust methods, and quantile normalization.\n\n        Args:\n            data (np.ndarray): Sample \u00d7 feature data matrix.\n            method (str): Normalization strategy. Options:\n                - 'sum'\n                - 'mean'\n                - 'median'\n                - 'max'\n                - 'reference_feature'\n                - 'robust_scale'\n                - 'quantile_transform'\n            use_nonmissing (bool): If True, computes scaling using only columns with no NaNs.\n\n        Returns:\n            np.ndarray: Normalized data matrix.\n        \"\"\"\n\n        if method in {'sum', 'mean', 'median', 'max'}:\n            reducer = {\n                    'sum': np.nansum,\n                    'mean': np.nanmean,\n                    'median': np.nanmedian,\n                    'max': np.nanmax\n                }[method]\n\n            if use_nonmissing:\n                fully_observed_cols = ~np.isnan(data).any(axis=0)\n                if not np.any(fully_observed_cols):\n                    raise ValueError(\"No fully observed columns available for normalization with `use_nonmissing=True`.\")\n                used_cols = np.where(fully_observed_cols)[0]\n                print(f\"{format_log_prefix('info_only',2)} Normalizing using only fully observed columns: {len(used_cols)}\")\n                row_vals = reducer(data[:, fully_observed_cols], axis=1)\n            else:\n                row_vals = reducer(data, axis=1)\n\n            with np.errstate(divide='ignore', invalid='ignore'):\n                scale = np.nanmax(row_vals) / row_vals\n            scale = np.where(np.isnan(scale), 1.0, scale) # metaboanalyst: scale = 1.0 / row_vals\n            data_norm = data * scale[:, None]\n\n        elif method == 'reference_feature':\n            # norm by reference feature: scale each row s.t. the reference column is the same across all rows (scale to max value of reference column)\n            reference_columns = kwargs.get('reference_columns', [2])\n            reference_method = kwargs.get('reference_method', 'median')  # default to median\n\n            reducer_map = {\n                'mean': np.nanmean,\n                'median': np.nanmedian,\n                'sum': np.nansum\n            }\n\n            if reference_method not in reducer_map:\n                raise ValueError(f\"Unsupported reference method: {reference_method}. Supported methods are: {list(reducer_map.keys())}\")\n            reducer = reducer_map[reference_method]\n\n            # resolve reference column names if needed\n            if isinstance(reference_columns[0], str):\n                gene_to_acc, _ = self.get_gene_maps(on='protein') # type: ignore[attr-defined], IdentifierMixin\n                resolved = utils.resolve_accessions(self.prot, reference_columns, gene_map=gene_to_acc)\n                reference_acc = [ref for ref in resolved if ref in self.prot.var.index]\n                reference_columns = [self.prot.var.index.get_loc(ref) for ref in reference_acc]\n                print(f\"{format_log_prefix('info')} Normalizing using found reference columns: {reference_acc}\")\n                self._history.append(f\"Used reference_feature normalization with resolved accessions: {resolved}\") # type: ignore[attr-defined]\n            else:\n                reference_columns = [int(ref) for ref in reference_columns]\n                reference_acc = [self.prot.var.index[ref] for ref in reference_columns if ref &lt; self.prot.shape[1]]\n                print(f\"{format_log_prefix('info')} Normalizing using reference columns: {reference_acc}\")\n                self._history.append(f\"Used reference_feature normalization with resolved accessions: {reference_acc}\") # type: ignore[attr-defined]\n\n            scaling_factors = np.nanmean(np.nanmax(data[:, reference_columns], axis=0) / (data[:, reference_columns]), axis=1)\n\n            nan_rows = np.where(np.isnan(scaling_factors))[0]\n            if nan_rows.size &gt; 0:\n                print(f\"{format_log_prefix('warn')} Rows {list(nan_rows)} have all missing reference values.\")\n                print(f\"{format_log_prefix('info')} Falling back to row median normalization for these rows.\")\n\n                fallback = np.nanmedian(data[nan_rows, :], axis=1)\n                fallback[fallback == 0] = np.nan  # avoid division by 0\n                fallback_scale = np.nanmax(fallback) / fallback\n                fallback_scale = np.where(np.isnan(fallback_scale), 1.0, fallback_scale)  # default to 1.0 if all else fails\n\n                scaling_factors[nan_rows] = fallback_scale\n\n            scaling_factors = np.where(np.isnan(scaling_factors), np.nanmean(scaling_factors), scaling_factors)\n            data_norm = data * scaling_factors[:, None]\n\n        elif method == 'robust_scale':\n            # norm by robust_scale: Center to the median and component wise scale according to the interquartile range. See sklearn.preprocessing.robust_scale for more information.\n            from sklearn.preprocessing import robust_scale\n            data_norm = robust_scale(data, axis=1)\n\n        elif method == 'quantile_transform':\n            # norm by quantile_transform: Transform features using quantiles information. See sklearn.preprocessing.quantile_transform for more information.\n            from sklearn.preprocessing import quantile_transform\n            import warnings\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", UserWarning)\n                data_norm = quantile_transform(data, axis=1)\n\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        return data_norm\n\n    def _normalize_helper_directlfq(self, input_type_to_use=\"pAnnData\", path=None, **kwargs):\n        \"\"\"\n        Run directlfq normalization and return normalized protein-level intensities.\n\n        Args:\n            input_type_to_use (str): Either 'pAnnData' (default) or \n                'diann_precursor_ms1_and_ms2'.\n            path (str, optional): Path to DIA-NN report file (required if \n                input_type_to_use='diann_precursor_ms1_and_ms2').\n            **kwargs: Passed to directlfq.lfq_manager.run_lfq().\n\n        Returns:\n            np.ndarray: Normalized data (samples \u00d7 proteins).\n        \"\"\"\n        import directlfq.lfq_manager as lfq_manager\n        import os\n\n        if input_type_to_use == \"diann_precursor_ms1_and_ms2\":\n            if path is None:\n                raise ValueError(\"For input_type_to_use='diann_precursor_ms1_and_ms2', please provide the DIA-NN report path via `path`.\")\n            lfq_manager.run_lfq(path, input_type_to_use=input_type_to_use, **kwargs)\n\n        else:\n            # check if pep exists\n            if self.pep is None:\n                raise ValueError(\"Peptide-level data not found. Please load peptide data before running directlfq normalization.\")\n\n            # Build peptide-level input table from .pep\n            X = self.pep.layers.get(\"X_precursor\", self.pep.X)\n            if not isinstance(X, pd.DataFrame):\n                X = X.toarray() if hasattr(X, \"toarray\") else X\n            X_df = pd.DataFrame(\n                X.T,\n                index=self.pep.var_names,\n                columns=self.pep.obs_names\n            )\n            prot_col = \"Protein.Group\" if \"Protein.Group\" in self.pep.var.columns else \"Master Protein Accessions\"\n            X_df.insert(0, \"protein\", self.pep.var[prot_col].to_list())\n            X_df.insert(1, \"ion\", X_df.index.to_list())\n            X_df.reset_index(drop=True, inplace=True)\n            tmp_file = \"peptide_matrix.aq_reformat.tsv\"\n            X_df.to_csv(tmp_file, sep=\"\\t\", index=False)\n            lfq_manager.run_lfq(tmp_file, **kwargs)\n\n        # Load directlfq output (look for protein_intensities file)\n        out_file = None\n        for f in os.listdir(\".\"):\n            if f.endswith(\"protein_intensities.tsv\"):\n                out_file = f\n        if out_file is None:\n            raise FileNotFoundError(\"directlfq did not produce a '*protein_intensities.tsv' file in current directory.\")\n\n        norm_prot = pd.read_csv(out_file, sep=\"\\t\").set_index(\"protein\")\n        aligned = norm_prot.reindex(\n            index=self.prot.var_names,\n            columns=self.prot.obs_names\n        ).fillna(0)\n\n        return aligned.T.to_numpy()\n\n    def clean_X(self, on='prot', inplace=True, set_to=0, layer=None, to_sparse=False, backup_layer=\"X_preclean\", verbose=True):\n        \"\"\"\n        Replace NaNs in `.X` or a specified layer with a given value (default: 0).\n\n        Optionally backs up the original data to a layer (default: `'X_preclean'`) before overwriting.\n        Typically used to prepare data for scanpy or sklearn functions that cannot handle missing values.\n\n        Args:\n            on (str): Target data to clean, either `'protein'` or `'peptide'`.\n            inplace (bool): If True, update `.X` or `.layers[layer]` in place. If False, return cleaned matrix.\n            set_to (float): Value to replace NaNs with (default: 0.0).\n            layer (str or None): If specified, applies to `.layers[layer]`; otherwise uses `.X`.\n            to_sparse (bool): If True, returns a sparse matrix.\n            backup_layer (str or None): If `inplace=True` and `layer=None`, saves the original `.X` to this layer.\n            verbose (bool): Whether to print summary messages.\n\n        Returns:\n            np.ndarray: Cleaned matrix if `inplace=False`, otherwise `None`.\n        \"\"\"\n        if not self._check_data(on):\n            return\n        if on == 'prot' or on == 'protein':\n            adata = self.prot\n        elif on == 'pep' or on == 'peptide': \n            adata = self.pep\n\n        print(f'{format_log_prefix(\"user\")} Cleaning {on} data: making scanpy compatible, replacing NaNs with {set_to} in {\"layer \" + layer if layer else \".X\"}.')\n\n        # Choose source matrix\n        X = adata.layers[layer] if layer else adata.X\n        is_sparse = sparse.issparse(X)\n\n        # Copy for manipulation\n        X_clean = X.copy()\n        nan_count = 0\n\n        if is_sparse:\n            nan_mask = np.isnan(X_clean.data)\n            nan_count = np.sum(nan_mask)\n            if nan_count &gt; 0:\n                X_clean.data[nan_mask] = set_to\n        else:\n            nan_mask = np.isnan(X_clean)\n            nan_count = np.sum(nan_mask)\n            X_clean[nan_mask] = set_to\n\n        if to_sparse and not is_sparse:\n            X_clean = sparse.csr_matrix(X_clean)\n\n        # Apply result\n        if inplace:\n            if layer:\n                self.prot.layers[layer] = X_clean\n            else:\n                # Save original .X if requested and not already backed up\n                if backup_layer and backup_layer not in self.prot.layers:\n                    self.prot.layers[backup_layer] = self.prot.X.copy()\n                    if verbose:\n                        print(f\"{format_log_prefix('info')} Backed up .X to .layers['{backup_layer}']\")\n                self.prot.X = X_clean\n            if verbose:\n                print(f\"{format_log_prefix('result')} Cleaned {'layer ' + layer if layer else '.X'}: replaced {nan_count} NaNs with {set_to}.\")\n        else:\n            if verbose:\n                print(f\"{format_log_prefix('result')} Returning cleaned matrix: {nan_count} NaNs replaced with {set_to}.\")\n            return X_clean \n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.analysis.AnalysisMixin._normalize_helper","title":"_normalize_helper","text":"<pre><code>_normalize_helper(data, method, use_nonmissing, **kwargs)\n</code></pre> <p>Perform row-wise normalization using a selected method.</p> <p>Used internally by <code>normalize()</code> to compute per-sample scaling. Supports reference feature scaling, robust methods, and quantile normalization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Sample \u00d7 feature data matrix.</p> required <code>method</code> <code>str</code> <p>Normalization strategy. Options: - 'sum' - 'mean' - 'median' - 'max' - 'reference_feature' - 'robust_scale' - 'quantile_transform'</p> required <code>use_nonmissing</code> <code>bool</code> <p>If True, computes scaling using only columns with no NaNs.</p> required <p>Returns:</p> Type Description <p>np.ndarray: Normalized data matrix.</p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def _normalize_helper(self, data, method, use_nonmissing, **kwargs):\n    \"\"\"\n    Perform row-wise normalization using a selected method.\n\n    Used internally by `normalize()` to compute per-sample scaling.\n    Supports reference feature scaling, robust methods, and quantile normalization.\n\n    Args:\n        data (np.ndarray): Sample \u00d7 feature data matrix.\n        method (str): Normalization strategy. Options:\n            - 'sum'\n            - 'mean'\n            - 'median'\n            - 'max'\n            - 'reference_feature'\n            - 'robust_scale'\n            - 'quantile_transform'\n        use_nonmissing (bool): If True, computes scaling using only columns with no NaNs.\n\n    Returns:\n        np.ndarray: Normalized data matrix.\n    \"\"\"\n\n    if method in {'sum', 'mean', 'median', 'max'}:\n        reducer = {\n                'sum': np.nansum,\n                'mean': np.nanmean,\n                'median': np.nanmedian,\n                'max': np.nanmax\n            }[method]\n\n        if use_nonmissing:\n            fully_observed_cols = ~np.isnan(data).any(axis=0)\n            if not np.any(fully_observed_cols):\n                raise ValueError(\"No fully observed columns available for normalization with `use_nonmissing=True`.\")\n            used_cols = np.where(fully_observed_cols)[0]\n            print(f\"{format_log_prefix('info_only',2)} Normalizing using only fully observed columns: {len(used_cols)}\")\n            row_vals = reducer(data[:, fully_observed_cols], axis=1)\n        else:\n            row_vals = reducer(data, axis=1)\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scale = np.nanmax(row_vals) / row_vals\n        scale = np.where(np.isnan(scale), 1.0, scale) # metaboanalyst: scale = 1.0 / row_vals\n        data_norm = data * scale[:, None]\n\n    elif method == 'reference_feature':\n        # norm by reference feature: scale each row s.t. the reference column is the same across all rows (scale to max value of reference column)\n        reference_columns = kwargs.get('reference_columns', [2])\n        reference_method = kwargs.get('reference_method', 'median')  # default to median\n\n        reducer_map = {\n            'mean': np.nanmean,\n            'median': np.nanmedian,\n            'sum': np.nansum\n        }\n\n        if reference_method not in reducer_map:\n            raise ValueError(f\"Unsupported reference method: {reference_method}. Supported methods are: {list(reducer_map.keys())}\")\n        reducer = reducer_map[reference_method]\n\n        # resolve reference column names if needed\n        if isinstance(reference_columns[0], str):\n            gene_to_acc, _ = self.get_gene_maps(on='protein') # type: ignore[attr-defined], IdentifierMixin\n            resolved = utils.resolve_accessions(self.prot, reference_columns, gene_map=gene_to_acc)\n            reference_acc = [ref for ref in resolved if ref in self.prot.var.index]\n            reference_columns = [self.prot.var.index.get_loc(ref) for ref in reference_acc]\n            print(f\"{format_log_prefix('info')} Normalizing using found reference columns: {reference_acc}\")\n            self._history.append(f\"Used reference_feature normalization with resolved accessions: {resolved}\") # type: ignore[attr-defined]\n        else:\n            reference_columns = [int(ref) for ref in reference_columns]\n            reference_acc = [self.prot.var.index[ref] for ref in reference_columns if ref &lt; self.prot.shape[1]]\n            print(f\"{format_log_prefix('info')} Normalizing using reference columns: {reference_acc}\")\n            self._history.append(f\"Used reference_feature normalization with resolved accessions: {reference_acc}\") # type: ignore[attr-defined]\n\n        scaling_factors = np.nanmean(np.nanmax(data[:, reference_columns], axis=0) / (data[:, reference_columns]), axis=1)\n\n        nan_rows = np.where(np.isnan(scaling_factors))[0]\n        if nan_rows.size &gt; 0:\n            print(f\"{format_log_prefix('warn')} Rows {list(nan_rows)} have all missing reference values.\")\n            print(f\"{format_log_prefix('info')} Falling back to row median normalization for these rows.\")\n\n            fallback = np.nanmedian(data[nan_rows, :], axis=1)\n            fallback[fallback == 0] = np.nan  # avoid division by 0\n            fallback_scale = np.nanmax(fallback) / fallback\n            fallback_scale = np.where(np.isnan(fallback_scale), 1.0, fallback_scale)  # default to 1.0 if all else fails\n\n            scaling_factors[nan_rows] = fallback_scale\n\n        scaling_factors = np.where(np.isnan(scaling_factors), np.nanmean(scaling_factors), scaling_factors)\n        data_norm = data * scaling_factors[:, None]\n\n    elif method == 'robust_scale':\n        # norm by robust_scale: Center to the median and component wise scale according to the interquartile range. See sklearn.preprocessing.robust_scale for more information.\n        from sklearn.preprocessing import robust_scale\n        data_norm = robust_scale(data, axis=1)\n\n    elif method == 'quantile_transform':\n        # norm by quantile_transform: Transform features using quantiles information. See sklearn.preprocessing.quantile_transform for more information.\n        from sklearn.preprocessing import quantile_transform\n        import warnings\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UserWarning)\n            data_norm = quantile_transform(data, axis=1)\n\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    return data_norm\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.analysis.AnalysisMixin._normalize_helper_directlfq","title":"_normalize_helper_directlfq","text":"<pre><code>_normalize_helper_directlfq(input_type_to_use='pAnnData', path=None, **kwargs)\n</code></pre> <p>Run directlfq normalization and return normalized protein-level intensities.</p> <p>Parameters:</p> Name Type Description Default <code>input_type_to_use</code> <code>str</code> <p>Either 'pAnnData' (default) or  'diann_precursor_ms1_and_ms2'.</p> <code>'pAnnData'</code> <code>path</code> <code>str</code> <p>Path to DIA-NN report file (required if  input_type_to_use='diann_precursor_ms1_and_ms2').</p> <code>None</code> <code>**kwargs</code> <p>Passed to directlfq.lfq_manager.run_lfq().</p> <code>{}</code> <p>Returns:</p> Type Description <p>np.ndarray: Normalized data (samples \u00d7 proteins).</p> Source code in <code>src/scpviz/pAnnData/analysis.py</code> <pre><code>def _normalize_helper_directlfq(self, input_type_to_use=\"pAnnData\", path=None, **kwargs):\n    \"\"\"\n    Run directlfq normalization and return normalized protein-level intensities.\n\n    Args:\n        input_type_to_use (str): Either 'pAnnData' (default) or \n            'diann_precursor_ms1_and_ms2'.\n        path (str, optional): Path to DIA-NN report file (required if \n            input_type_to_use='diann_precursor_ms1_and_ms2').\n        **kwargs: Passed to directlfq.lfq_manager.run_lfq().\n\n    Returns:\n        np.ndarray: Normalized data (samples \u00d7 proteins).\n    \"\"\"\n    import directlfq.lfq_manager as lfq_manager\n    import os\n\n    if input_type_to_use == \"diann_precursor_ms1_and_ms2\":\n        if path is None:\n            raise ValueError(\"For input_type_to_use='diann_precursor_ms1_and_ms2', please provide the DIA-NN report path via `path`.\")\n        lfq_manager.run_lfq(path, input_type_to_use=input_type_to_use, **kwargs)\n\n    else:\n        # check if pep exists\n        if self.pep is None:\n            raise ValueError(\"Peptide-level data not found. Please load peptide data before running directlfq normalization.\")\n\n        # Build peptide-level input table from .pep\n        X = self.pep.layers.get(\"X_precursor\", self.pep.X)\n        if not isinstance(X, pd.DataFrame):\n            X = X.toarray() if hasattr(X, \"toarray\") else X\n        X_df = pd.DataFrame(\n            X.T,\n            index=self.pep.var_names,\n            columns=self.pep.obs_names\n        )\n        prot_col = \"Protein.Group\" if \"Protein.Group\" in self.pep.var.columns else \"Master Protein Accessions\"\n        X_df.insert(0, \"protein\", self.pep.var[prot_col].to_list())\n        X_df.insert(1, \"ion\", X_df.index.to_list())\n        X_df.reset_index(drop=True, inplace=True)\n        tmp_file = \"peptide_matrix.aq_reformat.tsv\"\n        X_df.to_csv(tmp_file, sep=\"\\t\", index=False)\n        lfq_manager.run_lfq(tmp_file, **kwargs)\n\n    # Load directlfq output (look for protein_intensities file)\n    out_file = None\n    for f in os.listdir(\".\"):\n        if f.endswith(\"protein_intensities.tsv\"):\n            out_file = f\n    if out_file is None:\n        raise FileNotFoundError(\"directlfq did not produce a '*protein_intensities.tsv' file in current directory.\")\n\n    norm_prot = pd.read_csv(out_file, sep=\"\\t\").set_index(\"protein\")\n    aligned = norm_prot.reindex(\n        index=self.prot.var_names,\n        columns=self.prot.obs_names\n    ).fillna(0)\n\n    return aligned.T.to_numpy()\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.base","title":"src.scpviz.pAnnData.base","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.editing","title":"src.scpviz.pAnnData.editing","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.enrichment","title":"src.scpviz.pAnnData.enrichment","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.enrichment._pretty_vs_key","title":"_pretty_vs_key","text":"<pre><code>_pretty_vs_key(k)\n</code></pre> <p>Format a DE contrast key into a human-readable string.</p> <p>This function attempts to convert a string representation of a DE comparison (e.g., a list of dictionaries) into a simplified <code>\"group1 vs group2\"</code> format, using the values from each dictionary in the left and right group.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>str</code> <p>DE key string, typically in the format <code>\"[{{...}}] vs [{{...}}]\"</code>.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A simplified, human-readable version of the DE comparison key.</p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def _pretty_vs_key(k):\n    \"\"\"\n    Format a DE contrast key into a human-readable string.\n\n    This function attempts to convert a string representation of a DE comparison\n    (e.g., a list of dictionaries) into a simplified `\"group1 vs group2\"` format,\n    using the values from each dictionary in the left and right group.\n\n    Args:\n        k (str): DE key string, typically in the format `\"[{{...}}] vs [{{...}}]\"`.\n\n    Returns:\n        str: A simplified, human-readable version of the DE comparison key.\n    \"\"\"\n    import ast\n    try:\n        parts = k.split(\" vs \")\n        left = \"_\".join(str(v) for d in ast.literal_eval(parts[0]) for v in d.values())\n        right = \"_\".join(str(v) for d in ast.literal_eval(parts[1]) for v in d.values())\n        return f\"{left} vs {right}\"\n    except Exception:\n        return k  # fallback to raw key if anything goes wrong\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.enrichment._resolve_de_key","title":"_resolve_de_key","text":"<pre><code>_resolve_de_key(stats_dict, user_key, debug=False)\n</code></pre> <p>Resolve a user-supplied DE key to a valid key stored in <code>.stats[\"de_results\"]</code>.</p> <p>This function matches a flexible, human-readable DE key against the internal keys stored in the DE results dictionary. It supports both raw and pretty-formatted keys, and can handle suffixes like <code>_up</code> or <code>_down</code> for directional analysis.</p> <p>Parameters:</p> Name Type Description Default <code>stats_dict</code> <code>dict</code> <p>Dictionary of DE results (typically <code>pdata.stats[\"de_results\"]</code>).</p> required <code>user_key</code> <code>str</code> <p>User-supplied key to resolve, e.g., \"AS_kd vs AS_sc_down\".</p> required <code>debug</code> <code>bool</code> <p>If True, prints detailed debug output for tracing.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The matching internal DE result key.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching key is found.</p> Source code in <code>src/scpviz/pAnnData/enrichment.py</code> <pre><code>def _resolve_de_key(stats_dict, user_key, debug=False):\n    \"\"\"\n    Resolve a user-supplied DE key to a valid key stored in `.stats[\"de_results\"]`.\n\n    This function matches a flexible, human-readable DE key against the internal keys\n    stored in the DE results dictionary. It supports both raw and pretty-formatted keys,\n    and can handle suffixes like `_up` or `_down` for directional analysis.\n\n    Args:\n        stats_dict (dict): Dictionary of DE results (typically `pdata.stats[\"de_results\"]`).\n        user_key (str): User-supplied key to resolve, e.g., \"AS_kd vs AS_sc_down\".\n        debug (bool): If True, prints detailed debug output for tracing.\n\n    Returns:\n        str: The matching internal DE result key.\n\n    Raises:\n        ValueError: If no matching key is found.\n    \"\"\"\n    import re\n    print(f\"[DEBUG] Resolving user key: {user_key}\") if debug else None\n\n    # Extract suffix\n    suffix = \"\"\n    if user_key.endswith(\"_up\") or user_key.endswith(\"_down\"):\n        user_key, suffix = re.match(r\"(.+)(_up|_down)\", user_key).groups()\n        print(f\"[DEBUG] Split into base='{user_key}', suffix='{suffix}'\") if debug else None\n\n    # Build pretty key mapping\n    pretty_map = {}\n    for full_key in stats_dict:\n        if \"vs\" not in full_key:\n            continue\n\n        if full_key.endswith(\"_up\") or full_key.endswith(\"_down\"):\n            base = full_key.rsplit(\"_\", 1)[0]\n            full_suffix = \"_\" + full_key.rsplit(\"_\", 1)[1]\n        else:\n            base = full_key\n            full_suffix = \"\"\n\n        pretty_key = _pretty_vs_key(base)\n        final_key = pretty_key + full_suffix\n        pretty_map[final_key] = full_key\n        print(f\"[DEBUG] Mapped '{final_key}' \u2192 '{full_key}'\")  if debug else None\n\n    full_user_key = user_key + suffix\n    print(f\"[DEBUG] Full user key for lookup: '{full_user_key}'\")  if debug else None\n\n    if full_user_key in stats_dict:\n        print(\"[DEBUG] Found direct match in stats.\") if debug else None\n        return full_user_key\n    elif full_user_key in pretty_map:\n        print(f\"[DEBUG] Found in pretty map: {pretty_map[full_user_key]}\")  if debug else None\n        return pretty_map[full_user_key]\n    else:\n        pretty_keys = \"\\n\".join(f\"  - {k}\" for k in pretty_map.keys()) if pretty_map else \"  (none found)\"\n        raise ValueError(\n            f\"'{full_user_key}' not found in stats.\\n\"\n            f\"Available DE keys:\\n{pretty_keys}\"\n        )\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.filtering","title":"src.scpviz.pAnnData.filtering","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.filtering._detect_ambiguous_input","title":"_detect_ambiguous_input","text":"<pre><code>_detect_ambiguous_input(group, var, group_metrics=None)\n</code></pre> <p>Detects ambiguous user input mixing file and group identifiers.</p> <p>This helper checks whether the <code>group</code> list includes both file-like identifiers (present in <code>.var</code> as 'Found In: ') and group-like identifiers (present in <code>.uns</code> as ('group', 'count') tuples). <p>Parameters:</p> Name Type Description Default <code>group</code> <code>list of str</code> <p>User-provided identifiers for filtering.</p> required <code>var</code> <code>DataFrame</code> <p>The <code>.var</code> table of the corresponding AnnData object.</p> required <code>group_metrics</code> <code>DataFrame</code> <p>MultiIndex DataFrame from <code>.uns</code> containing per-group ('count', 'ratio') metrics.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(bool, list, list)</code> <p>(is_ambiguous, annotated_files, annotated_groups) - is_ambiguous (bool): True if both file-like and group-like entries coexist. - annotated_files (list): Entries that match file-level columns. - annotated_groups (list): Entries that match group-level metrics.</p> Source code in <code>src/scpviz/pAnnData/filtering.py</code> <pre><code>def _detect_ambiguous_input(group, var, group_metrics=None):\n    \"\"\"\n    Detects ambiguous user input mixing file and group identifiers.\n\n    This helper checks whether the `group` list includes both\n    file-like identifiers (present in `.var` as 'Found In: &lt;file&gt;')\n    and group-like identifiers (present in `.uns` as ('group', 'count') tuples).\n\n    Args:\n        group (list of str): User-provided identifiers for filtering.\n        var (pd.DataFrame): The `.var` table of the corresponding AnnData object.\n        group_metrics (pd.DataFrame, optional): MultiIndex DataFrame from `.uns`\n            containing per-group ('count', 'ratio') metrics.\n\n    Returns:\n        tuple(bool, list, list):\n            (is_ambiguous, annotated_files, annotated_groups)\n            - is_ambiguous (bool): True if both file-like and group-like entries coexist.\n            - annotated_files (list): Entries that match file-level columns.\n            - annotated_groups (list): Entries that match group-level metrics.\n    \"\"\"\n    annotated_files = [g for g in group if f\"Found In: {g}\" in var.columns]\n    annotated_groups = []\n    if group_metrics is not None:\n        annotated_groups = [\n            g for g in group\n            if (g, \"count\") in group_metrics.columns or (g, \"ratio\") in group_metrics.columns\n        ]\n\n    has_file_like = bool(annotated_files)\n    has_group_like = bool(annotated_groups)\n\n    # Ambiguous only if both exist and sets are distinct\n    is_ambiguous = has_file_like and has_group_like and set(annotated_files) != set(annotated_groups)\n\n    return is_ambiguous, annotated_files, annotated_groups\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.history","title":"src.scpviz.pAnnData.history","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.identifier","title":"src.scpviz.pAnnData.identifier","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.io","title":"src.scpviz.pAnnData.io","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.io._import_proteomeDiscoverer","title":"_import_proteomeDiscoverer","text":"<pre><code>_import_proteomeDiscoverer(prot_file: Optional[str] = None, pep_file: Optional[str] = None, obs_columns: Optional[List[str]] = ['sample'], **kwargs)\n</code></pre> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def _import_proteomeDiscoverer(prot_file: Optional[str] = None, pep_file: Optional[str] = None, obs_columns: Optional[List[str]] = ['sample'], **kwargs):\n    if not prot_file and not pep_file:\n        raise ValueError(f\"{format_log_prefix('error')} At least one of prot_file or pep_file must be provided to function. Try prot_file='proteome_discoverer_prot.txt' or pep_file='proteome_discoverer_pep.txt'.\")\n    print(\"--------------------------\\nStarting import [Proteome Discoverer]\\n\")\n\n    if prot_file:\n        # -----------------------------\n        print(f\"Source file: {prot_file} / {pep_file}\")\n        # PROTEIN DATA\n        # check file format, if '.txt' then use read_csv, if '.xlsx' then use read_excel\n        if prot_file.endswith('.txt') or prot_file.endswith('.tsv'):\n            prot_all = pd.read_csv(prot_file, sep='\\t')\n        elif prot_file.endswith('.xlsx'):\n            print(\"\ud83d\udca1 Tip: The read_excel function is slower compared to reading .tsv or .txt files. For improved performance, consider converting your data to .tsv or .txt format.\")\n            prot_all = pd.read_excel(prot_file)\n        # prot_X: sparse data matrix\n        prot_X = sparse.csr_matrix(prot_all.filter(regex='Abundance: F', axis=1).values).transpose()\n        # prot_layers['mbr']: protein MBR identification\n        prot_layers_mbr = prot_all.filter(regex='Found in Sample', axis=1).values.transpose()\n        # prot_var_names: protein names\n        prot_var_names = prot_all['Accession'].values\n        # prot_var: protein metadata\n        used_patterns = [\n            r'^Abundance: F',           # sample abundance columns (if any)\n            r'^Found In',               # found-in sample flags\n            r'^Significant In',         # significance flags\n            r'^Ratio:',                 # ratio columns (if PD quant ratios exist)\n            r'^Abundances \\(Grouped\\)', # grouped abundance or CV fields\n        ]\n\n        exclude_cols = prot_all.filter(regex='|'.join(used_patterns), axis=1).columns\n        prot_var = prot_all.drop(columns=exclude_cols, errors='ignore').copy()        \n\n        if 'Exp. q-value: Combined' in prot_all.columns:\n            prot_var['Exp. q-value: Combined'] = prot_all['Exp. q-value: Combined']\n        elif 'Exp. Protein Group q-value: Combined' in prot_all.columns:\n            prot_var['Exp. q-value: Combined'] = prot_all['Exp. Protein Group q-value: Combined']\n        else:\n            warnings.warn(\"\u26a0\ufe0f Neither 'Exp. q-value: Combined' nor 'Exp. Protein Group q-value: Combined' found in input file.\")\n\n        prot_var.rename(columns={\n            'Gene Symbol': 'Genes',\n            'Exp. q-value: Combined': 'Global_Q_value',\n            '# Unique Peptides': 'unique_peptides'\n        }, inplace=True)\n        # prot_obs_names: file names\n        prot_obs_names = prot_all.filter(regex='Abundance: F', axis=1).columns.str.extract(r'Abundance: (F\\d+):')[0].values\n        # prot_obs: sample typing from the column name, drop column if all 'n/a'\n        prot_obs = prot_all.filter(regex='Abundance: F', axis=1).columns.str.extract(r'Abundance: F\\d+: (.+)$')[0].values\n        prot_obs = pd.DataFrame(prot_obs, columns=['metadata'])['metadata'] \\\n            .str.split(',', expand=True)\n        prot_obs = _safe_strip(prot_obs).astype('category')\n\n        if (prot_obs == \"n/a\").all().any():\n            print(f\"{format_log_prefix('warn')} Found columns with all 'n/a'. Dropping these columns.\")\n            prot_obs = prot_obs.loc[:, ~(prot_obs == \"n/a\").all()]\n\n        print(f\"Number of files: {len(prot_obs_names)}\")\n        print(f\"Proteins: {len(prot_var)}\")\n    else:\n        prot_X = prot_layers_mbr = prot_var_names = prot_var = prot_obs_names = prot_obs = None\n\n    if pep_file:\n        # -----------------------------\n        # PEPTIDE DATA\n        if pep_file.endswith('.txt') or pep_file.endswith('.tsv'):\n            pep_all = pd.read_csv(pep_file, sep='\\t')\n        elif pep_file.endswith('.xlsx'):\n            print(f\"{format_log_prefix('warn')} The read_excel function is slower compared to reading .tsv or .txt files. For improved performance, consider converting your data to .tsv or .txt format.\")\n            pep_all = pd.read_excel(pep_file)\n        # pep_X: sparse data matrix\n        pep_X = sparse.csr_matrix(pep_all.filter(regex='Abundance: F', axis=1).values).transpose()\n        # pep_layers['mbr']: peptide MBR identification\n        pep_layers_mbr = pep_all.filter(regex='Found in Sample', axis=1).values.transpose()\n        # pep_var_names: peptide sequence with modifications\n        if 'Modifications' in pep_all.columns: # old PD version\n            mod_col = 'Modifications'\n        elif 'Modifications in Master Proteins' in pep_all.columns: # new PD version\n            mod_col = 'Modifications in Master Proteins'\n        else:\n            mod_col = None # handle fallback if user didn't import?\n\n        if mod_col is not None:\n            pep_all[mod_col] = pep_all[mod_col].astype(str)\n            pep_var_names = (\n                pep_all['Annotated Sequence'] +\n                np.where(pep_all[mod_col].isin(['nan', 'None', 'NaN']), '', ' MOD:' + pep_all[mod_col])\n            ).values\n        else:\n            pep_var_names = pep_all['Annotated Sequence'].values\n        # pep_obs_names: file names\n        pep_obs_names = pep_all.filter(regex='Abundance: F', axis=1).columns.str.extract(r'Abundance: (F\\d+):')[0].values\n        # pep_var: peptide metadata\n        pep_all.rename(columns={\n            'Qvality q-value': 'Global_Q_Value',\n            'q-Value': 'Global_Q_Value',\n            'q-Value (Best File Local)': 'Global_Q_Value',\n            'Qvality PEP': 'PEP',\n            'PEP (Best File Local)': 'PEP',\n        }, inplace=True)\n\n        used_patterns = ['^Abundance: F', '^Found in Sample', '^Abundances \\\\(Grouped', '^Abundances \\\\(Grouped\\\\) CV']\n        exclude_cols = pep_all.filter(regex='|'.join(used_patterns), axis=1).columns\n        pep_var = pep_all.drop(columns=exclude_cols, errors='ignore')\n\n        # ensure pep_var has only 1D columns\n        for col in pep_var.columns:\n            if isinstance(pep_var[col].iloc[0], (pd.Series, pd.DataFrame, np.ndarray, list, tuple)):\n                print(f\"{format_log_prefix('warn')} Dropping nested column '{col}' from peptide metadata.\")\n                pep_var = pep_var.drop(columns=[col])\n\n        pep_var = pep_var.copy()\n        for c in pep_var.columns:\n            if not np.issubdtype(pep_var[c].dtype, np.number):\n                pep_var[c] = pep_var[c].astype(str)\n\n        # prot_obs: sample typing from the column name, drop column if all 'n/a'\n        pep_obs = pep_all.filter(regex='Abundance: F', axis=1).columns.str.extract(r'Abundance: F\\d+: (.+)$')[0].values\n        pep_obs = pd.DataFrame(pep_obs, columns=['metadata'])['metadata'] \\\n            .str.split(',', expand=True)\n        pep_obs = _safe_strip(pep_obs).astype('category')\n\n        if (pep_obs == \"n/a\").all().any():\n            print(f\"{format_log_prefix('warn')} Found columns with all 'n/a'. Dropping these columns.\")\n            pep_obs = pep_obs.loc[:, ~(pep_obs == \"n/a\").all()]\n\n        print(f\"Peptides: {len(pep_var)}\")\n        if mod_col == 'Modifications in Master Proteins':\n            print(f\"\\n{format_log_prefix('info_only')} Using 'Modifications in Master Proteins' for modification annotation.\")\n        elif mod_col == 'Modifications':\n            print(f\"\\n{format_log_prefix('info_only')} Using 'Modifications' for modification annotation.\")\n        elif mod_col is None:\n            print(f\"\\n{format_log_prefix('warn')} No modification column found. Peptide modifications were not annotated. Please check if 'Modifications' or 'Modifications in Master Protein' columns were exported from PD.\")\n\n    else:\n        pep_X = pep_layers_mbr = pep_var_names = pep_var = pep_obs_names = pep_obs = None\n\n    if prot_file and pep_file:\n        # -----------------------------\n        # RS DATA\n        # rs is in the form of a binary matrix, protein x peptide\n        pep_prot_list = pep_all['Master Protein Accessions'].str.split('; ')\n        rs, mlb = _build_rs_matrix(pep_prot_list, prot_var_names = prot_var_names)\n\n    else:\n        rs = None\n\n    # ASSERTIONS\n    # -----------------------------\n    # check if mlb.classes_ has overlap with prot_var\n    if prot_file and pep_file:\n        mlb_classes_set = set(mlb.classes_)\n        prot_var_set = set(prot_var_names)\n\n        if mlb_classes_set != prot_var_set:\n            print(\n                f\"{format_log_prefix('warn')} Master proteins in the peptide matrix do not match proteins in the protein data, please check if files correspond to the same data.\\n\"\n                f\"{format_log_prefix('info')} If using PD3.2, this is a known issue due to changed protein grouping rules.\"\n            )\n\n    pdata = _create_pAnnData_from_parts(\n        prot_X, pep_X, rs,\n        prot_obs, prot_var, prot_obs_names, prot_var_names,\n        pep_obs, pep_var, pep_obs_names, pep_var_names,\n        obs_columns=obs_columns,\n        X_mbr_prot=prot_layers_mbr,\n        X_mbr_pep=pep_layers_mbr,\n        metadata={\n            \"source\": \"proteomeDiscoverer\",\n            \"prot_file\": prot_file,\n            \"pep_file\": pep_file\n        },\n        history_msg=f\"Imported Proteome Discoverer data using source file(s): {prot_file}, {pep_file}.\"\n    )\n\n    return pdata\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.io._import_diann","title":"_import_diann","text":"<pre><code>_import_diann(report_file: Optional[str] = None, obs_columns: Optional[List[str]] = None, delimiter: Optional[str] = '_', obs: Optional[DataFrame] = None, prot_value='PG.MaxLFQ', pep_value='Precursor.Normalised', prot_var_columns=['Genes', 'Master.Protein'], pep_var_columns=['Genes', 'Protein.Group', 'Precursor.Charge', 'Modified.Sequence', 'Stripped.Sequence', 'Precursor.Id', 'All Mapped Proteins', 'All Mapped Genes'], **kwargs)\n</code></pre> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def _import_diann(report_file: Optional[str] = None, obs_columns: Optional[List[str]] = None, delimiter: Optional[str] = '_', obs: Optional[pd.DataFrame] = None, prot_value = 'PG.MaxLFQ', pep_value = 'Precursor.Normalised', prot_var_columns = ['Genes', 'Master.Protein'], pep_var_columns = ['Genes', 'Protein.Group', 'Precursor.Charge','Modified.Sequence', 'Stripped.Sequence', 'Precursor.Id', 'All Mapped Proteins', 'All Mapped Genes'], **kwargs):\n    if not report_file:\n        raise ValueError(f\"{format_log_prefix('error')} Importing from DIA-NN: report.tsv or report.parquet must be provided to function. Try report_file='report.tsv' or report_file='report.parquet'\")\n    print(\"--------------------------\\nStarting import [DIA-NN]\\n\")\n\n    print(f\"Source file: {report_file}\")\n    # if csv, then use pd.read_csv, if parquet then use pd.read_parquet('example_pa.parquet', engine='pyarrow')\n    if report_file.endswith('.tsv'):\n        report_all = pd.read_csv(report_file, sep='\\t')\n    elif report_file.endswith('.parquet'):\n        report_all = pd.read_parquet(report_file, engine='pyarrow')\n    report_all['Master.Protein'] = report_all['Protein.Group'].str.split(';')\n    report_all = report_all.explode('Master.Protein')\n    # -----------------------------\n    # PROTEIN DATA\n    # prot_X: sparse data matrix\n    if prot_value != 'PG.MaxLFQ':\n        if report_file.endswith('.tsv') and prot_value == 'PG.Quantity':\n            # check if 'PG.Quantity' is in the columns, if yes then pass, if not then throw an error that DIA-NN version &gt;2.0 does not have PG.quantity\n            if 'PG.Quantity' not in report_all.columns:\n                raise ValueError(\"Reports generated with DIA-NN version &gt;2.0 do not contain PG.Quantity values, please use PG.MaxLFQ .\")\n        else:\n            print(f\"{format_log_prefix('info')} Protein value specified is not PG.MaxLFQ nor PG.Quantity, please check if correct.\")\n    prot_X_pivot = report_all.pivot_table(index='Master.Protein', columns='Run', values=prot_value, aggfunc='first', sort=False)\n    prot_X = sparse.csr_matrix(prot_X_pivot.values).T\n    # prot_var_names: protein names\n    prot_var_names = prot_X_pivot.index.values\n    # prot_obs: file names\n    prot_obs_names = prot_X_pivot.columns.values\n\n    # prot_var: protein metadata (default: Genes, Master.Protein)\n    if 'First.Protein.Description' in report_all.columns and 'First.Protein.Description' not in prot_var_columns:\n        prot_var_columns.insert(0, 'First.Protein.Description')\n\n    if 'Global.PG.Q.Value' in report_all.columns and 'Global.PG.Q.Value' not in prot_var_columns:\n        prot_var_columns.append('Global.PG.Q.Value')\n\n    existing_prot_var_columns = [col for col in prot_var_columns if col in report_all.columns]\n    missing_columns = set(prot_var_columns) - set(existing_prot_var_columns)\n\n    if missing_columns:\n        warnings.warn(\n            f\"{format_log_prefix('warn')} The following columns are missing: {', '.join(missing_columns)}. \"\n        )\n\n    prot_var = report_all.loc[:, existing_prot_var_columns].drop_duplicates(subset='Master.Protein').drop(columns='Master.Protein').rename(columns={'Global.PG.Q.Value': 'Global_Q_value'})\n    # prot_obs: sample typing from the column name\n    if obs is not None:\n        prot_obs = obs\n        # obs_columns = obs_columns\n    else:\n        prot_obs = pd.DataFrame(prot_X_pivot.columns.values, columns=['Run'])['Run'].str.split(delimiter, expand=True).rename(columns=dict(enumerate(obs_columns)))\n\n    # PG.Q.Value layer (sample x protein)\n    if 'PG.Q.Value' in report_all.columns:\n        pgq_pivot = report_all.pivot_table(index='Master.Protein', columns='Run', values='PG.Q.Value', aggfunc='first', sort=False)\n        prot_pgq_layer = sparse.csr_matrix(pgq_pivot.values).T\n    else:\n        prot_pgq_layer = None\n\n    print(f\"Number of files: {len(prot_obs_names)}\")\n    print(f\"Proteins: {len(prot_var)}\")\n\n    # -----------------------------\n    # PEPTIDE DATA\n    # pep_X: sparse data matrix\n    pep_X_pivot = report_all.pivot_table(index='Precursor.Id', columns='Run', values=pep_value, aggfunc='first', sort=False)\n    pep_X = sparse.csr_matrix(pep_X_pivot.values).T\n    # pep_var_names: peptide sequence\n    pep_var_names = pep_X_pivot.index.values\n    # pep_obs_names: file names\n    pep_obs_names = pep_X_pivot.columns.values\n    # pep_var: peptide sequence with modifications (default: Genes, Protein.Group, Precursor.Charge, Modified.Sequence, Stripped.Sequence, Precursor.Id, All Mapped Proteins, All Mapped Genes)\n    existing_pep_var_columns = [col for col in pep_var_columns if col in report_all.columns]\n    missing_columns = set(pep_var_columns) - set(existing_pep_var_columns)\n    # if missing columns are ['All Mapped Proteins'] and ['All Mapped Genes'], then it is likely that the DIA-NN version is &lt;1.8.1, so we can skip the warning\n    if missing_columns == {'All Mapped Proteins', 'All Mapped Genes'}:\n        missing_columns = set()\n\n    # Precursor.Quantity layer (if using directLFQ for nomalization)\n    if 'Precursor.Quantity' in report_all.columns:\n        precursor_q_pivot = report_all.pivot_table(index='Precursor.Id', columns='Run', values='Precursor.Quantity', aggfunc='first', sort=False)\n        precursor_q_layer = sparse.csr_matrix(precursor_q_pivot.values).T\n    else:\n        precursor_q_layer = None\n\n\n    pep_var = report_all.loc[:, existing_pep_var_columns].drop_duplicates(subset='Precursor.Id').drop(columns='Precursor.Id')\n    # pep_obs: sample typing from the column name, same as prot_obs\n    pep_obs = prot_obs\n\n    # Q.Value layer (sample x peptide)\n    if 'Q.Value' in report_all.columns:\n        pepq_pivot = report_all.pivot_table(index='Precursor.Id', columns='Run', values='Q.Value', aggfunc='first', sort=False)\n        pep_q_layer = sparse.csr_matrix(pepq_pivot.values).T\n    else:\n        pep_q_layer = None\n\n    print(f\"Peptides: {len(pep_var)}\")\n    if missing_columns:\n        print(\n            f\"{format_log_prefix('warn')} The following columns are missing: {', '.join(missing_columns)}. \"\n            \"Consider running analysis in the newer version of DIA-NN (1.8.1). \"\n            \"Peptide-protein mapping may differ.\"\n        )\n\n    # -----------------------------\n    # RS DATA\n    # rs: protein x peptide relational data\n    pep_prot_list = report_all.drop_duplicates(subset=['Precursor.Id'])['Protein.Group'].str.split(';')\n    rs, mlb = _build_rs_matrix(pep_prot_list, prot_var_names = prot_var_names)\n\n    # -----------------------------\n    # ASSERTIONS\n    # -----------------------------\n    # check if mlb.classes_ has overlap with prot_var\n    mlb_classes_set = set(mlb.classes_)\n    prot_var_set = set(prot_var_names)\n\n    if mlb_classes_set != prot_var_set:\n        print(f\"{format_log_prefix('warn')} Master proteins in the peptide matrix do not match proteins in the protein data, please check if files correspond to the same data.\")\n        print(f\"Overlap: {len(mlb_classes_set &amp; prot_var_set)}\")\n        print(f\"Unique to peptide data: {mlb_classes_set - prot_var_set}\")\n        print(f\"Unique to protein data: {prot_var_set - mlb_classes_set}\")\n\n    pdata = _create_pAnnData_from_parts(\n        prot_X, pep_X, rs,\n        prot_obs, prot_var, prot_obs_names, prot_var_names,\n        pep_obs, pep_var, pep_obs_names, pep_var_names,\n        obs_columns=obs_columns,\n        X_qval_prot=prot_pgq_layer,\n        X_qval_pep=pep_q_layer,\n        X_precursor_pep=precursor_q_layer,\n        metadata={\n            \"source\": \"diann\",\n            \"file\": report_file,\n            \"protein_metric\": prot_value,\n            \"peptide_metric\": pep_value\n        },\n        history_msg=f\"Imported DIA-NN report from {report_file} using {prot_value} (protein) and {pep_value} (peptide).\"\n    )\n\n    return pdata\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.io._create_pAnnData_from_parts","title":"_create_pAnnData_from_parts","text":"<pre><code>_create_pAnnData_from_parts(prot_X, pep_X, rs, prot_obs, prot_var, prot_obs_names, prot_var_names, pep_obs=None, pep_var=None, pep_obs_names=None, pep_var_names=None, obs_columns=None, X_mbr_prot=None, X_mbr_pep=None, X_qval_prot=None, X_qval_pep=None, X_precursor_pep=None, found_threshold=0, fdr_threshold=0.01, metadata=None, history_msg='')\n</code></pre> <p>Assemble a <code>pAnnData</code> object from processed matrices and metadata.</p> <p>This function is typically called internally by import functions. It constructs <code>.prot</code> and <code>.pep</code> AnnData objects, assigns optional metadata and MBR layers, adds identifier mappings and sample-level summary metrics, and returns a validated <code>pAnnData</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>prot_X</code> <code>csr_matrix</code> <p>Protein-level expression matrix (samples \u00d7 proteins).</p> required <code>pep_X</code> <code>csr_matrix or None</code> <p>Peptide-level expression matrix (samples \u00d7 peptides).</p> required <code>rs</code> <code>csr_matrix or None</code> <p>Binary matrix linking proteins (rows) to peptides (columns).</p> required <code>prot_obs</code> <code>DataFrame</code> <p>Sample-level metadata for protein data.</p> required <code>prot_var</code> <code>DataFrame</code> <p>Feature-level metadata for proteins.</p> required <code>prot_obs_names</code> <code>list - like</code> <p>Sample identifiers for <code>.prot</code>.</p> required <code>prot_var_names</code> <code>list - like</code> <p>Protein accession identifiers for <code>.prot</code>.</p> required <code>pep_obs</code> <code>DataFrame</code> <p>Sample metadata for <code>.pep</code>. If not provided, <code>.prot.obs</code> is reused.</p> <code>None</code> <code>pep_var</code> <code>DataFrame</code> <p>Feature metadata for peptides.</p> <code>None</code> <code>pep_obs_names</code> <code>list - like</code> <p>Sample identifiers for <code>.pep</code>.</p> <code>None</code> <code>pep_var_names</code> <code>list - like</code> <p>Peptide identifiers.</p> <code>None</code> <code>obs_columns</code> <code>list of str</code> <p>Columns from filenames to include in <code>.summary</code> and <code>.obs</code>.</p> <code>None</code> <code>X_mbr_prot</code> <code>ndarray or DataFrame</code> <p>Optional protein-level MBR identification info.</p> <code>None</code> <code>X_mbr_pep</code> <code>ndarray or DataFrame</code> <p>Optional peptide-level MBR identification info.</p> <code>None</code> <code>X_qval_prot</code> <code>ndarray or DataFrame</code> <p>Optional protein-level Q-value info.</p> <code>None</code> <code>X_qval_pep</code> <code>ndarray or DataFrame</code> <p>Optional peptide-level Q-value info.</p> <code>None</code> <code>X_precursor_pep</code> <code>ndarray or DataFrame</code> <p>Optional peptide-level precursor quantity info. (for directLFQ normalization)</p> <code>None</code> <code>metadata</code> <code>dict</code> <p>Optional dictionary of import metadata (e.g. <code>{'source': 'diann'}</code>).</p> <code>None</code> <code>history_msg</code> <code>str</code> <p>Operation description to append to the history log.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>Initialized object with <code>.prot</code>, <code>.pep</code>, <code>.summary</code>, <code>.rs</code>, and other metadata filled in.</p> Note <p>This is a low-level function. In most cases, users should call <code>import_data()</code>, <code>import_proteomeDiscoverer()</code>, or <code>import_diann()</code> instead.</p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def _create_pAnnData_from_parts(\n    prot_X, pep_X, rs,\n    prot_obs, prot_var, prot_obs_names, prot_var_names,\n    pep_obs=None, pep_var=None, pep_obs_names=None, pep_var_names=None,\n    obs_columns=None,\n    X_mbr_prot=None,\n    X_mbr_pep=None,\n    X_qval_prot=None,\n    X_qval_pep=None,\n    X_precursor_pep=None,\n    found_threshold=0,\n    fdr_threshold=0.01,\n    metadata=None,\n    history_msg=\"\"\n):\n    \"\"\"\n    Assemble a `pAnnData` object from processed matrices and metadata.\n\n    This function is typically called internally by import functions. It constructs\n    `.prot` and `.pep` AnnData objects, assigns optional metadata and MBR layers,\n    adds identifier mappings and sample-level summary metrics, and returns a\n    validated `pAnnData` object.\n\n    Args:\n        prot_X (csr_matrix): Protein-level expression matrix (samples \u00d7 proteins).\n        pep_X (csr_matrix or None): Peptide-level expression matrix (samples \u00d7 peptides).\n        rs (csr_matrix or None): Binary matrix linking proteins (rows) to peptides (columns).\n        prot_obs (pd.DataFrame): Sample-level metadata for protein data.\n        prot_var (pd.DataFrame): Feature-level metadata for proteins.\n        prot_obs_names (list-like): Sample identifiers for `.prot`.\n        prot_var_names (list-like): Protein accession identifiers for `.prot`.\n        pep_obs (pd.DataFrame, optional): Sample metadata for `.pep`. If not provided, `.prot.obs` is reused.\n        pep_var (pd.DataFrame, optional): Feature metadata for peptides.\n        pep_obs_names (list-like, optional): Sample identifiers for `.pep`.\n        pep_var_names (list-like, optional): Peptide identifiers.\n        obs_columns (list of str, optional): Columns from filenames to include in `.summary` and `.obs`.\n        X_mbr_prot (np.ndarray or DataFrame, optional): Optional protein-level MBR identification info.\n        X_mbr_pep (np.ndarray or DataFrame, optional): Optional peptide-level MBR identification info.\n        X_qval_prot (np.ndarray or DataFrame, optional): Optional protein-level Q-value info.\n        X_qval_pep (np.ndarray or DataFrame, optional): Optional peptide-level Q-value info.\n        X_precursor_pep (np.ndarray or DataFrame, optional): Optional peptide-level precursor quantity info. (for directLFQ normalization)\n        metadata (dict, optional): Optional dictionary of import metadata (e.g. `{'source': 'diann'}`).\n        history_msg (str): Operation description to append to the history log.\n\n    Returns:\n        pAnnData: Initialized object with `.prot`, `.pep`, `.summary`, `.rs`, and other metadata filled in.\n\n    Note:\n        This is a low-level function. In most cases, users should call `import_data()`, `import_proteomeDiscoverer()`, or `import_diann()` instead.\n    \"\"\"\n    from .pAnnData import pAnnData\n\n    print(\"\")\n    pdata = pAnnData(prot_X, pep_X, rs)\n\n    # --- PROTEIN ---\n    if prot_X is not None:\n        prot_var.index = prot_var.index.astype(str)\n\n        pdata.prot.obs = pd.DataFrame(prot_obs) # type: ignore[attr-defined]\n        pdata.prot.var = pd.DataFrame(prot_var) # type: ignore[attr-defined]\n        pdata.prot.obs_names = list(prot_obs_names) # type: ignore[attr-defined]\n        pdata.prot.var_names = list(prot_var_names) # type: ignore[attr-defined]\n        pdata.prot.obs.columns = obs_columns if obs_columns else list(range(pdata.prot.obs.shape[1])) # type: ignore[attr-defined]\n        pdata.prot.layers['X_raw'] = prot_X # type: ignore[attr-defined]\n        pdata.prot.uns['X_raw_obs_names'] = list(prot_obs_names) # type: ignore[attr-defined]\n        pdata.prot.uns['X_raw_var_names'] = list(prot_var_names)\n        if X_mbr_prot is not None:\n            pdata.prot.layers['X_mbr'] = X_mbr_prot # type: ignore[attr-defined]\n        if X_qval_prot is not None:\n            pdata.prot.layers['X_qval'] = X_qval_prot # type: ignore[attr-defined]\n\n    if \"Genes\" in pdata.prot.var.columns and pdata.prot.var[\"Genes\"].isna().any(): # type: ignore[attr-defined]\n        pdata.update_missing_genes(gene_col=\"Genes\", verbose=True)\n\n    # --- PEPTIDE ---\n    if pep_X is not None:\n        pep_var.index = pep_var.index.astype(str)\n\n        pdata.pep.obs = pd.DataFrame(pep_obs) # type: ignore[attr-defined]\n        pdata.pep.var = pd.DataFrame(pep_var) # type: ignore[attr-defined]\n        pdata.pep.obs_names = list(pep_obs_names) # type: ignore[attr-defined]\n        pdata.pep.var_names = list(pep_var_names) # type: ignore[attr-defined]\n        pdata.pep.obs.columns = obs_columns if obs_columns else list(range(pdata.pep.obs.shape[1])) # type: ignore[attr-defined]\n        pdata.pep.layers['X_raw'] = pep_X # type: ignore[attr-defined]\n        pdata.pep.uns['X_raw_obs_names'] = list(pep_obs_names) # type: ignore[attr-defined]\n        pdata.pep.uns['X_raw_var_names'] = list(pep_var_names)\n        if X_mbr_pep is not None:\n            pdata.pep.layers['X_mbr'] = X_mbr_pep # type: ignore[attr-defined]\n        if X_qval_pep is not None:\n            pdata.pep.layers['X_qval'] = X_qval_pep # type: ignore[attr-defined]\n        if X_precursor_pep is not None:\n            pdata.pep.layers['X_precursor'] = X_precursor_pep # type: ignore[attr-defined]\n\n    # --- Metadata ---\n    metadata = metadata or {}\n    metadata.setdefault(\"imported_at\", datetime.datetime.now().isoformat())\n\n    if pdata.prot is not None:\n        pdata.prot.uns['metadata'] = metadata\n    if pdata.pep is not None:\n        pdata.pep.uns['metadata'] = metadata\n\n    # --- Summary + Validation ---\n    pdata.update_summary(recompute=True, verbose=False)\n    pdata._cleanup_proteins_after_sample_filter(printout=True)\n    pdata._annotate_found_samples(threshold=found_threshold)\n    pdata._annotate_significant_samples(fdr_threshold=fdr_threshold)\n\n    print(\"\")\n    if not pdata.validate():\n        print(f\"{format_log_prefix('warn')} Validation issues found. Use `pdata.validate()` to inspect.\")\n\n    if history_msg:\n        pdata._append_history(history_msg)\n\n    print(f\"{format_log_prefix('result')} Import complete. Use `print(pdata)` to view the object.\")\n    print(\"--------------------------\")\n\n    return pdata\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.io._build_rs_matrix","title":"_build_rs_matrix","text":"<pre><code>_build_rs_matrix(pep_prot_list, prot_var_names=None)\n</code></pre> <p>Build a sparse boolean RS (protein \u00d7 peptide) relational matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pep_prot_list</code> <code>list or Series</code> <p>List/Series where each entry contains one or more protein accessions (as lists or split strings).</p> required <code>prot_var_names</code> <code>list</code> <p>Ordered list of protein accessions to align RS columns. If None, uses the order returned by MultiLabelBinarizer.</p> <code>None</code> <p>Returns:</p> Type Description <p>scipy.sparse.csr_matrix: Sparse boolean RS matrix (peptides \u00d7 proteins).</p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def _build_rs_matrix(pep_prot_list, prot_var_names=None):\n    \"\"\"\n    Build a sparse boolean RS (protein \u00d7 peptide) relational matrix.\n\n    Args:\n        pep_prot_list (list or pd.Series): List/Series where each entry\n            contains one or more protein accessions (as lists or split strings).\n        prot_var_names (list, optional): Ordered list of protein accessions to\n            align RS columns. If None, uses the order returned by MultiLabelBinarizer.\n\n    Returns:\n        scipy.sparse.csr_matrix: Sparse boolean RS matrix (peptides \u00d7 proteins).\n    \"\"\"\n    # sparse bool RS matrix to save RAM\n    mlb = MultiLabelBinarizer(sparse_output=True)\n    rs = mlb.fit_transform(pep_prot_list).astype(bool)\n\n    # reorder columns to match protein order\n    if prot_var_names is not None:\n        index_dict = {protein: idx for idx, protein in enumerate(mlb.classes_)}\n        reorder_indices = [\n            index_dict[p] for p in prot_var_names if p in index_dict\n        ]\n        rs = rs[:, reorder_indices]\n\n    # make csr matrix so downstream operations are faster\n    rs = sparse.csr_matrix(rs, dtype=bool)\n    rs.eliminate_zeros()\n\n    return rs, mlb\n</code></pre>"},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.metrics","title":"src.scpviz.pAnnData.metrics","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.summary","title":"src.scpviz.pAnnData.summary","text":""},{"location":"reference/pAnnData/hidden_functions/#src.scpviz.pAnnData.validation","title":"src.scpviz.pAnnData.validation","text":""},{"location":"reference/pAnnData/identifier_mixins/","title":"Identifier","text":"<p>Mixin for mapping gene, protein and peptide accessions.</p>"},{"location":"reference/pAnnData/identifier_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin","title":"IdentifierMixin","text":"<p>Handles mapping between genes, accessions, and peptides.</p> <p>This mixin provides utilities for:</p> <ul> <li> <p>Building and caching bidirectional mappings:</p> <ul> <li>For proteins: gene \u2194 accession  </li> <li>For peptides: peptide \u2194 protein accession</li> </ul> </li> <li> <p>Updating or refreshing identifier maps manually or via UniProt</p> </li> <li>Automatically filling in missing gene names using the UniProt API</li> </ul> <p>These mappings are cached and used throughout the <code>pAnnData</code> object to support resolution of user queries and consistent gene-accession-peptide tracking.</p> <p>Methods:</p> Name Description <code>_build_identifier_maps</code> <p>Create forward/reverse maps based on protein or peptide data</p> <code>refresh_identifier_maps</code> <p>Clear cached mappings to force rebuild</p> <code>get_identifier_maps</code> <p>Retrieve (gene \u2192 acc, acc \u2192 gene) or (peptide \u2194 protein) maps</p> <code>update_identifier_maps</code> <p>Add or overwrite mappings (e.g., manual corrections)</p> <code>update_missing_genes</code> <p>Fill missing gene names using the UniProt API</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>class IdentifierMixin:\n    \"\"\"\n    Handles mapping between genes, accessions, and peptides.\n\n    This mixin provides utilities for:\n\n    - Building and caching bidirectional mappings:\n        * For proteins: gene \u2194 accession  \n        * For peptides: peptide \u2194 protein accession\n\n    - Updating or refreshing identifier maps manually or via UniProt\n    - Automatically filling in missing gene names using the UniProt API\n\n    These mappings are cached and used throughout the `pAnnData` object to support resolution of user queries and consistent gene-accession-peptide tracking.\n\n    Functions:\n        _build_identifier_maps: Create forward/reverse maps based on protein or peptide data\n        refresh_identifier_maps: Clear cached mappings to force rebuild\n        get_identifier_maps: Retrieve (gene \u2192 acc, acc \u2192 gene) or (peptide \u2194 protein) maps\n        update_identifier_maps: Add or overwrite mappings (e.g., manual corrections)\n        update_missing_genes: Fill missing gene names using the UniProt API\n    \"\"\"\n\n    def _build_identifier_maps(self, adata, gene_col=\"Genes\"):\n        \"\"\"\n        Build bidirectional identifier mappings for genes/proteins or peptides/proteins.\n\n        Depending on whether `adata` is `.prot` or `.pep`, this builds:\n\n        - For proteins: gene \u2194 accession\n        - For peptides: peptide \u2194 protein accession\n\n        Args:\n            adata (AnnData): Either `self.prot` or `self.pep`.\n            gene_col (str): Column name in `.var` containing gene names (default: \"Genes\").\n\n        Returns:\n            tuple: A pair of dictionaries (`forward`, `reverse`) for identifier lookup.\n\n        Note:\n            For peptides, mapping relies on `utils.get_pep_prot_mapping()` to resolve protein accessions.\n\n        Raises:\n            Warning if peptide-to-protein mapping cannot be built.\n        \"\"\"\n        from pandas import notna\n\n        forward = {}\n        reverse = {}\n\n        if adata is self.prot:\n            if gene_col in adata.var.columns:\n                for acc, gene in zip(adata.var_names, adata.var[gene_col]):\n                    if notna(gene):\n                        gene = str(gene)\n                        forward[gene] = acc\n                        reverse[acc] = gene\n\n        elif adata is self.pep:\n            try:\n                prot_acc_col = utils.get_pep_prot_mapping(self)\n                pep_to_prot = adata.var[prot_acc_col]\n                for pep, prot in zip(adata.var_names, pep_to_prot):\n                    if notna(prot):\n                        forward[prot] = pep\n                        reverse[pep] = prot\n            except Exception as e:\n                warnings.warn(f\"Could not build peptide-to-protein map: {e}\")\n\n        return forward, reverse\n\n    def refresh_identifier_maps(self):\n        \"\"\"\n        Clear cached identifier maps to force regeneration on next access.\n\n        This removes the following attributes if present:\n\n        - `_gene_maps_protein`: Gene \u2194 Accession map for proteins\n        - `_protein_maps_peptide`: Protein \u2194 Peptide map for peptides\n\n        Useful when `.var` annotations are updated and identifier mappings may have changed.\n        \"\"\"\n        for attr in [\"_gene_maps_protein\", \"_protein_maps_peptide\"]:\n            if hasattr(self, attr):\n                delattr(self, attr)\n\n    def get_identifier_maps(self, on='protein'):\n        \"\"\"\n        Retrieve gene/accession or peptide/protein mapping dictionaries.\n\n        Depending on the `on` argument, returns a tuple of forward and reverse mappings:\n\n        - If `on='protein'`: (gene \u2192 accession, accession \u2192 gene)\n\n        - If `on='peptide'`: (protein accession \u2192 peptide, peptide \u2192 protein accession)\n\n        Note: Alias `get_gene_maps()` also calls this function for compatibility.\n\n        Args:\n            on (str): Source of mapping. Must be `'protein'` or `'peptide'`.\n\n        Returns:\n            Tuple[dict, dict]: (forward mapping, reverse mapping)\n\n        Raises:\n            ValueError: If `on` is not `'protein'` or `'peptide'`.\n        \"\"\"\n        if on in ('protein','prot'):\n            return self._cached_identifier_maps_protein\n        elif on in ('peptide','pep'):\n            return self._cached_identifier_maps_peptide\n        else:\n            raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n    # TODO: add peptide remapping to var, but need to also update rs if you do this.\n    def update_identifier_maps(self, mapping, on='protein', direction='forward', overwrite=False, verbose=True):\n        \"\"\"\n        Update cached identifier maps with user-supplied mappings.\n\n        This function updates the internal forward and reverse identifier maps\n        for either proteins or peptides. Ensures consistency by updating both\n        directions of the mapping.\n\n        - For `'protein'`:\n            * forward: gene \u2192 accession  \n            * reverse: accession \u2192 gene\n\n        - For `'peptide'`:\n            * forward: protein accession \u2192 peptide\n            * reverse: peptide \u2192 protein accession\n\n        Args:\n            mapping (dict): Dictionary of mappings to add.\n            on (str): Which maps to update. Must be `'protein'` or `'peptide'`.\n            direction (str): `'forward'` or `'reverse'` \u2014 determines how the `mapping` should be interpreted.\n            overwrite (bool): If True, allows overwriting existing entries.\n            verbose (bool): If True, prints a summary of updated keys.\n\n        Note:\n            The corresponding reverse map is automatically updated to maintain bidirectional consistency.\n\n        Example:\n            Add new gene-to-accession mappings (protein):\n                ```python\n                pdata.update_identifier_maps(\n                    {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n                    on='protein',\n                    direction='forward'\n                )\n                ```\n\n            Add peptide \u2192 protein mappings:\n                ```python\n                pdata.update_identifier_maps(\n                    {'PEPTIDE_ABC': 'P12345'},\n                    on='peptide',\n                    direction='reverse'\n                )\n                ```\n\n            Overwrite a protein \u2192 gene mapping:\n                ```python\n                pdata.update_identifier_maps(\n                    {'P12345': 'NEWGENE'},\n                    on='protein',\n                    direction='reverse',\n                    overwrite=True\n                )\n                ```\n\n        \"\"\"\n        if on == 'protein':\n            forward, reverse = self._cached_identifier_maps_protein\n        elif on == 'peptide':\n            forward, reverse = self._cached_identifier_maps_peptide\n        else:\n            raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n        source_map = forward if direction == 'forward' else reverse\n        target_map = reverse if direction == 'forward' else forward\n\n        added, updated, skipped = 0, 0, 0\n\n        for key, val in mapping.items():\n            if key in source_map:\n                if overwrite:\n                    source_map[key] = val\n                    target_map[val] = key\n                    updated += 1\n                else:\n                    skipped += 1\n            else:\n                source_map[key] = val\n                target_map[val] = key\n                added += 1\n\n        message = (\n            f\"[update_identifier_maps] Updated '{on}' ({direction}): \"\n            f\"{added} added, {updated} overwritten, {skipped} skipped.\"\n        )\n\n        if verbose:\n            print(message)\n        self._append_history(message)\n\n        # Update .prot.var[\"Genes\"] if updating protein identifier reverse map (accession \u2192 gene)\n        if on == 'protein' and direction == 'reverse':\n            updated_var_count = 0\n            updated_accessions = []\n\n            for acc, gene in mapping.items():\n                if acc in self.prot.var_names:\n                    self.prot.var.at[acc, \"Genes\"] = gene\n                    updated_accessions.append(acc)\n                    updated_var_count += 1\n\n            if updated_var_count &gt; 0:\n                var_message = (\n                    f\"\ud83d\udd01 Updated `.prot.var['Genes']` for {updated_var_count} entries from custom mapping. \"\n                    f\"(View details in `pdata.metadata['identifier_map_history']`)\"\n                )\n                if verbose:\n                    print(var_message)\n                self._append_history(var_message)\n\n        # Log detailed update history for all cases\n        import datetime\n\n        record = {\n            'on': on,\n            'direction': direction,\n            'input_mapping': dict(mapping),  # shallow copy\n            'overwrite': overwrite,\n            'timestamp': datetime.datetime.now().isoformat(timespec='seconds'),\n            'summary': {\n                'added': added,\n                'updated': updated,\n                'skipped': skipped,\n            }\n        }\n\n        if on == 'protein' and direction == 'reverse':\n            record['updated_var_column'] = {\n                'column': 'Genes',\n                'accessions': updated_accessions,\n                'n_updated': updated_var_count\n            }\n\n        self.metadata.setdefault(\"identifier_map_history\", []).append(record)\n\n    get_gene_maps = get_identifier_maps\n\n    def update_missing_genes(self, gene_col=\"Genes\", verbose=True):\n        \"\"\"\n        Fill missing gene names in `.prot.var` using UniProt API.\n\n        This function searches for missing values in the specified gene column\n        and attempts to fill them by querying the UniProt API using protein\n        accession IDs. If a gene name cannot be found, a placeholder\n        'UNKNOWN_&lt;accession&gt;' is used instead.\n\n        Args:\n            gene_col (str): Column name in `.prot.var` to update (default: \"Genes\").\n            verbose (bool): Whether to print summary information (default: True).\n\n        Returns:\n            None\n\n        Note:\n            - This function only operates on `.prot.var`, not `.pep.var`.\n            - If UniProt is unavailable or returns no match, the missing entry is filled as `'UNKNOWN_&lt;accession&gt;'`.\n            - To manually correct unknown entries later, use `update_identifier_maps()` with `direction='reverse'`.\n\n        Example:\n            Automatically fill missing gene names using UniProt:\n                ```python\n                pdata.update_missing_genes()\n                ```\n        \"\"\"\n        var = self.prot.var\n\n        if gene_col not in var.columns:\n            if verbose:\n                print(f\"{format_log_prefix('warn')} Column '{gene_col}' not found in .prot.var.\")\n            return\n\n        missing_mask = var[gene_col].isna()\n        if not missing_mask.any():\n            if verbose:\n                print(f\"{format_log_prefix('result')} No missing gene names found.\")\n            return\n\n        accessions = var.index[missing_mask].tolist()\n        if verbose:\n            print(f\"{format_log_prefix('info_only')} {len(accessions)} proteins with missing gene names.\")\n\n        try:\n            df = utils.get_uniprot_fields(\n                accessions,\n                search_fields=[\"accession\", \"gene_primary\"],\n                standardize=True\n            )\n        except Exception as e:\n            print(f\"{format_log_prefix('error')} UniProt query failed: {e}\")\n            return\n        df = utils.standardize_uniprot_columns(df)\n\n        if df.empty or \"accession\" not in df.columns or \"gene_primary\" not in df.columns:\n            print(f\"{format_log_prefix('warn')} UniProt returned no usable gene mapping columns.\")\n            return\n\n        gene_map = dict(zip(df[\"accession\"], df[\"gene_primary\"]))\n        filled = self.prot.var.loc[missing_mask].index.map(lambda acc: gene_map.get(acc))\n        final_genes = [\n            gene if pd.notna(gene) else f\"UNKNOWN_{acc}\"\n            for acc, gene in zip(self.prot.var.loc[missing_mask].index, filled)\n        ]\n        self.prot.var.loc[missing_mask, gene_col] = final_genes\n\n        found = sum(pd.notna(filled))\n        unknown = len(final_genes) - found\n        if verbose:\n            if found:\n                print(f\"{format_log_prefix('result')} Recovered {found} gene name(s) from UniProt. Genes found:\")\n                filled_clean = [str(g) for g in filled if pd.notna(g)]\n                preview = \", \".join(filled_clean[:10])\n                if found &gt; 10:\n                    preview += \"...\"\n                print(\"        \", preview)\n            if unknown:\n                missing_ids = self.prot.var.loc[missing_mask].index[pd.isna(filled)]\n                print(f\"{format_log_prefix('warn')} {unknown} gene name(s) still missing. Assigned as 'UNKNOWN_&lt;accession&gt;' for:\")\n                print(\"        \", \", \".join(missing_ids[:5]) + (\"...\" if unknown &gt; 10 else \"\"))\n                print(\"     \ud83d\udca1 Tip: You can update these using `pdata.update_identifier_maps({'GENE': 'ACCESSION'}, on='protein', direction='reverse', overwrite=True)`\\n\")\n\n    def search_annotations(self, query, on='protein', search_columns=None, case=False, return_all_matches=True):\n        \"\"\"\n        Search protein or peptide annotations for matching biological terms.\n\n        This function scans `.prot.var` or `.pep.var` for entries containing the provided keyword(s),\n        across common annotation fields.\n\n        Args:\n            query (str or list of str): Term(s) to search for (e.g., \"keratin\", \"KRT\").\n            on (str): Whether to search `\"protein\"` or `\"peptide\"` annotations (default: `\"protein\"`).\n            search_columns (list of str, optional): Columns to search in. Defaults to common biological fields.\n            case (bool): Case-sensitive search (default: False).\n            return_all_matches (bool): If True, return matches from any column. If False, returns only rows that match all terms.\n\n        Returns:\n            pd.DataFrame: Filtered dataframe with a `Matched` column (True/False) and optionally match columns per term.\n\n        Example:\n            ```python\n            pdata.search_annotations(\"keratin\")\n            pdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n            ```\n        \"\"\"\n        import pandas as pd\n\n        adata = self.prot if on == \"protein\" else self.pep\n        df = adata.var.copy()\n\n        if search_columns is None:\n            search_columns = [\n                \"Accession\", \"Description\", \"Biological Process\", \"Cellular Component\",\n                \"Molecular Function\", \"Genes\", \"Gene ID\", \"Reactome Pathways\"\n            ]\n\n        # Ensure index is available as a searchable column\n        df = df.copy()\n        df[\"Accession\"] = df.index.astype(str)\n\n        # Convert query to list\n        if isinstance(query, str):\n            query = [query]\n\n        # Search logic\n        def match_func(val, term):\n            if pd.isnull(val):\n                return False\n            return term in val if case else term.lower() in str(val).lower()\n\n        match_results = pd.DataFrame(index=df.index)\n\n        for term in query:\n            per_col_match = pd.DataFrame({\n                col: df[col].apply(match_func, args=(term,)) if col in df.columns else False\n                for col in search_columns\n            })\n            row_match = per_col_match.any(axis=1)\n            match_results[f\"Matched_{term}\"] = row_match\n\n        if return_all_matches:\n            matched_any = match_results.any(axis=1)\n        else:\n            matched_any = match_results.all(axis=1)\n\n        result_df = df.copy()\n        result_df[\"Matched\"] = matched_any\n        for col in match_results.columns:\n            result_df[col] = match_results[col]\n\n        return result_df[result_df[\"Matched\"]]\n</code></pre>"},{"location":"reference/pAnnData/identifier_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.get_identifier_maps","title":"get_identifier_maps","text":"<pre><code>get_identifier_maps(on='protein')\n</code></pre> <p>Retrieve gene/accession or peptide/protein mapping dictionaries.</p> <p>Depending on the <code>on</code> argument, returns a tuple of forward and reverse mappings:</p> <ul> <li> <p>If <code>on='protein'</code>: (gene \u2192 accession, accession \u2192 gene)</p> </li> <li> <p>If <code>on='peptide'</code>: (protein accession \u2192 peptide, peptide \u2192 protein accession)</p> </li> </ul> <p>Note: Alias <code>get_gene_maps()</code> also calls this function for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Source of mapping. Must be <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <p>Returns:</p> Type Description <p>Tuple[dict, dict]: (forward mapping, reverse mapping)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>on</code> is not <code>'protein'</code> or <code>'peptide'</code>.</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def get_identifier_maps(self, on='protein'):\n    \"\"\"\n    Retrieve gene/accession or peptide/protein mapping dictionaries.\n\n    Depending on the `on` argument, returns a tuple of forward and reverse mappings:\n\n    - If `on='protein'`: (gene \u2192 accession, accession \u2192 gene)\n\n    - If `on='peptide'`: (protein accession \u2192 peptide, peptide \u2192 protein accession)\n\n    Note: Alias `get_gene_maps()` also calls this function for compatibility.\n\n    Args:\n        on (str): Source of mapping. Must be `'protein'` or `'peptide'`.\n\n    Returns:\n        Tuple[dict, dict]: (forward mapping, reverse mapping)\n\n    Raises:\n        ValueError: If `on` is not `'protein'` or `'peptide'`.\n    \"\"\"\n    if on in ('protein','prot'):\n        return self._cached_identifier_maps_protein\n    elif on in ('peptide','pep'):\n        return self._cached_identifier_maps_peptide\n    else:\n        raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n</code></pre>"},{"location":"reference/pAnnData/identifier_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.refresh_identifier_maps","title":"refresh_identifier_maps","text":"<pre><code>refresh_identifier_maps()\n</code></pre> <p>Clear cached identifier maps to force regeneration on next access.</p> <p>This removes the following attributes if present:</p> <ul> <li><code>_gene_maps_protein</code>: Gene \u2194 Accession map for proteins</li> <li><code>_protein_maps_peptide</code>: Protein \u2194 Peptide map for peptides</li> </ul> <p>Useful when <code>.var</code> annotations are updated and identifier mappings may have changed.</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def refresh_identifier_maps(self):\n    \"\"\"\n    Clear cached identifier maps to force regeneration on next access.\n\n    This removes the following attributes if present:\n\n    - `_gene_maps_protein`: Gene \u2194 Accession map for proteins\n    - `_protein_maps_peptide`: Protein \u2194 Peptide map for peptides\n\n    Useful when `.var` annotations are updated and identifier mappings may have changed.\n    \"\"\"\n    for attr in [\"_gene_maps_protein\", \"_protein_maps_peptide\"]:\n        if hasattr(self, attr):\n            delattr(self, attr)\n</code></pre>"},{"location":"reference/pAnnData/identifier_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.search_annotations","title":"search_annotations","text":"<pre><code>search_annotations(query, on='protein', search_columns=None, case=False, return_all_matches=True)\n</code></pre> <p>Search protein or peptide annotations for matching biological terms.</p> <p>This function scans <code>.prot.var</code> or <code>.pep.var</code> for entries containing the provided keyword(s), across common annotation fields.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str or list of str</code> <p>Term(s) to search for (e.g., \"keratin\", \"KRT\").</p> required <code>on</code> <code>str</code> <p>Whether to search <code>\"protein\"</code> or <code>\"peptide\"</code> annotations (default: <code>\"protein\"</code>).</p> <code>'protein'</code> <code>search_columns</code> <code>list of str</code> <p>Columns to search in. Defaults to common biological fields.</p> <code>None</code> <code>case</code> <code>bool</code> <p>Case-sensitive search (default: False).</p> <code>False</code> <code>return_all_matches</code> <code>bool</code> <p>If True, return matches from any column. If False, returns only rows that match all terms.</p> <code>True</code> <p>Returns:</p> Type Description <p>pd.DataFrame: Filtered dataframe with a <code>Matched</code> column (True/False) and optionally match columns per term.</p> Example <pre><code>pdata.search_annotations(\"keratin\")\npdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n</code></pre> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def search_annotations(self, query, on='protein', search_columns=None, case=False, return_all_matches=True):\n    \"\"\"\n    Search protein or peptide annotations for matching biological terms.\n\n    This function scans `.prot.var` or `.pep.var` for entries containing the provided keyword(s),\n    across common annotation fields.\n\n    Args:\n        query (str or list of str): Term(s) to search for (e.g., \"keratin\", \"KRT\").\n        on (str): Whether to search `\"protein\"` or `\"peptide\"` annotations (default: `\"protein\"`).\n        search_columns (list of str, optional): Columns to search in. Defaults to common biological fields.\n        case (bool): Case-sensitive search (default: False).\n        return_all_matches (bool): If True, return matches from any column. If False, returns only rows that match all terms.\n\n    Returns:\n        pd.DataFrame: Filtered dataframe with a `Matched` column (True/False) and optionally match columns per term.\n\n    Example:\n        ```python\n        pdata.search_annotations(\"keratin\")\n        pdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n        ```\n    \"\"\"\n    import pandas as pd\n\n    adata = self.prot if on == \"protein\" else self.pep\n    df = adata.var.copy()\n\n    if search_columns is None:\n        search_columns = [\n            \"Accession\", \"Description\", \"Biological Process\", \"Cellular Component\",\n            \"Molecular Function\", \"Genes\", \"Gene ID\", \"Reactome Pathways\"\n        ]\n\n    # Ensure index is available as a searchable column\n    df = df.copy()\n    df[\"Accession\"] = df.index.astype(str)\n\n    # Convert query to list\n    if isinstance(query, str):\n        query = [query]\n\n    # Search logic\n    def match_func(val, term):\n        if pd.isnull(val):\n            return False\n        return term in val if case else term.lower() in str(val).lower()\n\n    match_results = pd.DataFrame(index=df.index)\n\n    for term in query:\n        per_col_match = pd.DataFrame({\n            col: df[col].apply(match_func, args=(term,)) if col in df.columns else False\n            for col in search_columns\n        })\n        row_match = per_col_match.any(axis=1)\n        match_results[f\"Matched_{term}\"] = row_match\n\n    if return_all_matches:\n        matched_any = match_results.any(axis=1)\n    else:\n        matched_any = match_results.all(axis=1)\n\n    result_df = df.copy()\n    result_df[\"Matched\"] = matched_any\n    for col in match_results.columns:\n        result_df[col] = match_results[col]\n\n    return result_df[result_df[\"Matched\"]]\n</code></pre>"},{"location":"reference/pAnnData/identifier_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.update_identifier_maps","title":"update_identifier_maps","text":"<pre><code>update_identifier_maps(mapping, on='protein', direction='forward', overwrite=False, verbose=True)\n</code></pre> <p>Update cached identifier maps with user-supplied mappings.</p> <p>This function updates the internal forward and reverse identifier maps for either proteins or peptides. Ensures consistency by updating both directions of the mapping.</p> <ul> <li> <p>For <code>'protein'</code>:</p> <ul> <li>forward: gene \u2192 accession  </li> <li>reverse: accession \u2192 gene</li> </ul> </li> <li> <p>For <code>'peptide'</code>:</p> <ul> <li>forward: protein accession \u2192 peptide</li> <li>reverse: peptide \u2192 protein accession</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>dict</code> <p>Dictionary of mappings to add.</p> required <code>on</code> <code>str</code> <p>Which maps to update. Must be <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <code>direction</code> <code>str</code> <p><code>'forward'</code> or <code>'reverse'</code> \u2014 determines how the <code>mapping</code> should be interpreted.</p> <code>'forward'</code> <code>overwrite</code> <code>bool</code> <p>If True, allows overwriting existing entries.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, prints a summary of updated keys.</p> <code>True</code> Note <p>The corresponding reverse map is automatically updated to maintain bidirectional consistency.</p> Example <p>Add new gene-to-accession mappings (protein):     <pre><code>pdata.update_identifier_maps(\n    {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n    on='protein',\n    direction='forward'\n)\n</code></pre></p> <p>Add peptide \u2192 protein mappings:     <pre><code>pdata.update_identifier_maps(\n    {'PEPTIDE_ABC': 'P12345'},\n    on='peptide',\n    direction='reverse'\n)\n</code></pre></p> <p>Overwrite a protein \u2192 gene mapping:     <pre><code>pdata.update_identifier_maps(\n    {'P12345': 'NEWGENE'},\n    on='protein',\n    direction='reverse',\n    overwrite=True\n)\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def update_identifier_maps(self, mapping, on='protein', direction='forward', overwrite=False, verbose=True):\n    \"\"\"\n    Update cached identifier maps with user-supplied mappings.\n\n    This function updates the internal forward and reverse identifier maps\n    for either proteins or peptides. Ensures consistency by updating both\n    directions of the mapping.\n\n    - For `'protein'`:\n        * forward: gene \u2192 accession  \n        * reverse: accession \u2192 gene\n\n    - For `'peptide'`:\n        * forward: protein accession \u2192 peptide\n        * reverse: peptide \u2192 protein accession\n\n    Args:\n        mapping (dict): Dictionary of mappings to add.\n        on (str): Which maps to update. Must be `'protein'` or `'peptide'`.\n        direction (str): `'forward'` or `'reverse'` \u2014 determines how the `mapping` should be interpreted.\n        overwrite (bool): If True, allows overwriting existing entries.\n        verbose (bool): If True, prints a summary of updated keys.\n\n    Note:\n        The corresponding reverse map is automatically updated to maintain bidirectional consistency.\n\n    Example:\n        Add new gene-to-accession mappings (protein):\n            ```python\n            pdata.update_identifier_maps(\n                {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n                on='protein',\n                direction='forward'\n            )\n            ```\n\n        Add peptide \u2192 protein mappings:\n            ```python\n            pdata.update_identifier_maps(\n                {'PEPTIDE_ABC': 'P12345'},\n                on='peptide',\n                direction='reverse'\n            )\n            ```\n\n        Overwrite a protein \u2192 gene mapping:\n            ```python\n            pdata.update_identifier_maps(\n                {'P12345': 'NEWGENE'},\n                on='protein',\n                direction='reverse',\n                overwrite=True\n            )\n            ```\n\n    \"\"\"\n    if on == 'protein':\n        forward, reverse = self._cached_identifier_maps_protein\n    elif on == 'peptide':\n        forward, reverse = self._cached_identifier_maps_peptide\n    else:\n        raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n    source_map = forward if direction == 'forward' else reverse\n    target_map = reverse if direction == 'forward' else forward\n\n    added, updated, skipped = 0, 0, 0\n\n    for key, val in mapping.items():\n        if key in source_map:\n            if overwrite:\n                source_map[key] = val\n                target_map[val] = key\n                updated += 1\n            else:\n                skipped += 1\n        else:\n            source_map[key] = val\n            target_map[val] = key\n            added += 1\n\n    message = (\n        f\"[update_identifier_maps] Updated '{on}' ({direction}): \"\n        f\"{added} added, {updated} overwritten, {skipped} skipped.\"\n    )\n\n    if verbose:\n        print(message)\n    self._append_history(message)\n\n    # Update .prot.var[\"Genes\"] if updating protein identifier reverse map (accession \u2192 gene)\n    if on == 'protein' and direction == 'reverse':\n        updated_var_count = 0\n        updated_accessions = []\n\n        for acc, gene in mapping.items():\n            if acc in self.prot.var_names:\n                self.prot.var.at[acc, \"Genes\"] = gene\n                updated_accessions.append(acc)\n                updated_var_count += 1\n\n        if updated_var_count &gt; 0:\n            var_message = (\n                f\"\ud83d\udd01 Updated `.prot.var['Genes']` for {updated_var_count} entries from custom mapping. \"\n                f\"(View details in `pdata.metadata['identifier_map_history']`)\"\n            )\n            if verbose:\n                print(var_message)\n            self._append_history(var_message)\n\n    # Log detailed update history for all cases\n    import datetime\n\n    record = {\n        'on': on,\n        'direction': direction,\n        'input_mapping': dict(mapping),  # shallow copy\n        'overwrite': overwrite,\n        'timestamp': datetime.datetime.now().isoformat(timespec='seconds'),\n        'summary': {\n            'added': added,\n            'updated': updated,\n            'skipped': skipped,\n        }\n    }\n\n    if on == 'protein' and direction == 'reverse':\n        record['updated_var_column'] = {\n            'column': 'Genes',\n            'accessions': updated_accessions,\n            'n_updated': updated_var_count\n        }\n\n    self.metadata.setdefault(\"identifier_map_history\", []).append(record)\n</code></pre>"},{"location":"reference/pAnnData/identifier_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.update_missing_genes","title":"update_missing_genes","text":"<pre><code>update_missing_genes(gene_col='Genes', verbose=True)\n</code></pre> <p>Fill missing gene names in <code>.prot.var</code> using UniProt API.</p> <p>This function searches for missing values in the specified gene column and attempts to fill them by querying the UniProt API using protein accession IDs. If a gene name cannot be found, a placeholder 'UNKNOWN_' is used instead. <p>Parameters:</p> Name Type Description Default <code>gene_col</code> <code>str</code> <p>Column name in <code>.prot.var</code> to update (default: \"Genes\").</p> <code>'Genes'</code> <code>verbose</code> <code>bool</code> <p>Whether to print summary information (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Note <ul> <li>This function only operates on <code>.prot.var</code>, not <code>.pep.var</code>.</li> <li>If UniProt is unavailable or returns no match, the missing entry is filled as <code>'UNKNOWN_&lt;accession&gt;'</code>.</li> <li>To manually correct unknown entries later, use <code>update_identifier_maps()</code> with <code>direction='reverse'</code>.</li> </ul> Example <p>Automatically fill missing gene names using UniProt:     <pre><code>pdata.update_missing_genes()\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def update_missing_genes(self, gene_col=\"Genes\", verbose=True):\n    \"\"\"\n    Fill missing gene names in `.prot.var` using UniProt API.\n\n    This function searches for missing values in the specified gene column\n    and attempts to fill them by querying the UniProt API using protein\n    accession IDs. If a gene name cannot be found, a placeholder\n    'UNKNOWN_&lt;accession&gt;' is used instead.\n\n    Args:\n        gene_col (str): Column name in `.prot.var` to update (default: \"Genes\").\n        verbose (bool): Whether to print summary information (default: True).\n\n    Returns:\n        None\n\n    Note:\n        - This function only operates on `.prot.var`, not `.pep.var`.\n        - If UniProt is unavailable or returns no match, the missing entry is filled as `'UNKNOWN_&lt;accession&gt;'`.\n        - To manually correct unknown entries later, use `update_identifier_maps()` with `direction='reverse'`.\n\n    Example:\n        Automatically fill missing gene names using UniProt:\n            ```python\n            pdata.update_missing_genes()\n            ```\n    \"\"\"\n    var = self.prot.var\n\n    if gene_col not in var.columns:\n        if verbose:\n            print(f\"{format_log_prefix('warn')} Column '{gene_col}' not found in .prot.var.\")\n        return\n\n    missing_mask = var[gene_col].isna()\n    if not missing_mask.any():\n        if verbose:\n            print(f\"{format_log_prefix('result')} No missing gene names found.\")\n        return\n\n    accessions = var.index[missing_mask].tolist()\n    if verbose:\n        print(f\"{format_log_prefix('info_only')} {len(accessions)} proteins with missing gene names.\")\n\n    try:\n        df = utils.get_uniprot_fields(\n            accessions,\n            search_fields=[\"accession\", \"gene_primary\"],\n            standardize=True\n        )\n    except Exception as e:\n        print(f\"{format_log_prefix('error')} UniProt query failed: {e}\")\n        return\n    df = utils.standardize_uniprot_columns(df)\n\n    if df.empty or \"accession\" not in df.columns or \"gene_primary\" not in df.columns:\n        print(f\"{format_log_prefix('warn')} UniProt returned no usable gene mapping columns.\")\n        return\n\n    gene_map = dict(zip(df[\"accession\"], df[\"gene_primary\"]))\n    filled = self.prot.var.loc[missing_mask].index.map(lambda acc: gene_map.get(acc))\n    final_genes = [\n        gene if pd.notna(gene) else f\"UNKNOWN_{acc}\"\n        for acc, gene in zip(self.prot.var.loc[missing_mask].index, filled)\n    ]\n    self.prot.var.loc[missing_mask, gene_col] = final_genes\n\n    found = sum(pd.notna(filled))\n    unknown = len(final_genes) - found\n    if verbose:\n        if found:\n            print(f\"{format_log_prefix('result')} Recovered {found} gene name(s) from UniProt. Genes found:\")\n            filled_clean = [str(g) for g in filled if pd.notna(g)]\n            preview = \", \".join(filled_clean[:10])\n            if found &gt; 10:\n                preview += \"...\"\n            print(\"        \", preview)\n        if unknown:\n            missing_ids = self.prot.var.loc[missing_mask].index[pd.isna(filled)]\n            print(f\"{format_log_prefix('warn')} {unknown} gene name(s) still missing. Assigned as 'UNKNOWN_&lt;accession&gt;' for:\")\n            print(\"        \", \", \".join(missing_ids[:5]) + (\"...\" if unknown &gt; 10 else \"\"))\n            print(\"     \ud83d\udca1 Tip: You can update these using `pdata.update_identifier_maps({'GENE': 'ACCESSION'}, on='protein', direction='reverse', overwrite=True)`\\n\")\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/","title":"Importing","text":"<p>Mixins for importing data into <code>pAnndata</code> objects.</p>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.IOMixin","title":"IOMixin","text":"<p>Data import utilities for building <code>pAnnData</code> objects from supported proteomics tools.</p> <p>This module provides functions to parse outputs from common tools such as Proteome Discoverer and DIA-NN, automatically extracting protein and peptide quantification matrices, sample metadata, and relational mappings between peptides and proteins.</p> Supported tools <ul> <li>Proteome Discoverer (PD 3.1, PD 2.4, etc.)</li> <li>DIA-NN (&lt;1.8.1 and &gt;2.0)</li> </ul> <p>Methods:</p> Name Description <code>import_data</code> <p>Main entry point that dispatches to the appropriate import function based on source_type.</p> <code>import_proteomeDiscoverer</code> <p>Parses PD output files and initializes a pAnnData object.</p> <code>import_diann</code> <p>Parses DIA-NN report file and initializes a pAnnData object.</p> <code>resolve_obs_columns</code> <p>Extracts <code>.obs</code> column structure from filenames or metadata.</p> <code>suggest_obs_from_file</code> <p>Suggests sample-level metadata based on consistent filename tokens.</p> <code>analyze_filename_formats</code> <p>Analyzes filename structures to identify possible grouping patterns.</p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>class IOMixin:\n    \"\"\"\n    Data import utilities for building `pAnnData` objects from supported proteomics tools.\n\n    This module provides functions to parse outputs from common tools such as Proteome Discoverer and DIA-NN,\n    automatically extracting protein and peptide quantification matrices, sample metadata, and relational mappings\n    between peptides and proteins.\n\n    Supported tools:\n        - Proteome Discoverer (PD 3.1, PD 2.4, etc.)\n        - DIA-NN (&lt;1.8.1 and &gt;2.0)\n\n    Functions:\n        import_data: Main entry point that dispatches to the appropriate import function based on source_type.\n        import_proteomeDiscoverer: Parses PD output files and initializes a pAnnData object.\n        import_diann: Parses DIA-NN report file and initializes a pAnnData object.\n        resolve_obs_columns: Extracts `.obs` column structure from filenames or metadata.\n        suggest_obs_from_file: Suggests sample-level metadata based on consistent filename tokens.\n        analyze_filename_formats: Analyzes filename structures to identify possible grouping patterns.\n    \"\"\"\n\n    @classmethod\n    def import_data(cls, *args, **kwargs):\n        \"\"\"\n        Unified wrapper for importing data into a `pAnnData` object.\n\n        This function routes to a specific import handler based on the `source_type`,\n        such as Proteome Discoverer or DIA-NN. It parses protein/peptide expression data\n        and associated sample metadata, returning a fully initialized `pAnnData` object.\n\n        Args:\n            source_type (str): The input tool or data source. Supported values:\n\n                - `'pd'`, `'proteomeDiscoverer'`, `'pd13'`, `'pd24'`:  \n                \u2192 Uses `import_proteomeDiscoverer()`.  \n                Required kwargs:\n                    - `prot_file` (str): Path to protein-level report file\n                    - `obs_columns` (list of str): Columns to extract for `.obs`\n                Optional kwargs:\n                    - `pep_file` (str): Path to peptide-level report file\n\n                - `'diann'`, `'dia-nn'`:  \n                \u2192 Uses `import_diann()`.  \n                Required kwargs:\n                    - `report_file` (str): Path to DIA-NN report file\n                    - `obs_columns` (list of str): Columns to extract for `.obs`\n\n                - `'fragpipe'`, `'fp'`: Not yet implemented  \n                - `'spectronaut'`, `'sn'`: Not yet implemented\n\n            **kwargs: Additional keyword arguments forwarded to the relevant import function.\n\n        Returns:\n            pAnnData: A populated pAnnData object with `.prot`, `.pep`, `.summary`, and identifier mappings.\n\n        Example:\n            Importing Proteome Discoverer output for single-cell data:\n                ```python\n                obs_columns = ['Sample', 'method', 'duration', 'cell_line']\n                pdata_untreated_sc = import_data(\n                    source_type='pd',\n                    prot_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_prot_Proteins.txt',\n                    pep_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_pep_PeptideGroups.txt',\n                    obs_columns=obs_columns\n                )\n                ```\n\n            Importing PD output for bulk data from an Excel file:\n                ```python\n                obs_columns = ['Sample', 'cell_line']\n                pdata_bulk = import_data(\n                    source_type='pd',\n                    prot_file='HCT116 resistance_20230601_pdoutput.xlsx',\n                    obs_columns=obs_columns\n                )\n                ```\n\n        Note:\n            If `obs_columns` is not provided and filename formats are inconsistent,\n            fallback parsing is applied with generic columns (`\"File\"`, `\"parsingType\"`).\n        \"\"\"\n        return import_data(*args, **kwargs)\n\n    @classmethod\n    def import_diann(cls, *args, **kwargs):\n        \"\"\"\n        Import DIA-NN output into a `pAnnData` object.\n\n        This function parses a DIA-NN report file and separates protein- and peptide-level expression matrices\n        using the specified abundance and metadata columns.\n\n        Args:\n            report_file (str): Path to the DIA-NN report file (required).\n            obs_columns (list of str): List of metadata columns to extract from the filename for `.obs`.\n            prot_value (str): Column name in DIA-NN output to use for protein quantification.\n                Default: `'PG.MaxLFQ'`.\n            pep_value (str): Column name in DIA-NN output to use for peptide quantification.\n                Default: `'Precursor.Normalised'`.\n            prot_var_columns (list of str): Columns from the protein group table to store in `.prot.var`.\n                Default includes gene and master protein annotations.\n            pep_var_columns (list of str): Columns from the precursor table to store in `.pep.var`.\n                Default includes peptide sequence, precursor ID, and mapping annotations.\n            **kwargs: Additional keyword arguments passed to `import_data()`.\n\n        Returns:\n            pAnnData: A populated object with `.prot`, `.pep`, `.summary`, and identifier mappings.\n\n        Example:\n            To import data from a DIA-NN report file:\n                ```python\n                obs_columns = ['Sample', 'treatment', 'replicate']\n                pdata = import_diann(\n                    report_file='data/project_diaNN_output.tsv',\n                    obs_columns=obs_columns,\n                    prot_value='PG.MaxLFQ',\n                    pep_value='Precursor.Normalised'\n                )\n                ```\n\n        Note:\n            - DIA-NN report should contain both protein group and precursor-level information.\n            - Metadata columns in filenames must be consistently formatted to extract `.obs`.\n        \"\"\"\n        return import_diann(*args, **kwargs)\n\n    @classmethod\n    def import_proteomeDiscoverer(cls, *args, **kwargs):\n        \"\"\"\n        Import Proteome Discoverer (PD) output into a `pAnnData` object.\n\n        This is a convenience wrapper for `import_data(source_type='pd')`. It loads protein- and optionally peptide-level\n        expression data from PD report files and parses sample metadata columns.\n\n        Args:\n            prot_file (str): Path to the protein-level report file (required).\n            pep_file (str, optional): Path to the peptide-level report file (optional but recommended).\n            obs_columns (list of str): List of columns to extract for `.obs`. These should match metadata tokens\n                embedded in the filenames (e.g. sample, condition, replicate).\n            **kwargs: Additional keyword arguments passed to `import_data()`.\n\n        Returns:\n            pAnnData: A populated object with `.prot`, `.pep` (if provided), `.summary`, and identifier mappings.\n\n        Example:\n            To import data from Proteome Discoverer:\n                ```python\n                obs_columns = ['Sample', 'condition', 'cell_line']\n                pdata = import_proteomeDiscoverer(\n                    prot_file='my_project/proteins.txt',\n                    pep_file='my_project/peptides.txt',\n                    obs_columns=obs_columns\n                )\n                ```\n\n        Note:\n            - If `pep_file` is omitted, the resulting `pAnnData` will not include `.pep` or an RS matrix.\n            - If filename structure is inconsistent and `obs_columns` cannot be inferred, fallback columns are used.\n        \"\"\"\n        return import_proteomeDiscoverer(*args, **kwargs)\n\n    @classmethod\n    def get_filenames(cls, *args, **kwargs):\n        \"\"\"\n        Extract sample filenames from a DIA-NN or Proteome Discoverer report.\n\n        For DIA-NN reports, this extracts the 'Run' column from the table.\n        For Proteome Discoverer (PD) output, it collects unique sample identifiers \n        based on column headers (e.g. abundance columns like \"Abundances (SampleX)\").\n\n        Args:\n            source (str or Path): Path to the input report file.\n            source_type (str): Tool used to generate the report. Must be one of {'diann', 'pd'}.\n\n        Returns:\n            list of str: Extracted list of sample names or run filenames.\n\n        Example:\n            Extract DIA-NN run names:\n                ```python\n                get_filenames(\"diann_output.tsv\", source_type=\"diann\")\n                ```\n                ```\n                ['Sample1.raw', 'Sample2.raw', 'Sample3.raw']\n                ```\n\n            Extract PD sample names from abundance columns:\n                ```python\n                get_filenames(\"pd_output.xlsx\", source_type=\"pd\")\n                ```\n                ```\n                ['SampleA', 'SampleB', 'SampleC']\n                ```\n        \"\"\"\n        return get_filenames(*args, **kwargs)\n\n    @classmethod\n    def suggest_obs_columns(cls, *args, **kwargs):\n        \"\"\"\n        Suggest `.obs` column names based on parsed sample names.\n\n        This function analyzes filenames or run names extracted from Proteome Discoverer\n        or DIA-NN reports and attempts to identify consistent metadata fields. These fields\n        may include `gradient`, `amount`, `cell_line`, or `well_position`, depending on\n        naming conventions and regular expression matches.\n\n        Args:\n            source (str or Path, optional): Path to a DIA-NN or PD output file.\n            source_type (str, optional): Type of the input file. Supports `'diann'` or `'pd'`.\n                If not provided, inferred from filename or fallback heuristics.\n            filenames (list of str, optional): List of sample file names or run labels to parse.\n                If provided, bypasses file loading.\n            delimiter (str, optional): Delimiter to use for tokenizing filenames (e.g., `','`, `'_'`).\n                If not specified, will be inferred automatically.\n\n        Returns:\n            list of str: Suggested list of metadata column names to assign to `.obs`.\n\n        Example:\n            To suggest observation columns from a file:\n                ```python\n                suggest_obs_columns(\"my_experiment_PD.txt\", source_type=\"pd\")\n                ```\n\n                ```\n                # Suggested columns: ['Sample', 'gradient', 'cell_line', 'duration']\n                ['Sample', 'gradient', 'cell_line', 'duration']\n                ```\n\n        Note:\n            This function is typically used as part of the `.import_data()` flow\n            when filenames embed experimental metadata.\n        \"\"\"\n        return suggest_obs_columns(*args, **kwargs)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.IOMixin.get_filenames","title":"get_filenames  <code>classmethod</code>","text":"<pre><code>get_filenames(*args, **kwargs)\n</code></pre> <p>Extract sample filenames from a DIA-NN or Proteome Discoverer report.</p> <p>For DIA-NN reports, this extracts the 'Run' column from the table. For Proteome Discoverer (PD) output, it collects unique sample identifiers  based on column headers (e.g. abundance columns like \"Abundances (SampleX)\").</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str or Path</code> <p>Path to the input report file.</p> required <code>source_type</code> <code>str</code> <p>Tool used to generate the report. Must be one of {'diann', 'pd'}.</p> required <p>Returns:</p> Type Description <p>list of str: Extracted list of sample names or run filenames.</p> Example <p>Extract DIA-NN run names:     <pre><code>get_filenames(\"diann_output.tsv\", source_type=\"diann\")\n</code></pre> <pre><code>['Sample1.raw', 'Sample2.raw', 'Sample3.raw']\n</code></pre></p> <p>Extract PD sample names from abundance columns:     <pre><code>get_filenames(\"pd_output.xlsx\", source_type=\"pd\")\n</code></pre> <pre><code>['SampleA', 'SampleB', 'SampleC']\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>@classmethod\ndef get_filenames(cls, *args, **kwargs):\n    \"\"\"\n    Extract sample filenames from a DIA-NN or Proteome Discoverer report.\n\n    For DIA-NN reports, this extracts the 'Run' column from the table.\n    For Proteome Discoverer (PD) output, it collects unique sample identifiers \n    based on column headers (e.g. abundance columns like \"Abundances (SampleX)\").\n\n    Args:\n        source (str or Path): Path to the input report file.\n        source_type (str): Tool used to generate the report. Must be one of {'diann', 'pd'}.\n\n    Returns:\n        list of str: Extracted list of sample names or run filenames.\n\n    Example:\n        Extract DIA-NN run names:\n            ```python\n            get_filenames(\"diann_output.tsv\", source_type=\"diann\")\n            ```\n            ```\n            ['Sample1.raw', 'Sample2.raw', 'Sample3.raw']\n            ```\n\n        Extract PD sample names from abundance columns:\n            ```python\n            get_filenames(\"pd_output.xlsx\", source_type=\"pd\")\n            ```\n            ```\n            ['SampleA', 'SampleB', 'SampleC']\n            ```\n    \"\"\"\n    return get_filenames(*args, **kwargs)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.IOMixin.import_data","title":"import_data  <code>classmethod</code>","text":"<pre><code>import_data(*args, **kwargs)\n</code></pre> <p>Unified wrapper for importing data into a <code>pAnnData</code> object.</p> <p>This function routes to a specific import handler based on the <code>source_type</code>, such as Proteome Discoverer or DIA-NN. It parses protein/peptide expression data and associated sample metadata, returning a fully initialized <code>pAnnData</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>source_type</code> <code>str</code> <p>The input tool or data source. Supported values:</p> <ul> <li> <p><code>'pd'</code>, <code>'proteomeDiscoverer'</code>, <code>'pd13'</code>, <code>'pd24'</code>: \u2192 Uses <code>import_proteomeDiscoverer()</code>. Required kwargs:</p> <ul> <li><code>prot_file</code> (str): Path to protein-level report file</li> <li><code>obs_columns</code> (list of str): Columns to extract for <code>.obs</code> Optional kwargs:</li> <li><code>pep_file</code> (str): Path to peptide-level report file</li> </ul> </li> <li> <p><code>'diann'</code>, <code>'dia-nn'</code>: \u2192 Uses <code>import_diann()</code>. Required kwargs:</p> <ul> <li><code>report_file</code> (str): Path to DIA-NN report file</li> <li><code>obs_columns</code> (list of str): Columns to extract for <code>.obs</code></li> </ul> </li> <li> <p><code>'fragpipe'</code>, <code>'fp'</code>: Not yet implemented  </p> </li> <li><code>'spectronaut'</code>, <code>'sn'</code>: Not yet implemented</li> </ul> required <code>**kwargs</code> <p>Additional keyword arguments forwarded to the relevant import function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>A populated pAnnData object with <code>.prot</code>, <code>.pep</code>, <code>.summary</code>, and identifier mappings.</p> Example <p>Importing Proteome Discoverer output for single-cell data:     <pre><code>obs_columns = ['Sample', 'method', 'duration', 'cell_line']\npdata_untreated_sc = import_data(\n    source_type='pd',\n    prot_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_prot_Proteins.txt',\n    pep_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_pep_PeptideGroups.txt',\n    obs_columns=obs_columns\n)\n</code></pre></p> <p>Importing PD output for bulk data from an Excel file:     <pre><code>obs_columns = ['Sample', 'cell_line']\npdata_bulk = import_data(\n    source_type='pd',\n    prot_file='HCT116 resistance_20230601_pdoutput.xlsx',\n    obs_columns=obs_columns\n)\n</code></pre></p> Note <p>If <code>obs_columns</code> is not provided and filename formats are inconsistent, fallback parsing is applied with generic columns (<code>\"File\"</code>, <code>\"parsingType\"</code>).</p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>@classmethod\ndef import_data(cls, *args, **kwargs):\n    \"\"\"\n    Unified wrapper for importing data into a `pAnnData` object.\n\n    This function routes to a specific import handler based on the `source_type`,\n    such as Proteome Discoverer or DIA-NN. It parses protein/peptide expression data\n    and associated sample metadata, returning a fully initialized `pAnnData` object.\n\n    Args:\n        source_type (str): The input tool or data source. Supported values:\n\n            - `'pd'`, `'proteomeDiscoverer'`, `'pd13'`, `'pd24'`:  \n            \u2192 Uses `import_proteomeDiscoverer()`.  \n            Required kwargs:\n                - `prot_file` (str): Path to protein-level report file\n                - `obs_columns` (list of str): Columns to extract for `.obs`\n            Optional kwargs:\n                - `pep_file` (str): Path to peptide-level report file\n\n            - `'diann'`, `'dia-nn'`:  \n            \u2192 Uses `import_diann()`.  \n            Required kwargs:\n                - `report_file` (str): Path to DIA-NN report file\n                - `obs_columns` (list of str): Columns to extract for `.obs`\n\n            - `'fragpipe'`, `'fp'`: Not yet implemented  \n            - `'spectronaut'`, `'sn'`: Not yet implemented\n\n        **kwargs: Additional keyword arguments forwarded to the relevant import function.\n\n    Returns:\n        pAnnData: A populated pAnnData object with `.prot`, `.pep`, `.summary`, and identifier mappings.\n\n    Example:\n        Importing Proteome Discoverer output for single-cell data:\n            ```python\n            obs_columns = ['Sample', 'method', 'duration', 'cell_line']\n            pdata_untreated_sc = import_data(\n                source_type='pd',\n                prot_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_prot_Proteins.txt',\n                pep_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_pep_PeptideGroups.txt',\n                obs_columns=obs_columns\n            )\n            ```\n\n        Importing PD output for bulk data from an Excel file:\n            ```python\n            obs_columns = ['Sample', 'cell_line']\n            pdata_bulk = import_data(\n                source_type='pd',\n                prot_file='HCT116 resistance_20230601_pdoutput.xlsx',\n                obs_columns=obs_columns\n            )\n            ```\n\n    Note:\n        If `obs_columns` is not provided and filename formats are inconsistent,\n        fallback parsing is applied with generic columns (`\"File\"`, `\"parsingType\"`).\n    \"\"\"\n    return import_data(*args, **kwargs)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.IOMixin.import_diann","title":"import_diann  <code>classmethod</code>","text":"<pre><code>import_diann(*args, **kwargs)\n</code></pre> <p>Import DIA-NN output into a <code>pAnnData</code> object.</p> <p>This function parses a DIA-NN report file and separates protein- and peptide-level expression matrices using the specified abundance and metadata columns.</p> <p>Parameters:</p> Name Type Description Default <code>report_file</code> <code>str</code> <p>Path to the DIA-NN report file (required).</p> required <code>obs_columns</code> <code>list of str</code> <p>List of metadata columns to extract from the filename for <code>.obs</code>.</p> required <code>prot_value</code> <code>str</code> <p>Column name in DIA-NN output to use for protein quantification. Default: <code>'PG.MaxLFQ'</code>.</p> required <code>pep_value</code> <code>str</code> <p>Column name in DIA-NN output to use for peptide quantification. Default: <code>'Precursor.Normalised'</code>.</p> required <code>prot_var_columns</code> <code>list of str</code> <p>Columns from the protein group table to store in <code>.prot.var</code>. Default includes gene and master protein annotations.</p> required <code>pep_var_columns</code> <code>list of str</code> <p>Columns from the precursor table to store in <code>.pep.var</code>. Default includes peptide sequence, precursor ID, and mapping annotations.</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to <code>import_data()</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>A populated object with <code>.prot</code>, <code>.pep</code>, <code>.summary</code>, and identifier mappings.</p> Example <p>To import data from a DIA-NN report file:     <pre><code>obs_columns = ['Sample', 'treatment', 'replicate']\npdata = import_diann(\n    report_file='data/project_diaNN_output.tsv',\n    obs_columns=obs_columns,\n    prot_value='PG.MaxLFQ',\n    pep_value='Precursor.Normalised'\n)\n</code></pre></p> Note <ul> <li>DIA-NN report should contain both protein group and precursor-level information.</li> <li>Metadata columns in filenames must be consistently formatted to extract <code>.obs</code>.</li> </ul> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>@classmethod\ndef import_diann(cls, *args, **kwargs):\n    \"\"\"\n    Import DIA-NN output into a `pAnnData` object.\n\n    This function parses a DIA-NN report file and separates protein- and peptide-level expression matrices\n    using the specified abundance and metadata columns.\n\n    Args:\n        report_file (str): Path to the DIA-NN report file (required).\n        obs_columns (list of str): List of metadata columns to extract from the filename for `.obs`.\n        prot_value (str): Column name in DIA-NN output to use for protein quantification.\n            Default: `'PG.MaxLFQ'`.\n        pep_value (str): Column name in DIA-NN output to use for peptide quantification.\n            Default: `'Precursor.Normalised'`.\n        prot_var_columns (list of str): Columns from the protein group table to store in `.prot.var`.\n            Default includes gene and master protein annotations.\n        pep_var_columns (list of str): Columns from the precursor table to store in `.pep.var`.\n            Default includes peptide sequence, precursor ID, and mapping annotations.\n        **kwargs: Additional keyword arguments passed to `import_data()`.\n\n    Returns:\n        pAnnData: A populated object with `.prot`, `.pep`, `.summary`, and identifier mappings.\n\n    Example:\n        To import data from a DIA-NN report file:\n            ```python\n            obs_columns = ['Sample', 'treatment', 'replicate']\n            pdata = import_diann(\n                report_file='data/project_diaNN_output.tsv',\n                obs_columns=obs_columns,\n                prot_value='PG.MaxLFQ',\n                pep_value='Precursor.Normalised'\n            )\n            ```\n\n    Note:\n        - DIA-NN report should contain both protein group and precursor-level information.\n        - Metadata columns in filenames must be consistently formatted to extract `.obs`.\n    \"\"\"\n    return import_diann(*args, **kwargs)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.IOMixin.import_proteomeDiscoverer","title":"import_proteomeDiscoverer  <code>classmethod</code>","text":"<pre><code>import_proteomeDiscoverer(*args, **kwargs)\n</code></pre> <p>Import Proteome Discoverer (PD) output into a <code>pAnnData</code> object.</p> <p>This is a convenience wrapper for <code>import_data(source_type='pd')</code>. It loads protein- and optionally peptide-level expression data from PD report files and parses sample metadata columns.</p> <p>Parameters:</p> Name Type Description Default <code>prot_file</code> <code>str</code> <p>Path to the protein-level report file (required).</p> required <code>pep_file</code> <code>str</code> <p>Path to the peptide-level report file (optional but recommended).</p> required <code>obs_columns</code> <code>list of str</code> <p>List of columns to extract for <code>.obs</code>. These should match metadata tokens embedded in the filenames (e.g. sample, condition, replicate).</p> required <code>**kwargs</code> <p>Additional keyword arguments passed to <code>import_data()</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>A populated object with <code>.prot</code>, <code>.pep</code> (if provided), <code>.summary</code>, and identifier mappings.</p> Example <p>To import data from Proteome Discoverer:     <pre><code>obs_columns = ['Sample', 'condition', 'cell_line']\npdata = import_proteomeDiscoverer(\n    prot_file='my_project/proteins.txt',\n    pep_file='my_project/peptides.txt',\n    obs_columns=obs_columns\n)\n</code></pre></p> Note <ul> <li>If <code>pep_file</code> is omitted, the resulting <code>pAnnData</code> will not include <code>.pep</code> or an RS matrix.</li> <li>If filename structure is inconsistent and <code>obs_columns</code> cannot be inferred, fallback columns are used.</li> </ul> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>@classmethod\ndef import_proteomeDiscoverer(cls, *args, **kwargs):\n    \"\"\"\n    Import Proteome Discoverer (PD) output into a `pAnnData` object.\n\n    This is a convenience wrapper for `import_data(source_type='pd')`. It loads protein- and optionally peptide-level\n    expression data from PD report files and parses sample metadata columns.\n\n    Args:\n        prot_file (str): Path to the protein-level report file (required).\n        pep_file (str, optional): Path to the peptide-level report file (optional but recommended).\n        obs_columns (list of str): List of columns to extract for `.obs`. These should match metadata tokens\n            embedded in the filenames (e.g. sample, condition, replicate).\n        **kwargs: Additional keyword arguments passed to `import_data()`.\n\n    Returns:\n        pAnnData: A populated object with `.prot`, `.pep` (if provided), `.summary`, and identifier mappings.\n\n    Example:\n        To import data from Proteome Discoverer:\n            ```python\n            obs_columns = ['Sample', 'condition', 'cell_line']\n            pdata = import_proteomeDiscoverer(\n                prot_file='my_project/proteins.txt',\n                pep_file='my_project/peptides.txt',\n                obs_columns=obs_columns\n            )\n            ```\n\n    Note:\n        - If `pep_file` is omitted, the resulting `pAnnData` will not include `.pep` or an RS matrix.\n        - If filename structure is inconsistent and `obs_columns` cannot be inferred, fallback columns are used.\n    \"\"\"\n    return import_proteomeDiscoverer(*args, **kwargs)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.IOMixin.suggest_obs_columns","title":"suggest_obs_columns  <code>classmethod</code>","text":"<pre><code>suggest_obs_columns(*args, **kwargs)\n</code></pre> <p>Suggest <code>.obs</code> column names based on parsed sample names.</p> <p>This function analyzes filenames or run names extracted from Proteome Discoverer or DIA-NN reports and attempts to identify consistent metadata fields. These fields may include <code>gradient</code>, <code>amount</code>, <code>cell_line</code>, or <code>well_position</code>, depending on naming conventions and regular expression matches.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str or Path</code> <p>Path to a DIA-NN or PD output file.</p> required <code>source_type</code> <code>str</code> <p>Type of the input file. Supports <code>'diann'</code> or <code>'pd'</code>. If not provided, inferred from filename or fallback heuristics.</p> required <code>filenames</code> <code>list of str</code> <p>List of sample file names or run labels to parse. If provided, bypasses file loading.</p> required <code>delimiter</code> <code>str</code> <p>Delimiter to use for tokenizing filenames (e.g., <code>','</code>, <code>'_'</code>). If not specified, will be inferred automatically.</p> required <p>Returns:</p> Type Description <p>list of str: Suggested list of metadata column names to assign to <code>.obs</code>.</p> Example <p>To suggest observation columns from a file:     <pre><code>suggest_obs_columns(\"my_experiment_PD.txt\", source_type=\"pd\")\n</code></pre></p> <pre><code>```\n# Suggested columns: ['Sample', 'gradient', 'cell_line', 'duration']\n['Sample', 'gradient', 'cell_line', 'duration']\n```\n</code></pre> Note <p>This function is typically used as part of the <code>.import_data()</code> flow when filenames embed experimental metadata.</p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>@classmethod\ndef suggest_obs_columns(cls, *args, **kwargs):\n    \"\"\"\n    Suggest `.obs` column names based on parsed sample names.\n\n    This function analyzes filenames or run names extracted from Proteome Discoverer\n    or DIA-NN reports and attempts to identify consistent metadata fields. These fields\n    may include `gradient`, `amount`, `cell_line`, or `well_position`, depending on\n    naming conventions and regular expression matches.\n\n    Args:\n        source (str or Path, optional): Path to a DIA-NN or PD output file.\n        source_type (str, optional): Type of the input file. Supports `'diann'` or `'pd'`.\n            If not provided, inferred from filename or fallback heuristics.\n        filenames (list of str, optional): List of sample file names or run labels to parse.\n            If provided, bypasses file loading.\n        delimiter (str, optional): Delimiter to use for tokenizing filenames (e.g., `','`, `'_'`).\n            If not specified, will be inferred automatically.\n\n    Returns:\n        list of str: Suggested list of metadata column names to assign to `.obs`.\n\n    Example:\n        To suggest observation columns from a file:\n            ```python\n            suggest_obs_columns(\"my_experiment_PD.txt\", source_type=\"pd\")\n            ```\n\n            ```\n            # Suggested columns: ['Sample', 'gradient', 'cell_line', 'duration']\n            ['Sample', 'gradient', 'cell_line', 'duration']\n            ```\n\n    Note:\n        This function is typically used as part of the `.import_data()` flow\n        when filenames embed experimental metadata.\n    \"\"\"\n    return suggest_obs_columns(*args, **kwargs)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.analyze_filename_formats","title":"analyze_filename_formats","text":"<pre><code>analyze_filename_formats(filenames, delimiter: str = '_', group_labels=None)\n</code></pre> <p>Analyze filename structures to detect format consistency.</p> <p>This function checks if all filenames can be split into the same number of tokens using the provided delimiter. It can optionally group files by token count and assign custom group labels.</p> <p>Parameters:</p> Name Type Description Default <code>filenames</code> <code>list of str</code> <p>List of sample or file names.</p> required <code>delimiter</code> <code>str</code> <p>Delimiter used to split each filename (default: \"_\").</p> <code>'_'</code> <code>group_labels</code> <code>list of str</code> <p>Optional group labels to assign to each unique token length group.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Format information containing: - 'uniform': True if all filenames split into the same number of tokens. - 'n_tokens': List of token counts for each filename. - 'group_map': Mapping of filename to group label (if labels are provided).</p> Example <p>Check if filenames have a uniform structure:     <pre><code>filenames = [\"A_60min_KD\", \"B_60min_SC\", \"C_120min_KD\"]\nanalyze_filename_formats(filenames)\n</code></pre> <pre><code>{\n    'uniform': True,\n    'n_tokens': [3, 3, 3],\n    'group_map': {}\n}\n</code></pre></p> <p>With group labels:     <pre><code>analyze_filename_formats(filenames, group_labels=[\"Group1\"])\n</code></pre> <pre><code>{\n    'uniform': True,\n    'n_tokens': [3, 3, 3],\n    'group_map': {\n        'A_60min_KD': 'Group1',\n        'B_60min_SC': 'Group1',\n        'C_120min_KD': 'Group1'\n    }\n}\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def analyze_filename_formats(filenames, delimiter: str = \"_\", group_labels=None):\n    \"\"\"\n    Analyze filename structures to detect format consistency.\n\n    This function checks if all filenames can be split into the same number of tokens\n    using the provided delimiter. It can optionally group files by token count and assign\n    custom group labels.\n\n    Args:\n        filenames (list of str): List of sample or file names.\n        delimiter (str): Delimiter used to split each filename (default: \"_\").\n        group_labels (list of str, optional): Optional group labels to assign to each unique token length group.\n\n    Returns:\n        dict: Format information containing:\n            - 'uniform': True if all filenames split into the same number of tokens.\n            - 'n_tokens': List of token counts for each filename.\n            - 'group_map': Mapping of filename to group label (if labels are provided).\n\n    Example:\n        Check if filenames have a uniform structure:\n            ```python\n            filenames = [\"A_60min_KD\", \"B_60min_SC\", \"C_120min_KD\"]\n            analyze_filename_formats(filenames)\n            ```\n            ```\n            {\n                'uniform': True,\n                'n_tokens': [3, 3, 3],\n                'group_map': {}\n            }\n            ```\n\n        With group labels:\n            ```python\n            analyze_filename_formats(filenames, group_labels=[\"Group1\"])\n            ```\n            ```\n            {\n                'uniform': True,\n                'n_tokens': [3, 3, 3],\n                'group_map': {\n                    'A_60min_KD': 'Group1',\n                    'B_60min_SC': 'Group1',\n                    'C_120min_KD': 'Group1'\n                }\n            }\n            ```\n    \"\"\"\n    group_counts = defaultdict(list)\n    for fname in filenames:\n        tokens = fname.split(delimiter)\n        group_counts[len(tokens)].append(fname)\n\n    token_lengths = list(group_counts.keys())\n    uniform = len(token_lengths) == 1\n\n    if group_labels is None:\n        group_labels = [f\"{n}-tokens\" for n in token_lengths]\n\n    group_map = {}\n    for label, n_tok in zip(group_labels, token_lengths):\n        for fname in group_counts[n_tok]:\n            group_map[fname] = label\n\n    return {\n        \"uniform\": uniform,\n        \"n_tokens\": token_lengths,\n        \"group_map\": group_map\n    }\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.classify_subtokens","title":"classify_subtokens","text":"<pre><code>classify_subtokens(token, used_labels=None, keyword_map=None)\n</code></pre> <p>Classify a token into one or more metadata categories based on keyword or pattern matching.</p> <p>This function splits a token (e.g. from a filename) into subtokens using character-type transitions  (e.g., \"Aur60minDIA\" \u2192 \"Aur\", \"60\", \"min\", \"DIA\"), then attempts to classify each subtoken using:</p> <ul> <li>Regex patterns (e.g., dates, well positions like A01)</li> <li>Fuzzy substring matching via a user-defined or default keyword map</li> </ul> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>str</code> <p>The input string to classify (e.g., \"Aur60minDIA\").</p> required <code>used_labels</code> <code>set</code> <p>Reserved for future logic to avoid assigning the same label twice.</p> <code>None</code> <code>keyword_map</code> <code>dict</code> <p>A dictionary of metadata categories (e.g., 'gradient') to example substrings.</p> <code>None</code> <p>Returns:</p> Type Description <p>list of str: A list of predicted metadata labels for the token (e.g., ['gradient', 'acquisition']).          If no match is found, returns ['unknown??'].</p> Example <p>Classify a gradient+time token:     <pre><code>classify_subtokens(\"Aur60minDIA\")\n</code></pre> <pre><code>['gradient', 'acquisition']\n</code></pre></p> <p>Classify a well position:     <pre><code>classify_subtokens(\"B07\")\n</code></pre> <pre><code>['well_position']\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def classify_subtokens(token, used_labels=None, keyword_map=None):\n    \"\"\"\n    Classify a token into one or more metadata categories based on keyword or pattern matching.\n\n    This function splits a token (e.g. from a filename) into subtokens using character-type transitions \n    (e.g., \"Aur60minDIA\" \u2192 \"Aur\", \"60\", \"min\", \"DIA\"), then attempts to classify each subtoken using:\n\n    - Regex patterns (e.g., dates, well positions like A01)\n    - Fuzzy substring matching via a user-defined or default keyword map\n\n    Args:\n        token (str): The input string to classify (e.g., \"Aur60minDIA\").\n        used_labels (set, optional): Reserved for future logic to avoid assigning the same label twice.\n        keyword_map (dict, optional): A dictionary of metadata categories (e.g., 'gradient') to example substrings.\n\n    Returns:\n        list of str: A list of predicted metadata labels for the token (e.g., ['gradient', 'acquisition']).\n                     If no match is found, returns ['unknown??'].\n\n    Example:\n        Classify a gradient+time token:\n            ```python\n            classify_subtokens(\"Aur60minDIA\")\n            ```\n            ```\n            ['gradient', 'acquisition']\n            ```\n\n        Classify a well position:\n            ```python\n            classify_subtokens(\"B07\")\n            ```\n            ```\n            ['well_position']\n            ```\n    \"\"\"\n\n    default_map = {\n        \"gradient\": [\"min\", \"hr\", \"gradient\", \"short\", \"long\", \"fast\", \"slow\"],\n        \"amount\": [\"cell\", \"cells\", \"sc\", \"bulk\", \"ng\", \"ug\", \"pg\", \"fmol\"],\n        \"enzyme\": [\"trypsin\", \"lysC\", \"chymotrypsin\", \"gluc\", \"tryp\", \"lys-c\", \"glu-c\"],\n        \"condition\": [\"ctrl\", \"stim\", \"wt\", \"ko\", \"kd\", \"scramble\", \"si\", \"drug\"],\n        \"sample_type\": [\"embryo\", \"brain\", \"liver\", \"cellline\", \"mix\", \"qc\"],\n        \"instrument\": [\"tims\", \"tof\", \"fusion\", \"exploris\",\"astral\",\"stellar\",\"eclipse\",\"OA\",\"OE480\",\"OE\",\"QE\",\"qexecutive\",\"OTE\"],\n        \"acquisition\": [\"dia\", \"prm\", \"dda\", \"srm\"],\n        \"column\": ['TS25','TS15','TS8','Aur'],\n        \"organism\": [\"human\", \"mouse\", \"mus\", \"homo\", \"drosophila\", \"musculus\", \"sapiens\"]\n    }\n\n    keyword_map = keyword_map or default_map\n    labels = set()\n\n    # Split into subtokens (case preserved), in case one token has multiple labels\n    subtokens = re.findall(r'[A-Za-z]+|\\d+min|\\d+(?:ng|ug|pg|fmol)|\\d{6,8}', token)\n\n    for sub in subtokens:\n        # Check unmodified for regex-based rules\n        if is_date_like(sub):\n            labels.add(\"date\")\n        elif re.match(r\"[A-Ha-h]\\d{1,2}$\", sub):\n            labels.add(\"well_position\")\n        else:\n            # Lowercase for keyword matches\n            sub_lower = sub.lower()\n            for label, keywords in keyword_map.items():\n                if any(kw in sub_lower for kw in keywords):\n                    labels.add(label)\n\n    if not labels:\n        labels.add(\"unknown??\")\n    return list(labels)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.get_filenames","title":"get_filenames","text":"<pre><code>get_filenames(source: Union[str, Path], source_type: str) -&gt; List[str]\n</code></pre> <p>Extract sample filenames from a DIA-NN or Proteome Discoverer report.</p> <p>For DIA-NN reports, this extracts the 'Run' column from the table. For Proteome Discoverer (PD) output, it collects unique sample identifiers  based on column headers (e.g. abundance columns like \"Abundances (SampleX)\").</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str or Path</code> <p>Path to the input report file.</p> required <code>source_type</code> <code>str</code> <p>Tool used to generate the report. Must be one of {'diann', 'pd'}.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>list of str: Extracted list of sample names or run filenames.</p> Example <p>Extract DIA-NN run names:     <pre><code>get_filenames(\"diann_output.tsv\", source_type=\"diann\")\n</code></pre> <pre><code>['Sample1.raw', 'Sample2.raw', 'Sample3.raw']\n</code></pre></p> <p>Extract PD sample names from abundance columns:     <pre><code>get_filenames(\"pd_output.xlsx\", source_type=\"pd\")\n</code></pre> <pre><code>['SampleA', 'SampleB', 'SampleC']\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def get_filenames(source: Union[str, Path], source_type: str) -&gt; List[str]:\n    \"\"\"\n    Extract sample filenames from a DIA-NN or Proteome Discoverer report.\n\n    For DIA-NN reports, this extracts the 'Run' column from the table.\n    For Proteome Discoverer (PD) output, it collects unique sample identifiers \n    based on column headers (e.g. abundance columns like \"Abundances (SampleX)\").\n\n    Args:\n        source (str or Path): Path to the input report file.\n        source_type (str): Tool used to generate the report. Must be one of {'diann', 'pd'}.\n\n    Returns:\n        list of str: Extracted list of sample names or run filenames.\n\n    Example:\n        Extract DIA-NN run names:\n            ```python\n            get_filenames(\"diann_output.tsv\", source_type=\"diann\")\n            ```\n            ```\n            ['Sample1.raw', 'Sample2.raw', 'Sample3.raw']\n            ```\n\n        Extract PD sample names from abundance columns:\n            ```python\n            get_filenames(\"pd_output.xlsx\", source_type=\"pd\")\n            ```\n            ```\n            ['SampleA', 'SampleB', 'SampleC']\n            ```\n    \"\"\"\n    source = Path(source)\n    ext = source.suffix.lower()\n\n    # --- DIA-NN ---\n    if source_type == \"diann\":\n        if ext in [\".csv\", \".tsv\"]:\n            df = pd.read_csv(source, sep=\"\\t\" if ext == \".tsv\" else \",\", usecols=[\"Run\"], low_memory=False)\n        elif ext == \".parquet\":\n            df = pd.read_parquet(source, columns=[\"Run\"], engine=\"pyarrow\")\n        else:\n            raise ValueError(f\"Unsupported file type for DIA-NN: {ext}\")\n\n        filenames = df[\"Run\"].dropna().unique().tolist()\n\n    # --- Proteome Discoverer ---\n    elif source_type == \"pd\":\n        if ext in [\".txt\", \".tsv\"]:\n            df = pd.read_csv(source, sep=\"\\t\", nrows=0)\n        elif ext == \".xlsx\":\n            df = pd.read_excel(source, nrows=0)\n        else:\n            raise ValueError(f\"Unsupported file type for PD: {ext}\")\n\n        abundance_cols = [col for col in df.columns if re.search(r\"Abundance: F\\d+: \", col)]\n        if not abundance_cols:\n            raise ValueError(\"No 'Abundance: F#:' columns found in PD file.\")\n\n        filenames = []\n        for col in abundance_cols:\n            match = re.match(r\"Abundance: F\\d+: (.+)\", col)\n            if match:\n                filenames.append(match.group(1).strip())\n\n    else:\n        raise ValueError(\"source_type must be 'pd' or 'diann'\")\n\n    return filenames\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.import_data","title":"import_data","text":"<pre><code>import_data(source_type: str, **kwargs)\n</code></pre> <p>Unified wrapper for importing data into a <code>pAnnData</code> object.</p> <p>This function routes to a specific import handler based on the <code>source_type</code>, such as Proteome Discoverer or DIA-NN. It parses protein/peptide expression data and associated sample metadata, returning a fully initialized <code>pAnnData</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>source_type</code> <code>str</code> <p>The input tool or data source. Supported values:</p> <ul> <li> <p><code>'pd'</code>, <code>'proteomeDiscoverer'</code>, <code>'pd13'</code>, <code>'pd24'</code>:   \u2192 Uses <code>import_proteomeDiscoverer()</code>.   Required kwargs:</p> <ul> <li><code>prot_file</code> (str): Path to protein-level report file</li> <li><code>obs_columns</code> (list of str): Columns to extract for <code>.obs</code>   Optional kwargs:</li> <li><code>pep_file</code> (str): Path to peptide-level report file</li> </ul> </li> <li> <p><code>'diann'</code>, <code>'dia-nn'</code>:   \u2192 Uses <code>import_diann()</code>.   Required kwargs:</p> <ul> <li><code>report_file</code> (str): Path to DIA-NN report file</li> <li><code>obs_columns</code> (list of str): Columns to extract for <code>.obs</code></li> </ul> </li> <li> <p><code>'fragpipe'</code>, <code>'fp'</code>: Not yet implemented  </p> </li> <li><code>'spectronaut'</code>, <code>'sn'</code>: Not yet implemented</li> </ul> required <code>**kwargs</code> <p>Additional keyword arguments forwarded to the relevant import function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>A populated pAnnData object with <code>.prot</code>, <code>.pep</code>, <code>.summary</code>, and identifier mappings.</p> Example <p>Importing Proteome Discoverer output for single-cell data:     <pre><code>obs_columns = ['Sample', 'method', 'duration', 'cell_line']\npdata_untreated_sc = import_data(\n    source_type='pd',\n    prot_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_prot_Proteins.txt',\n    pep_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_pep_PeptideGroups.txt',\n    obs_columns=obs_columns\n)\n</code></pre></p> <p>Importing PD output for bulk data from an Excel file:     <pre><code>obs_columns = ['Sample', 'cell_line']\npdata_bulk = import_data(\n    source_type='pd',\n    prot_file='HCT116 resistance_20230601_pdoutput.xlsx',\n    obs_columns=obs_columns\n)\n</code></pre></p> Note <p>If <code>obs_columns</code> is not provided and filename formats are inconsistent, fallback parsing is applied with generic columns (<code>\"File\"</code>, <code>\"parsingType\"</code>).</p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def import_data(source_type: str, **kwargs):\n    \"\"\"\n    Unified wrapper for importing data into a `pAnnData` object.\n\n    This function routes to a specific import handler based on the `source_type`,\n    such as Proteome Discoverer or DIA-NN. It parses protein/peptide expression data\n    and associated sample metadata, returning a fully initialized `pAnnData` object.\n\n    Args:\n        source_type (str): The input tool or data source. Supported values:\n\n            - `'pd'`, `'proteomeDiscoverer'`, `'pd13'`, `'pd24'`:  \n              \u2192 Uses `import_proteomeDiscoverer()`.  \n              Required kwargs:\n                - `prot_file` (str): Path to protein-level report file\n                - `obs_columns` (list of str): Columns to extract for `.obs`\n              Optional kwargs:\n                - `pep_file` (str): Path to peptide-level report file\n\n            - `'diann'`, `'dia-nn'`:  \n              \u2192 Uses `import_diann()`.  \n              Required kwargs:\n                - `report_file` (str): Path to DIA-NN report file\n                - `obs_columns` (list of str): Columns to extract for `.obs`\n\n            - `'fragpipe'`, `'fp'`: Not yet implemented  \n            - `'spectronaut'`, `'sn'`: Not yet implemented\n\n        **kwargs: Additional keyword arguments forwarded to the relevant import function.\n\n    Returns:\n        pAnnData: A populated pAnnData object with `.prot`, `.pep`, `.summary`, and identifier mappings.\n\n    Example:\n        Importing Proteome Discoverer output for single-cell data:\n            ```python\n            obs_columns = ['Sample', 'method', 'duration', 'cell_line']\n            pdata_untreated_sc = import_data(\n                source_type='pd',\n                prot_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_prot_Proteins.txt',\n                pep_file='data/202312_untreated/Marion_20231218_OTE_Aur60min_CBR_pep_PeptideGroups.txt',\n                obs_columns=obs_columns\n            )\n            ```\n\n        Importing PD output for bulk data from an Excel file:\n            ```python\n            obs_columns = ['Sample', 'cell_line']\n            pdata_bulk = import_data(\n                source_type='pd',\n                prot_file='HCT116 resistance_20230601_pdoutput.xlsx',\n                obs_columns=obs_columns\n            )\n            ```\n\n    Note:\n        If `obs_columns` is not provided and filename formats are inconsistent,\n        fallback parsing is applied with generic columns (`\"File\"`, `\"parsingType\"`).\n    \"\"\"\n\n    print(f\"{format_log_prefix('user')} Importing data of type [{source_type}]\")\n\n    source_type = source_type.lower()\n    obs_columns = kwargs.get('obs_columns', None)\n    if obs_columns is None:\n        source = kwargs.get('report_file') if 'report_file' in kwargs else kwargs.get('prot_file')\n        delimiter = kwargs.get('delimiter') if 'delimiter' in kwargs else None\n        format_info, fallback_columns, fallback_obs = resolve_obs_columns(source, source_type, delimiter=delimiter)\n\n        if format_info[\"uniform\"]:\n            # Prompt user to rerun with obs_columns\n            return None\n        else:\n            # non-uniform format, use fallback obs\n            kwargs[\"obs_columns\"] = fallback_columns\n            kwargs[\"obs\"] = fallback_obs\n\n    if source_type in ['diann', 'dia-nn']:\n        return _import_diann(**kwargs)\n\n    elif source_type in ['pd', 'proteomediscoverer', 'proteome_discoverer', 'pd2.5', 'pd24']:\n        return _import_proteomeDiscoverer(**kwargs)\n\n    elif source_type in ['fragpipe', 'fp']:\n        raise NotImplementedError(\"FragPipe import is not yet implemented. Stay tuned!\")\n\n    elif source_type in ['spectronaut', 'sn']:\n        raise NotImplementedError(\"Spectronaut import is not yet implemented. Stay tuned!\")\n\n    else:\n        raise ValueError(f\"{format_log_prefix('error')} Unsupported import source: '{source_type}'. \"\n                         \"Valid options: 'diann', 'proteomeDiscoverer', 'fragpipe', 'spectronaut'.\")\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.import_diann","title":"import_diann","text":"<pre><code>import_diann(report_file: Optional[str] = None, obs_columns: Optional[List[str]] = None, delimiter: Optional[str] = '_', prot_value: str = 'PG.MaxLFQ', pep_value: str = 'Precursor.Normalised', prot_var_columns: List[str] = ['Genes', 'Master.Protein'], pep_var_columns: List[str] = ['Genes', 'Protein.Group', 'Precursor.Charge', 'Modified.Sequence', 'Stripped.Sequence', 'Precursor.Id', 'All Mapped Proteins', 'All Mapped Genes'], **kwargs)\n</code></pre> <p>Import DIA-NN output into a <code>pAnnData</code> object.</p> <p>This function parses a DIA-NN report file and separates protein- and peptide-level expression matrices using the specified abundance and metadata columns.</p> <p>Parameters:</p> Name Type Description Default <code>report_file</code> <code>str</code> <p>Path to the DIA-NN report file (required).</p> <code>None</code> <code>obs_columns</code> <code>list of str</code> <p>List of metadata columns to extract from the filename for <code>.obs</code>.</p> <code>None</code> <code>delimiter</code> <code>str</code> <p>Character to split file names by to set up metadata in obs.</p> <code>'_'</code> <code>prot_value</code> <code>str</code> <p>Column name in DIA-NN output to use for protein quantification. Default: <code>'PG.MaxLFQ'</code>.</p> <code>'PG.MaxLFQ'</code> <code>pep_value</code> <code>str</code> <p>Column name in DIA-NN output to use for peptide quantification. Default: <code>'Precursor.Normalised'</code>.</p> <code>'Precursor.Normalised'</code> <code>prot_var_columns</code> <code>list of str</code> <p>Columns from the protein group table to store in <code>.prot.var</code>. Default includes gene and master protein annotations.</p> <code>['Genes', 'Master.Protein']</code> <code>pep_var_columns</code> <code>list of str</code> <p>Columns from the precursor table to store in <code>.pep.var</code>. Default includes peptide sequence, precursor ID, and mapping annotations.</p> <code>['Genes', 'Protein.Group', 'Precursor.Charge', 'Modified.Sequence', 'Stripped.Sequence', 'Precursor.Id', 'All Mapped Proteins', 'All Mapped Genes']</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>import_data()</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>A populated object with <code>.prot</code>, <code>.pep</code>, <code>.summary</code>, and identifier mappings.</p> Example <p>To import data from a DIA-NN report file:     <pre><code>obs_columns = ['Sample', 'treatment', 'replicate']\npdata = import_diann(\n    report_file='data/project_diaNN_output.tsv',\n    obs_columns=obs_columns,\n    prot_value='PG.MaxLFQ',\n    pep_value='Precursor.Normalised'\n)\n</code></pre></p> Note <ul> <li>DIA-NN report should contain both protein group and precursor-level information.</li> <li>Metadata columns in filenames must be consistently formatted to extract <code>.obs</code>.</li> </ul> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def import_diann(report_file: Optional[str] = None, obs_columns: Optional[List[str]] = None, delimiter: Optional[str] = '_', prot_value: str = 'PG.MaxLFQ', pep_value: str = 'Precursor.Normalised', prot_var_columns: List[str] = ['Genes', 'Master.Protein'], pep_var_columns: List[str] = ['Genes', 'Protein.Group', 'Precursor.Charge', 'Modified.Sequence', 'Stripped.Sequence', 'Precursor.Id', 'All Mapped Proteins', 'All Mapped Genes'], **kwargs):\n    \"\"\"\n    Import DIA-NN output into a `pAnnData` object.\n\n    This function parses a DIA-NN report file and separates protein- and peptide-level expression matrices\n    using the specified abundance and metadata columns.\n\n    Args:\n        report_file (str): Path to the DIA-NN report file (required).\n        obs_columns (list of str): List of metadata columns to extract from the filename for `.obs`.\n        delimiter (str): Character to split file names by to set up metadata in obs.\n        prot_value (str): Column name in DIA-NN output to use for protein quantification.\n            Default: `'PG.MaxLFQ'`.\n        pep_value (str): Column name in DIA-NN output to use for peptide quantification.\n            Default: `'Precursor.Normalised'`.\n        prot_var_columns (list of str): Columns from the protein group table to store in `.prot.var`.\n            Default includes gene and master protein annotations.\n        pep_var_columns (list of str): Columns from the precursor table to store in `.pep.var`.\n            Default includes peptide sequence, precursor ID, and mapping annotations.\n        **kwargs: Additional keyword arguments passed to `import_data()`.\n\n    Returns:\n        pAnnData: A populated object with `.prot`, `.pep`, `.summary`, and identifier mappings.\n\n    Example:\n        To import data from a DIA-NN report file:\n            ```python\n            obs_columns = ['Sample', 'treatment', 'replicate']\n            pdata = import_diann(\n                report_file='data/project_diaNN_output.tsv',\n                obs_columns=obs_columns,\n                prot_value='PG.MaxLFQ',\n                pep_value='Precursor.Normalised'\n            )\n            ```\n\n    Note:\n        - DIA-NN report should contain both protein group and precursor-level information.\n        - Metadata columns in filenames must be consistently formatted to extract `.obs`.\n    \"\"\"\n    return import_data(source_type='diann', report_file=report_file, obs_columns=obs_columns, delimiter = delimiter, prot_value=prot_value, pep_value=pep_value, prot_var_columns=prot_var_columns, pep_var_columns=pep_var_columns, **kwargs)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.import_proteomeDiscoverer","title":"import_proteomeDiscoverer","text":"<pre><code>import_proteomeDiscoverer(prot_file: Optional[str] = None, pep_file: Optional[str] = None, obs_columns: Optional[List[str]] = ['sample'], **kwargs)\n</code></pre> <p>Import Proteome Discoverer (PD) output into a <code>pAnnData</code> object.</p> <p>This is a convenience wrapper for <code>import_data(source_type='pd')</code>. It loads protein- and optionally peptide-level expression data from PD report files and parses sample metadata columns.</p> <p>Parameters:</p> Name Type Description Default <code>prot_file</code> <code>str</code> <p>Path to the protein-level report file (required).</p> <code>None</code> <code>pep_file</code> <code>str</code> <p>Path to the peptide-level report file (optional but recommended).</p> <code>None</code> <code>obs_columns</code> <code>list of str</code> <p>List of columns to extract for <code>.obs</code>. These should match metadata tokens embedded in the filenames (e.g. sample, condition, replicate).</p> <code>['sample']</code> <code>**kwargs</code> <p>Additional keyword arguments passed to <code>import_data()</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pAnnData</code> <p>A populated object with <code>.prot</code>, <code>.pep</code> (if provided), <code>.summary</code>, and identifier mappings.</p> Example <p>To import data from Proteome Discoverer:     <pre><code>obs_columns = ['Sample', 'condition', 'cell_line']\npdata = import_proteomeDiscoverer(\n    prot_file='my_project/proteins.txt',\n    pep_file='my_project/peptides.txt',\n    obs_columns=obs_columns\n)\n</code></pre></p> Note <ul> <li>If <code>pep_file</code> is omitted, the resulting <code>pAnnData</code> will not include <code>.pep</code> or an RS matrix.</li> <li>If filename structure is inconsistent and <code>obs_columns</code> cannot be inferred, fallback columns are used.</li> </ul> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def import_proteomeDiscoverer(prot_file: Optional[str] = None, pep_file: Optional[str] = None, obs_columns: Optional[List[str]] = ['sample'], **kwargs):\n    \"\"\"\n    Import Proteome Discoverer (PD) output into a `pAnnData` object.\n\n    This is a convenience wrapper for `import_data(source_type='pd')`. It loads protein- and optionally peptide-level\n    expression data from PD report files and parses sample metadata columns.\n\n    Args:\n        prot_file (str): Path to the protein-level report file (required).\n        pep_file (str, optional): Path to the peptide-level report file (optional but recommended).\n        obs_columns (list of str): List of columns to extract for `.obs`. These should match metadata tokens\n            embedded in the filenames (e.g. sample, condition, replicate).\n        **kwargs: Additional keyword arguments passed to `import_data()`.\n\n    Returns:\n        pAnnData: A populated object with `.prot`, `.pep` (if provided), `.summary`, and identifier mappings.\n\n    Example:\n        To import data from Proteome Discoverer:\n            ```python\n            obs_columns = ['Sample', 'condition', 'cell_line']\n            pdata = import_proteomeDiscoverer(\n                prot_file='my_project/proteins.txt',\n                pep_file='my_project/peptides.txt',\n                obs_columns=obs_columns\n            )\n            ```\n\n    Note:\n        - If `pep_file` is omitted, the resulting `pAnnData` will not include `.pep` or an RS matrix.\n        - If filename structure is inconsistent and `obs_columns` cannot be inferred, fallback columns are used.\n    \"\"\"\n    return import_data(source_type='pd', prot_file=prot_file, pep_file=pep_file, obs_columns=obs_columns)\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.resolve_obs_columns","title":"resolve_obs_columns","text":"<pre><code>resolve_obs_columns(source: str, source_type: str, delimiter: Optional[str] = None) -&gt; Tuple[Dict[str, Any], Optional[List[str]], Optional[pd.DataFrame]]\n</code></pre> <p>Resolve observation columns from sample filenames or metadata fields.</p> <p>This function attempts to infer sample-level metadata (<code>.obs</code>) from filenames or a report file (DIA-NN or Proteome Discoverer). It classifies tokens using  regex patterns and known metadata heuristics.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to the report file (DIA-NN or PD).</p> required <code>source_type</code> <code>str</code> <p>Source type \u2014 one of {'diann', 'pd'}.</p> required <code>delimiter</code> <code>str</code> <p>Delimiter used to split filename tokens. If None, auto-inferred.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Tuple[dict, list[str] or None, pd.DataFrame or None]: A tuple of:</p> <code>Optional[List[str]]</code> <ul> <li>metadata (dict): Metadata extracted during parsing, including fallback flags.</li> </ul> <code>Optional[DataFrame]</code> <ul> <li>suggested_obs (list of str or None): Suggested observation column names, or None if inconsistent format.</li> </ul> <code>Tuple[Dict[str, Any], Optional[List[str]], Optional[DataFrame]]</code> <ul> <li>obs_df (pd.DataFrame or None): Parsed observation DataFrame.</li> </ul> Note <p>If filename formats are inconsistent across samples, the fallback <code>.obs</code> will include: - A generic 'File' column with raw filenames - A 'parsingType' column indicating parsing structure</p> Example <p>Inferring observation columns from a PD file:     <pre><code>resolve_obs_columns('filepaths/pd_report.xlsx', source_type='pd')\n</code></pre></p> <p>Inferring from a DIA-NN report with custom delimiter:     <pre><code>resolve_obs_columns('filepaths/diann.tsv', source_type='diann', delimiter='_')\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def resolve_obs_columns(source: str, source_type: str, delimiter: Optional[str] = None) -&gt; Tuple[Dict[str, Any], Optional[List[str]], Optional[pd.DataFrame]]:\n    \"\"\"\n    Resolve observation columns from sample filenames or metadata fields.\n\n    This function attempts to infer sample-level metadata (`.obs`) from filenames\n    or a report file (DIA-NN or Proteome Discoverer). It classifies tokens using \n    regex patterns and known metadata heuristics.\n\n    Args:\n        source (str): Path to the report file (DIA-NN or PD).\n        source_type (str): Source type \u2014 one of {'diann', 'pd'}.\n        delimiter (str, optional): Delimiter used to split filename tokens. If None, auto-inferred.\n\n    Returns:\n        Tuple[dict, list[str] or None, pd.DataFrame or None]: A tuple of:\n\n        - **metadata** (dict): Metadata extracted during parsing, including fallback flags.\n        - **suggested_obs** (list of str or None): Suggested observation column names, or None if inconsistent format.\n        - **obs_df** (pd.DataFrame or None): Parsed observation DataFrame.\n\n    Note:\n        If filename formats are inconsistent across samples, the fallback `.obs` will include:\n        - A generic 'File' column with raw filenames\n        - A 'parsingType' column indicating parsing structure\n\n    Example:\n        Inferring observation columns from a PD file:\n            ```python\n            resolve_obs_columns('filepaths/pd_report.xlsx', source_type='pd')\n            ```\n\n        Inferring from a DIA-NN report with custom delimiter:\n            ```python\n            resolve_obs_columns('filepaths/diann.tsv', source_type='diann', delimiter='_')\n            ```\n    \"\"\"\n\n    filenames = get_filenames(source, source_type=source_type)\n    if not filenames:\n        raise ValueError(f\"{format_log_prefix('error')} No sample filenames could be extracted from the provided source: {source}.\")\n\n    if delimiter is None:\n        first_fname = filenames[0]\n        all_delims = re.findall(r'[^A-Za-z0-9]', first_fname)\n        delimiter = Counter(all_delims).most_common(1)[0][0] if all_delims else '_'\n        print(f\"      Auto-detecting '{delimiter}' as delimiter from first filename.\")\n\n    format_info = analyze_filename_formats(filenames, delimiter=delimiter)\n\n    if format_info[\"uniform\"]:\n        # Uniform format \u2014 suggest obs_columns using classification\n        print(f\"{format_log_prefix('info_only')} Filenames are uniform. Using `suggest_obs_columns()` to recommend obs_columns...\")\n        obs_columns = suggest_obs_columns(filenames=filenames, source_type=source_type, delimiter=delimiter)\n        print(f\"{format_log_prefix('warn')} Please review the suggested `obs_columns` above.\")\n        print(\"   \u2192 If acceptable, rerun `import_data(..., obs_columns=...)` with this list.\\n\")\n        return format_info, obs_columns, None\n    else:\n        # Non-uniform format \u2014 return fallback DataFrame\n        print(f\"{format_log_prefix('warn',indent=2)} {len(format_info['n_tokens'])} different filename formats detected. Proceeding with fallback `.obs` structure... (File Number, Parsing Type)\")\n\n        obs = pd.DataFrame({\n            \"File\": list(range(1, len(filenames) + 1)),\n            \"parsingType\": [format_info['group_map'][fname] for fname in filenames]\n        })\n        obs_columns = [\"File\", \"parsingType\"]\n        return format_info, obs_columns, obs\n</code></pre>"},{"location":"reference/pAnnData/import_mixin/#src.scpviz.pAnnData.io.suggest_obs_columns","title":"suggest_obs_columns","text":"<pre><code>suggest_obs_columns(source=None, source_type=None, filenames=None, delimiter=None)\n</code></pre> <p>Suggest <code>.obs</code> column names based on parsed sample names.</p> <p>This function analyzes filenames or run names extracted from Proteome Discoverer or DIA-NN reports and attempts to identify consistent metadata fields. These fields may include <code>gradient</code>, <code>amount</code>, <code>cell_line</code>, or <code>well_position</code>, depending on naming conventions and regular expression matches.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str or Path</code> <p>Path to a DIA-NN or PD output file.</p> <code>None</code> <code>source_type</code> <code>str</code> <p>Type of the input file. Supports <code>'diann'</code> or <code>'pd'</code>. If not provided, inferred from filename or fallback heuristics.</p> <code>None</code> <code>filenames</code> <code>list of str</code> <p>List of sample file names or run labels to parse. If provided, bypasses file loading.</p> <code>None</code> <code>delimiter</code> <code>str</code> <p>Delimiter to use for tokenizing filenames (e.g., <code>','</code>, <code>'_'</code>). If not specified, will be inferred automatically.</p> <code>None</code> <p>Returns:</p> Type Description <p>list of str: Suggested list of metadata column names to assign to <code>.obs</code>.</p> Example <p>To suggest observation columns from a file:     <pre><code>suggest_obs_columns(\"my_experiment_PD.txt\", source_type=\"pd\")\n</code></pre></p> <p>Suggested columns: ['Sample', 'gradient', 'cell_line', 'duration']     <pre><code>['Sample', 'gradient', 'cell_line', 'duration']\n</code></pre></p> Note <p>This function is typically used as part of the <code>.import_data()</code> flow when filenames embed experimental metadata.</p> Source code in <code>src/scpviz/pAnnData/io.py</code> <pre><code>def suggest_obs_columns(source=None, source_type=None, filenames=None, delimiter=None):\n    \"\"\"\n    Suggest `.obs` column names based on parsed sample names.\n\n    This function analyzes filenames or run names extracted from Proteome Discoverer\n    or DIA-NN reports and attempts to identify consistent metadata fields. These fields\n    may include `gradient`, `amount`, `cell_line`, or `well_position`, depending on\n    naming conventions and regular expression matches.\n\n    Args:\n        source (str or Path, optional): Path to a DIA-NN or PD output file.\n        source_type (str, optional): Type of the input file. Supports `'diann'` or `'pd'`.\n            If not provided, inferred from filename or fallback heuristics.\n        filenames (list of str, optional): List of sample file names or run labels to parse.\n            If provided, bypasses file loading.\n        delimiter (str, optional): Delimiter to use for tokenizing filenames (e.g., `','`, `'_'`).\n            If not specified, will be inferred automatically.\n\n    Returns:\n        list of str: Suggested list of metadata column names to assign to `.obs`.\n\n    Example:\n        To suggest observation columns from a file:\n            ```python\n            suggest_obs_columns(\"my_experiment_PD.txt\", source_type=\"pd\")\n            ```\n\n        Suggested columns: ['Sample', 'gradient', 'cell_line', 'duration']\n            ```python\n            ['Sample', 'gradient', 'cell_line', 'duration']\n            ```\n\n    Note:\n        This function is typically used as part of the `.import_data()` flow\n        when filenames embed experimental metadata.\n    \"\"\"\n    from pathlib import Path\n    import csv\n    from collections import Counter\n\n    if filenames is None:\n        if source is None or source_type is None:\n            raise ValueError(\"If `filenames` is not provided, both `source` and `source_type` must be specified.\")\n        source = Path(source)\n        filenames = get_filenames(source, source_type=source_type)\n\n    if not filenames:\n        raise ValueError(\"No sample filenames could be extracted from the provided source.\")\n\n    # Pick the first filename for token analysis\n    fname = filenames[0]\n\n    # Infer delimiter if not provided\n    if delimiter is None:\n        all_delims = re.findall(r'[^A-Za-z0-9]', fname)\n        delimiter = Counter(all_delims).most_common(1)[0][0] if all_delims else '_'\n        print(f\"Auto-detecting '{delimiter}' as delimiter.\")\n\n    if source_type == 'pd':\n        # Custom comma-based parsing for PD\n        match = re.match(r'Abundance: (F\\d+): (.+)', f\"Abundance: F1: {fname}\")\n        if match:\n            _, meta = match.groups()\n            raw_tokens = [t.strip() for t in meta.split(',') if t.strip().lower() != 'n/a']\n            fname = ', '.join(raw_tokens)  # for clean display later\n            tokens = raw_tokens\n            delimiter = ','\n        else:\n            raise ValueError(f\"Could not parse metadata from PD filename: {fname}\")\n\n    else:\n        # --- Generic tokenization for DIA-NN or other formats ---\n        tokens = [t.strip() for t in fname.split(delimiter) if t.strip()]\n\n    # --- Classify tokens ---\n    suggestion = {}\n    obs_columns = []\n    token_label_map = []\n    multi_matched_tokens = []\n    unrecognized_tokens = []\n\n    for tok in tokens:\n        labels = classify_subtokens(tok)\n        label = labels[0]\n        if label == \"unknown??\":\n            obs_columns.append(f\"&lt;{tok}?&gt;\")\n        else:\n            obs_columns.append(label)\n        token_label_map.append((tok, labels))\n        if label != \"unknown??\" and label not in suggestion:\n            suggestion[label] = tok\n        if \"unknown??\" in labels:\n            unrecognized_tokens.append(tok)\n        elif len(labels) &gt; 1:\n            multi_matched_tokens.append((labels, tok))\n\n    # --- Print suggestions ---\n    print(f\"\\nFrom filename: {fname}\")\n    print(\"Suggested .obs columns:\")\n    for tok, labels in token_label_map:\n        print(f\"  {' OR '.join(labels):&lt;26}: {tok}\")\n    if multi_matched_tokens:\n        print(f\"\\nMultiple matched token(s): {[t for _, t in multi_matched_tokens]}\")\n    if unrecognized_tokens:\n        print(f\"Unrecognized token(s): {unrecognized_tokens}\")\n    if multi_matched_tokens or unrecognized_tokens:\n        print(\"Please manually label these.\")\n\n    print(f\"\\n{format_log_prefix('info_only')} Suggested obs:\\nobs_columns = {obs_columns}\")\n\n    return obs_columns\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/","title":"Metadata &amp; Mappings","text":"<p>Mixins for computing and updating metadata from <code>.obs</code>, <code>.var</code>, or relational data.</p>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.summary","title":"summary","text":""},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.summary.SummaryMixin","title":"SummaryMixin","text":"<p>Handles creation, synchronization, and metric updates for the <code>.summary</code> attribute.</p> <p>This mixin maintains a unified sample-level summary table by merging <code>.prot.obs</code> and <code>.pep.obs</code>, with automatic flagging and update mechanisms to track when recomputation or syncing is needed.</p> <p>Features:</p> <ul> <li>Merges sample-level metadata from <code>.prot.obs</code> and <code>.pep.obs</code> into <code>.summary</code></li> <li>Tracks when <code>.summary</code> becomes out of sync via <code>_summary_is_stale</code></li> <li>Supports recomputing per-sample metrics and syncing edits back to <code>.obs</code></li> <li>Enables passive refresh of <code>.summary</code> after filtering or manual editing</li> </ul> <p>Methods:</p> Name Description <code>update_summary</code> <p>Rebuild and optionally recompute or sync <code>.summary</code></p> <code>_update_summary</code> <p>Legacy alias for <code>update_summary()</code></p> <code>_merge_obs</code> <p>Internal merge logic for <code>.prot.obs</code> and <code>.pep.obs</code></p> <code>_push_summary_to_obs</code> <p>Sync edited <code>.summary</code> values back into <code>.obs</code></p> <code>_mark_summary_stale</code> <p>Mark the summary as stale for downstream tracking</p> Source code in <code>src/scpviz/pAnnData/summary.py</code> <pre><code>class SummaryMixin:\n    \"\"\"\n    Handles creation, synchronization, and metric updates for the `.summary` attribute.\n\n    This mixin maintains a unified sample-level summary table by merging `.prot.obs` and `.pep.obs`,\n    with automatic flagging and update mechanisms to track when recomputation or syncing is needed.\n\n    Features:\n\n    - Merges sample-level metadata from `.prot.obs` and `.pep.obs` into `.summary`\n    - Tracks when `.summary` becomes out of sync via `_summary_is_stale`\n    - Supports recomputing per-sample metrics and syncing edits back to `.obs`\n    - Enables passive refresh of `.summary` after filtering or manual editing\n\n    Functions:\n        update_summary: Rebuild and optionally recompute or sync `.summary`\n        _update_summary: Legacy alias for `update_summary()`\n        _merge_obs: Internal merge logic for `.prot.obs` and `.pep.obs`\n        _push_summary_to_obs: Sync edited `.summary` values back into `.obs`\n        _mark_summary_stale: Mark the summary as stale for downstream tracking\n    \"\"\"\n    def update_summary(self, recompute=True, sync_back=False, verbose=True):\n        \"\"\"\n        Update the `.summary` DataFrame to reflect current state of `.obs` and metadata.\n\n        This function ensures `.summary` stays synchronized with sample-level metadata\n        stored in `.prot.obs` / `.pep.obs`. You can choose to recompute metrics,\n        sync edits back to `.obs`, or simply refresh the merged view.\n\n        Args:\n            recompute (bool): If True, re-calculate protein/peptide stats.\n            sync_back (bool): If True, push edited `.summary` values back to `.prot.obs` / `.pep.obs`.\n                False by default, as `.summary` is derived.\n            verbose (bool): If True, print action messages.\n\n        ??? example \"Typical Usage Scenarios\"\n            | Scenario                        | Call                                | recompute | sync_back | _summary_is_stale | Effect                                                           |\n            |---------------------------------|-------------------------------------|-----------|-----------|-------------------|------------------------------------------------------------------|\n            | Filtering `.prot` or `.pep`     | `.update_summary(recompute=True)`   | \u2705        | \u274c        | \u274c                | Recalculate protein/peptide stats and merge into `.summary`.     |\n            | Filtering samples               | `.update_summary(recompute=False)`  | \u274c        | \u274c        | \u274c                | Refresh `.summary` view of `.obs` without recomputation.         |\n            | Manual `.summary[...] = ...`    | `.update_summary()`                 | \u2705/\u274c     | \u2705        | \u2705                | Push edited `.summary` values back to `.obs`.                    |\n            | After setting `.summary = ...`  | `.update_summary()`                 | \u2705        | \u2705        | \u2705                | Sync back and recompute stats from new `.summary`.               |\n            | No changes                      | `.update_summary()`                 | \u274c        | \u274c        | \u274c                | No-op other than passive re-merge.                               |\n\n        Note:\n            - For most typical use cases, we auto-detect which flags need to be applied.\n                You usually don\u2019t need to set `recompute` or `sync_back` manually.\n            - `recompute=True` triggers `_update_metrics()` from `.prot` / `.pep` data.\n            - `sync_back=True` ensures changes to `.summary` are reflected in `.obs`.\n            - `.summary_is_stale` is automatically set when `.summary` is edited directly\n            (e.g. via `TrackedDataFrame`) or when assigned via the setter.\n        \"\"\"\n\n        # 1. Push back first if summary was edited by the user\n        if sync_back or getattr(self, \"_summary_is_stale\", False):\n            updated_prot, updated_pep = self._push_summary_to_obs()\n            updated_cols = list(set(updated_prot + updated_pep))\n            updated_str = f\" Columns updated: {', '.join(updated_cols)}.\" if updated_cols else \"\"\n\n            if verbose:\n                reason = \" (marked stale)\" if not sync_back else \"\"\n                print(f\"{format_log_prefix('update',indent=1)} Updating summary [sync_back]: pushed edits from `.summary` to `.obs`{reason}.\\n{format_log_prefix('blank',indent=2)}{updated_str}\")\n\n            self._summary_is_stale = False  # reset before recompute\n\n        # 2. Recompute or re-merge afterward\n        if recompute:\n            self._update_metrics() # type: ignore #, in MetricsMixin\n        self._merge_obs()\n        self._update_summary_metrics() # type: ignore #, in MetricsMixin\n        self.refresh_identifier_maps() # type: ignore #, in IdentifierMixin\n\n        # 3. Final messaging\n        if verbose and not (sync_back or self._summary_is_stale):\n            if recompute:\n                print(f\"{format_log_prefix('update',indent=3)} Updating summary [recompute]: Recomputed metrics and refreshed `.summary` from `.obs`.\")\n            else:\n                print(f\"{format_log_prefix('update',indent=3)} Updating summary [refresh]: Refreshed `.summary` view (no recompute).\")\n\n        # 4. Final cleanup\n        self._summary_is_stale = False\n\n    def _update_summary(self):\n        \"\"\"\n        Legacy method for updating the `.summary` table.\n\n        This method is retained for backward compatibility and simply calls the newer\n        `update_summary()` function with default arguments:\n        `recompute=True`, `sync_back=False`, and `verbose=False`.\n\n        Note:\n            This method is deprecated and may be removed in a future version.\n            Use `update_summary()` instead.\n        \"\"\"\n        print(\"\u26a0\ufe0f  Legacy _update_summary() called \u2014 consider switching to update_summary()\")\n        self.update_summary(recompute=True, sync_back=False, verbose=False)\n\n    def _merge_obs(self):\n        \"\"\"\n        Merge `.prot.obs` and `.pep.obs` into a unified sample-level summary.\n\n        This function combines metadata from protein-level and peptide-level `.obs` tables\n        into a single summary DataFrame. Shared columns (e.g., 'gradient', 'condition') \n        are taken from `.prot.obs` by default if present in both.\n\n        Returns:\n            pandas.DataFrame: Merged observation metadata for all samples.\n        \"\"\"\n        if self.prot is not None:\n            summary = self.prot.obs.copy()\n            if self.pep is not None:\n                for col in self.pep.obs.columns:\n                    if col not in summary.columns:\n                        summary[col] = self.pep.obs[col]\n        elif self.pep is not None:\n            summary = self.pep.obs.copy()\n        else:\n            summary = pd.DataFrame()\n\n\n        self._summary = TrackedDataFrame(\n            summary, parent=self, mark_stale_fn=self._mark_summary_stale)\n        self._previous_summary = summary.copy()\n\n    def _push_summary_to_obs(self, skip_if_contains='pep', verbose=False):\n        \"\"\"\n        Push changes from `.summary` back into `.prot.obs` and `.pep.obs`.\n\n        This function updates `.prot.obs` and `.pep.obs` with any modified columns\n        in `.summary`. To avoid overwriting incompatible fields, columns containing\n        `skip_if_contains` are excluded when updating `.prot.obs`, and similarly,\n        columns containing 'prot' are excluded when updating `.pep.obs`.\n\n        Args:\n            skip_if_contains (str): Substring used to skip incompatible columns for `.prot.obs`.\n                                    Defaults to 'pep'.\n            verbose (bool): If True, print updates being pushed to `.obs`.\n\n        Note:\n            This is typically called internally by `update_summary(sync_back=True)`.\n        \"\"\"\n        if not self._has_data():\n            return\n\n        def update_obs_with_summary(obs, summary, skip_if_contains):\n            skipped, updated = [], []\n            for col in summary.columns:\n                if skip_if_contains in str(col):\n                    skipped.append(col)\n                    continue\n                if col not in obs.columns or not obs[col].equals(summary[col]):\n                    updated.append(col)\n                obs[col] = summary[col]\n            return skipped, updated\n\n        if self.prot is not None:\n            if not self.prot.obs.index.equals(self._summary.index):\n                raise ValueError(\"Mismatch: .summary and .prot.obs have different sample indices.\")\n            skipped_prot, updated_prot = update_obs_with_summary(self.prot.obs, self._summary, skip_if_contains)\n        else:\n            skipped_prot, updated_prot = None, []\n\n        if self.pep is not None:\n            if not self.pep.obs.index.equals(self._summary.index):\n                raise ValueError(\"Mismatch: .summary and .pep.obs have different sample indices.\")\n            skipped_pep, updated_pep = update_obs_with_summary(self.pep.obs, self._summary, skip_if_contains='prot')\n        else:\n            skipped_pep, updated_pep = None, []\n\n        msg = \"Pushed summary values back to obs. \"\n        if skipped_prot:\n            msg += f\"Skipped for prot: {', '.join(skipped_prot)}. \"\n        if skipped_pep:\n            msg += f\"Skipped for pep: {', '.join(skipped_pep)}. \"\n\n        self._append_history(msg)\n        if verbose:\n            print(msg)\n\n        return updated_prot, updated_pep\n\n    def _mark_summary_stale(self):\n        \"\"\"\n        Mark the `.summary` as stale.\n\n        This sets the `_summary_is_stale` flag to True, indicating that the\n        summary is out of sync with `.obs` or metrics and should be updated\n        using `update_summary()`.\n\n        Note:\n            This is typically triggered automatically when `.summary` is edited\n            (e.g., via `TrackedDataFrame`) or reassigned.\n        \"\"\"\n        self._summary_is_stale = True\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.summary.SummaryMixin.update_summary","title":"update_summary","text":"<pre><code>update_summary(recompute=True, sync_back=False, verbose=True)\n</code></pre> <p>Update the <code>.summary</code> DataFrame to reflect current state of <code>.obs</code> and metadata.</p> <p>This function ensures <code>.summary</code> stays synchronized with sample-level metadata stored in <code>.prot.obs</code> / <code>.pep.obs</code>. You can choose to recompute metrics, sync edits back to <code>.obs</code>, or simply refresh the merged view.</p> <p>Parameters:</p> Name Type Description Default <code>recompute</code> <code>bool</code> <p>If True, re-calculate protein/peptide stats.</p> <code>True</code> <code>sync_back</code> <code>bool</code> <p>If True, push edited <code>.summary</code> values back to <code>.prot.obs</code> / <code>.pep.obs</code>. False by default, as <code>.summary</code> is derived.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print action messages.</p> <code>True</code> Typical Usage Scenarios Scenario Call recompute sync_back _summary_is_stale Effect Filtering <code>.prot</code> or <code>.pep</code> <code>.update_summary(recompute=True)</code> \u2705 \u274c \u274c Recalculate protein/peptide stats and merge into <code>.summary</code>. Filtering samples <code>.update_summary(recompute=False)</code> \u274c \u274c \u274c Refresh <code>.summary</code> view of <code>.obs</code> without recomputation. Manual <code>.summary[...] = ...</code> <code>.update_summary()</code> \u2705/\u274c \u2705 \u2705 Push edited <code>.summary</code> values back to <code>.obs</code>. After setting <code>.summary = ...</code> <code>.update_summary()</code> \u2705 \u2705 \u2705 Sync back and recompute stats from new <code>.summary</code>. No changes <code>.update_summary()</code> \u274c \u274c \u274c No-op other than passive re-merge. Note <ul> <li>For most typical use cases, we auto-detect which flags need to be applied.     You usually don\u2019t need to set <code>recompute</code> or <code>sync_back</code> manually.</li> <li><code>recompute=True</code> triggers <code>_update_metrics()</code> from <code>.prot</code> / <code>.pep</code> data.</li> <li><code>sync_back=True</code> ensures changes to <code>.summary</code> are reflected in <code>.obs</code>.</li> <li><code>.summary_is_stale</code> is automatically set when <code>.summary</code> is edited directly (e.g. via <code>TrackedDataFrame</code>) or when assigned via the setter.</li> </ul> Source code in <code>src/scpviz/pAnnData/summary.py</code> <pre><code>def update_summary(self, recompute=True, sync_back=False, verbose=True):\n    \"\"\"\n    Update the `.summary` DataFrame to reflect current state of `.obs` and metadata.\n\n    This function ensures `.summary` stays synchronized with sample-level metadata\n    stored in `.prot.obs` / `.pep.obs`. You can choose to recompute metrics,\n    sync edits back to `.obs`, or simply refresh the merged view.\n\n    Args:\n        recompute (bool): If True, re-calculate protein/peptide stats.\n        sync_back (bool): If True, push edited `.summary` values back to `.prot.obs` / `.pep.obs`.\n            False by default, as `.summary` is derived.\n        verbose (bool): If True, print action messages.\n\n    ??? example \"Typical Usage Scenarios\"\n        | Scenario                        | Call                                | recompute | sync_back | _summary_is_stale | Effect                                                           |\n        |---------------------------------|-------------------------------------|-----------|-----------|-------------------|------------------------------------------------------------------|\n        | Filtering `.prot` or `.pep`     | `.update_summary(recompute=True)`   | \u2705        | \u274c        | \u274c                | Recalculate protein/peptide stats and merge into `.summary`.     |\n        | Filtering samples               | `.update_summary(recompute=False)`  | \u274c        | \u274c        | \u274c                | Refresh `.summary` view of `.obs` without recomputation.         |\n        | Manual `.summary[...] = ...`    | `.update_summary()`                 | \u2705/\u274c     | \u2705        | \u2705                | Push edited `.summary` values back to `.obs`.                    |\n        | After setting `.summary = ...`  | `.update_summary()`                 | \u2705        | \u2705        | \u2705                | Sync back and recompute stats from new `.summary`.               |\n        | No changes                      | `.update_summary()`                 | \u274c        | \u274c        | \u274c                | No-op other than passive re-merge.                               |\n\n    Note:\n        - For most typical use cases, we auto-detect which flags need to be applied.\n            You usually don\u2019t need to set `recompute` or `sync_back` manually.\n        - `recompute=True` triggers `_update_metrics()` from `.prot` / `.pep` data.\n        - `sync_back=True` ensures changes to `.summary` are reflected in `.obs`.\n        - `.summary_is_stale` is automatically set when `.summary` is edited directly\n        (e.g. via `TrackedDataFrame`) or when assigned via the setter.\n    \"\"\"\n\n    # 1. Push back first if summary was edited by the user\n    if sync_back or getattr(self, \"_summary_is_stale\", False):\n        updated_prot, updated_pep = self._push_summary_to_obs()\n        updated_cols = list(set(updated_prot + updated_pep))\n        updated_str = f\" Columns updated: {', '.join(updated_cols)}.\" if updated_cols else \"\"\n\n        if verbose:\n            reason = \" (marked stale)\" if not sync_back else \"\"\n            print(f\"{format_log_prefix('update',indent=1)} Updating summary [sync_back]: pushed edits from `.summary` to `.obs`{reason}.\\n{format_log_prefix('blank',indent=2)}{updated_str}\")\n\n        self._summary_is_stale = False  # reset before recompute\n\n    # 2. Recompute or re-merge afterward\n    if recompute:\n        self._update_metrics() # type: ignore #, in MetricsMixin\n    self._merge_obs()\n    self._update_summary_metrics() # type: ignore #, in MetricsMixin\n    self.refresh_identifier_maps() # type: ignore #, in IdentifierMixin\n\n    # 3. Final messaging\n    if verbose and not (sync_back or self._summary_is_stale):\n        if recompute:\n            print(f\"{format_log_prefix('update',indent=3)} Updating summary [recompute]: Recomputed metrics and refreshed `.summary` from `.obs`.\")\n        else:\n            print(f\"{format_log_prefix('update',indent=3)} Updating summary [refresh]: Refreshed `.summary` view (no recompute).\")\n\n    # 4. Final cleanup\n    self._summary_is_stale = False\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.identifier","title":"identifier","text":""},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin","title":"IdentifierMixin","text":"<p>Handles mapping between genes, accessions, and peptides.</p> <p>This mixin provides utilities for:</p> <ul> <li> <p>Building and caching bidirectional mappings:</p> <ul> <li>For proteins: gene \u2194 accession  </li> <li>For peptides: peptide \u2194 protein accession</li> </ul> </li> <li> <p>Updating or refreshing identifier maps manually or via UniProt</p> </li> <li>Automatically filling in missing gene names using the UniProt API</li> </ul> <p>These mappings are cached and used throughout the <code>pAnnData</code> object to support resolution of user queries and consistent gene-accession-peptide tracking.</p> <p>Methods:</p> Name Description <code>_build_identifier_maps</code> <p>Create forward/reverse maps based on protein or peptide data</p> <code>refresh_identifier_maps</code> <p>Clear cached mappings to force rebuild</p> <code>get_identifier_maps</code> <p>Retrieve (gene \u2192 acc, acc \u2192 gene) or (peptide \u2194 protein) maps</p> <code>update_identifier_maps</code> <p>Add or overwrite mappings (e.g., manual corrections)</p> <code>update_missing_genes</code> <p>Fill missing gene names using the UniProt API</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>class IdentifierMixin:\n    \"\"\"\n    Handles mapping between genes, accessions, and peptides.\n\n    This mixin provides utilities for:\n\n    - Building and caching bidirectional mappings:\n        * For proteins: gene \u2194 accession  \n        * For peptides: peptide \u2194 protein accession\n\n    - Updating or refreshing identifier maps manually or via UniProt\n    - Automatically filling in missing gene names using the UniProt API\n\n    These mappings are cached and used throughout the `pAnnData` object to support resolution of user queries and consistent gene-accession-peptide tracking.\n\n    Functions:\n        _build_identifier_maps: Create forward/reverse maps based on protein or peptide data\n        refresh_identifier_maps: Clear cached mappings to force rebuild\n        get_identifier_maps: Retrieve (gene \u2192 acc, acc \u2192 gene) or (peptide \u2194 protein) maps\n        update_identifier_maps: Add or overwrite mappings (e.g., manual corrections)\n        update_missing_genes: Fill missing gene names using the UniProt API\n    \"\"\"\n\n    def _build_identifier_maps(self, adata, gene_col=\"Genes\"):\n        \"\"\"\n        Build bidirectional identifier mappings for genes/proteins or peptides/proteins.\n\n        Depending on whether `adata` is `.prot` or `.pep`, this builds:\n\n        - For proteins: gene \u2194 accession\n        - For peptides: peptide \u2194 protein accession\n\n        Args:\n            adata (AnnData): Either `self.prot` or `self.pep`.\n            gene_col (str): Column name in `.var` containing gene names (default: \"Genes\").\n\n        Returns:\n            tuple: A pair of dictionaries (`forward`, `reverse`) for identifier lookup.\n\n        Note:\n            For peptides, mapping relies on `utils.get_pep_prot_mapping()` to resolve protein accessions.\n\n        Raises:\n            Warning if peptide-to-protein mapping cannot be built.\n        \"\"\"\n        from pandas import notna\n\n        forward = {}\n        reverse = {}\n\n        if adata is self.prot:\n            if gene_col in adata.var.columns:\n                for acc, gene in zip(adata.var_names, adata.var[gene_col]):\n                    if notna(gene):\n                        gene = str(gene)\n                        forward[gene] = acc\n                        reverse[acc] = gene\n\n        elif adata is self.pep:\n            try:\n                prot_acc_col = utils.get_pep_prot_mapping(self)\n                pep_to_prot = adata.var[prot_acc_col]\n                for pep, prot in zip(adata.var_names, pep_to_prot):\n                    if notna(prot):\n                        forward[prot] = pep\n                        reverse[pep] = prot\n            except Exception as e:\n                warnings.warn(f\"Could not build peptide-to-protein map: {e}\")\n\n        return forward, reverse\n\n    def refresh_identifier_maps(self):\n        \"\"\"\n        Clear cached identifier maps to force regeneration on next access.\n\n        This removes the following attributes if present:\n\n        - `_gene_maps_protein`: Gene \u2194 Accession map for proteins\n        - `_protein_maps_peptide`: Protein \u2194 Peptide map for peptides\n\n        Useful when `.var` annotations are updated and identifier mappings may have changed.\n        \"\"\"\n        for attr in [\"_gene_maps_protein\", \"_protein_maps_peptide\"]:\n            if hasattr(self, attr):\n                delattr(self, attr)\n\n    def get_identifier_maps(self, on='protein'):\n        \"\"\"\n        Retrieve gene/accession or peptide/protein mapping dictionaries.\n\n        Depending on the `on` argument, returns a tuple of forward and reverse mappings:\n\n        - If `on='protein'`: (gene \u2192 accession, accession \u2192 gene)\n\n        - If `on='peptide'`: (protein accession \u2192 peptide, peptide \u2192 protein accession)\n\n        Note: Alias `get_gene_maps()` also calls this function for compatibility.\n\n        Args:\n            on (str): Source of mapping. Must be `'protein'` or `'peptide'`.\n\n        Returns:\n            Tuple[dict, dict]: (forward mapping, reverse mapping)\n\n        Raises:\n            ValueError: If `on` is not `'protein'` or `'peptide'`.\n        \"\"\"\n        if on in ('protein','prot'):\n            return self._cached_identifier_maps_protein\n        elif on in ('peptide','pep'):\n            return self._cached_identifier_maps_peptide\n        else:\n            raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n    # TODO: add peptide remapping to var, but need to also update rs if you do this.\n    def update_identifier_maps(self, mapping, on='protein', direction='forward', overwrite=False, verbose=True):\n        \"\"\"\n        Update cached identifier maps with user-supplied mappings.\n\n        This function updates the internal forward and reverse identifier maps\n        for either proteins or peptides. Ensures consistency by updating both\n        directions of the mapping.\n\n        - For `'protein'`:\n            * forward: gene \u2192 accession  \n            * reverse: accession \u2192 gene\n\n        - For `'peptide'`:\n            * forward: protein accession \u2192 peptide\n            * reverse: peptide \u2192 protein accession\n\n        Args:\n            mapping (dict): Dictionary of mappings to add.\n            on (str): Which maps to update. Must be `'protein'` or `'peptide'`.\n            direction (str): `'forward'` or `'reverse'` \u2014 determines how the `mapping` should be interpreted.\n            overwrite (bool): If True, allows overwriting existing entries.\n            verbose (bool): If True, prints a summary of updated keys.\n\n        Note:\n            The corresponding reverse map is automatically updated to maintain bidirectional consistency.\n\n        Example:\n            Add new gene-to-accession mappings (protein):\n                ```python\n                pdata.update_identifier_maps(\n                    {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n                    on='protein',\n                    direction='forward'\n                )\n                ```\n\n            Add peptide \u2192 protein mappings:\n                ```python\n                pdata.update_identifier_maps(\n                    {'PEPTIDE_ABC': 'P12345'},\n                    on='peptide',\n                    direction='reverse'\n                )\n                ```\n\n            Overwrite a protein \u2192 gene mapping:\n                ```python\n                pdata.update_identifier_maps(\n                    {'P12345': 'NEWGENE'},\n                    on='protein',\n                    direction='reverse',\n                    overwrite=True\n                )\n                ```\n\n        \"\"\"\n        if on == 'protein':\n            forward, reverse = self._cached_identifier_maps_protein\n        elif on == 'peptide':\n            forward, reverse = self._cached_identifier_maps_peptide\n        else:\n            raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n        source_map = forward if direction == 'forward' else reverse\n        target_map = reverse if direction == 'forward' else forward\n\n        added, updated, skipped = 0, 0, 0\n\n        for key, val in mapping.items():\n            if key in source_map:\n                if overwrite:\n                    source_map[key] = val\n                    target_map[val] = key\n                    updated += 1\n                else:\n                    skipped += 1\n            else:\n                source_map[key] = val\n                target_map[val] = key\n                added += 1\n\n        message = (\n            f\"[update_identifier_maps] Updated '{on}' ({direction}): \"\n            f\"{added} added, {updated} overwritten, {skipped} skipped.\"\n        )\n\n        if verbose:\n            print(message)\n        self._append_history(message)\n\n        # Update .prot.var[\"Genes\"] if updating protein identifier reverse map (accession \u2192 gene)\n        if on == 'protein' and direction == 'reverse':\n            updated_var_count = 0\n            updated_accessions = []\n\n            for acc, gene in mapping.items():\n                if acc in self.prot.var_names:\n                    self.prot.var.at[acc, \"Genes\"] = gene\n                    updated_accessions.append(acc)\n                    updated_var_count += 1\n\n            if updated_var_count &gt; 0:\n                var_message = (\n                    f\"\ud83d\udd01 Updated `.prot.var['Genes']` for {updated_var_count} entries from custom mapping. \"\n                    f\"(View details in `pdata.metadata['identifier_map_history']`)\"\n                )\n                if verbose:\n                    print(var_message)\n                self._append_history(var_message)\n\n        # Log detailed update history for all cases\n        import datetime\n\n        record = {\n            'on': on,\n            'direction': direction,\n            'input_mapping': dict(mapping),  # shallow copy\n            'overwrite': overwrite,\n            'timestamp': datetime.datetime.now().isoformat(timespec='seconds'),\n            'summary': {\n                'added': added,\n                'updated': updated,\n                'skipped': skipped,\n            }\n        }\n\n        if on == 'protein' and direction == 'reverse':\n            record['updated_var_column'] = {\n                'column': 'Genes',\n                'accessions': updated_accessions,\n                'n_updated': updated_var_count\n            }\n\n        self.metadata.setdefault(\"identifier_map_history\", []).append(record)\n\n    get_gene_maps = get_identifier_maps\n\n    def update_missing_genes(self, gene_col=\"Genes\", verbose=True):\n        \"\"\"\n        Fill missing gene names in `.prot.var` using UniProt API.\n\n        This function searches for missing values in the specified gene column\n        and attempts to fill them by querying the UniProt API using protein\n        accession IDs. If a gene name cannot be found, a placeholder\n        'UNKNOWN_&lt;accession&gt;' is used instead.\n\n        Args:\n            gene_col (str): Column name in `.prot.var` to update (default: \"Genes\").\n            verbose (bool): Whether to print summary information (default: True).\n\n        Returns:\n            None\n\n        Note:\n            - This function only operates on `.prot.var`, not `.pep.var`.\n            - If UniProt is unavailable or returns no match, the missing entry is filled as `'UNKNOWN_&lt;accession&gt;'`.\n            - To manually correct unknown entries later, use `update_identifier_maps()` with `direction='reverse'`.\n\n        Example:\n            Automatically fill missing gene names using UniProt:\n                ```python\n                pdata.update_missing_genes()\n                ```\n        \"\"\"\n        var = self.prot.var\n\n        if gene_col not in var.columns:\n            if verbose:\n                print(f\"{format_log_prefix('warn')} Column '{gene_col}' not found in .prot.var.\")\n            return\n\n        missing_mask = var[gene_col].isna()\n        if not missing_mask.any():\n            if verbose:\n                print(f\"{format_log_prefix('result')} No missing gene names found.\")\n            return\n\n        accessions = var.index[missing_mask].tolist()\n        if verbose:\n            print(f\"{format_log_prefix('info_only')} {len(accessions)} proteins with missing gene names.\")\n\n        try:\n            df = utils.get_uniprot_fields(\n                accessions,\n                search_fields=[\"accession\", \"gene_primary\"],\n                standardize=True\n            )\n        except Exception as e:\n            print(f\"{format_log_prefix('error')} UniProt query failed: {e}\")\n            return\n        df = utils.standardize_uniprot_columns(df)\n\n        if df.empty or \"accession\" not in df.columns or \"gene_primary\" not in df.columns:\n            print(f\"{format_log_prefix('warn')} UniProt returned no usable gene mapping columns.\")\n            return\n\n        gene_map = dict(zip(df[\"accession\"], df[\"gene_primary\"]))\n        filled = self.prot.var.loc[missing_mask].index.map(lambda acc: gene_map.get(acc))\n        final_genes = [\n            gene if pd.notna(gene) else f\"UNKNOWN_{acc}\"\n            for acc, gene in zip(self.prot.var.loc[missing_mask].index, filled)\n        ]\n        self.prot.var.loc[missing_mask, gene_col] = final_genes\n\n        found = sum(pd.notna(filled))\n        unknown = len(final_genes) - found\n        if verbose:\n            if found:\n                print(f\"{format_log_prefix('result')} Recovered {found} gene name(s) from UniProt. Genes found:\")\n                filled_clean = [str(g) for g in filled if pd.notna(g)]\n                preview = \", \".join(filled_clean[:10])\n                if found &gt; 10:\n                    preview += \"...\"\n                print(\"        \", preview)\n            if unknown:\n                missing_ids = self.prot.var.loc[missing_mask].index[pd.isna(filled)]\n                print(f\"{format_log_prefix('warn')} {unknown} gene name(s) still missing. Assigned as 'UNKNOWN_&lt;accession&gt;' for:\")\n                print(\"        \", \", \".join(missing_ids[:5]) + (\"...\" if unknown &gt; 10 else \"\"))\n                print(\"     \ud83d\udca1 Tip: You can update these using `pdata.update_identifier_maps({'GENE': 'ACCESSION'}, on='protein', direction='reverse', overwrite=True)`\\n\")\n\n    def search_annotations(self, query, on='protein', search_columns=None, case=False, return_all_matches=True):\n        \"\"\"\n        Search protein or peptide annotations for matching biological terms.\n\n        This function scans `.prot.var` or `.pep.var` for entries containing the provided keyword(s),\n        across common annotation fields.\n\n        Args:\n            query (str or list of str): Term(s) to search for (e.g., \"keratin\", \"KRT\").\n            on (str): Whether to search `\"protein\"` or `\"peptide\"` annotations (default: `\"protein\"`).\n            search_columns (list of str, optional): Columns to search in. Defaults to common biological fields.\n            case (bool): Case-sensitive search (default: False).\n            return_all_matches (bool): If True, return matches from any column. If False, returns only rows that match all terms.\n\n        Returns:\n            pd.DataFrame: Filtered dataframe with a `Matched` column (True/False) and optionally match columns per term.\n\n        Example:\n            ```python\n            pdata.search_annotations(\"keratin\")\n            pdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n            ```\n        \"\"\"\n        import pandas as pd\n\n        adata = self.prot if on == \"protein\" else self.pep\n        df = adata.var.copy()\n\n        if search_columns is None:\n            search_columns = [\n                \"Accession\", \"Description\", \"Biological Process\", \"Cellular Component\",\n                \"Molecular Function\", \"Genes\", \"Gene ID\", \"Reactome Pathways\"\n            ]\n\n        # Ensure index is available as a searchable column\n        df = df.copy()\n        df[\"Accession\"] = df.index.astype(str)\n\n        # Convert query to list\n        if isinstance(query, str):\n            query = [query]\n\n        # Search logic\n        def match_func(val, term):\n            if pd.isnull(val):\n                return False\n            return term in val if case else term.lower() in str(val).lower()\n\n        match_results = pd.DataFrame(index=df.index)\n\n        for term in query:\n            per_col_match = pd.DataFrame({\n                col: df[col].apply(match_func, args=(term,)) if col in df.columns else False\n                for col in search_columns\n            })\n            row_match = per_col_match.any(axis=1)\n            match_results[f\"Matched_{term}\"] = row_match\n\n        if return_all_matches:\n            matched_any = match_results.any(axis=1)\n        else:\n            matched_any = match_results.all(axis=1)\n\n        result_df = df.copy()\n        result_df[\"Matched\"] = matched_any\n        for col in match_results.columns:\n            result_df[col] = match_results[col]\n\n        return result_df[result_df[\"Matched\"]]\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.get_identifier_maps","title":"get_identifier_maps","text":"<pre><code>get_identifier_maps(on='protein')\n</code></pre> <p>Retrieve gene/accession or peptide/protein mapping dictionaries.</p> <p>Depending on the <code>on</code> argument, returns a tuple of forward and reverse mappings:</p> <ul> <li> <p>If <code>on='protein'</code>: (gene \u2192 accession, accession \u2192 gene)</p> </li> <li> <p>If <code>on='peptide'</code>: (protein accession \u2192 peptide, peptide \u2192 protein accession)</p> </li> </ul> <p>Note: Alias <code>get_gene_maps()</code> also calls this function for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <code>str</code> <p>Source of mapping. Must be <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <p>Returns:</p> Type Description <p>Tuple[dict, dict]: (forward mapping, reverse mapping)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>on</code> is not <code>'protein'</code> or <code>'peptide'</code>.</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def get_identifier_maps(self, on='protein'):\n    \"\"\"\n    Retrieve gene/accession or peptide/protein mapping dictionaries.\n\n    Depending on the `on` argument, returns a tuple of forward and reverse mappings:\n\n    - If `on='protein'`: (gene \u2192 accession, accession \u2192 gene)\n\n    - If `on='peptide'`: (protein accession \u2192 peptide, peptide \u2192 protein accession)\n\n    Note: Alias `get_gene_maps()` also calls this function for compatibility.\n\n    Args:\n        on (str): Source of mapping. Must be `'protein'` or `'peptide'`.\n\n    Returns:\n        Tuple[dict, dict]: (forward mapping, reverse mapping)\n\n    Raises:\n        ValueError: If `on` is not `'protein'` or `'peptide'`.\n    \"\"\"\n    if on in ('protein','prot'):\n        return self._cached_identifier_maps_protein\n    elif on in ('peptide','pep'):\n        return self._cached_identifier_maps_peptide\n    else:\n        raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.refresh_identifier_maps","title":"refresh_identifier_maps","text":"<pre><code>refresh_identifier_maps()\n</code></pre> <p>Clear cached identifier maps to force regeneration on next access.</p> <p>This removes the following attributes if present:</p> <ul> <li><code>_gene_maps_protein</code>: Gene \u2194 Accession map for proteins</li> <li><code>_protein_maps_peptide</code>: Protein \u2194 Peptide map for peptides</li> </ul> <p>Useful when <code>.var</code> annotations are updated and identifier mappings may have changed.</p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def refresh_identifier_maps(self):\n    \"\"\"\n    Clear cached identifier maps to force regeneration on next access.\n\n    This removes the following attributes if present:\n\n    - `_gene_maps_protein`: Gene \u2194 Accession map for proteins\n    - `_protein_maps_peptide`: Protein \u2194 Peptide map for peptides\n\n    Useful when `.var` annotations are updated and identifier mappings may have changed.\n    \"\"\"\n    for attr in [\"_gene_maps_protein\", \"_protein_maps_peptide\"]:\n        if hasattr(self, attr):\n            delattr(self, attr)\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.search_annotations","title":"search_annotations","text":"<pre><code>search_annotations(query, on='protein', search_columns=None, case=False, return_all_matches=True)\n</code></pre> <p>Search protein or peptide annotations for matching biological terms.</p> <p>This function scans <code>.prot.var</code> or <code>.pep.var</code> for entries containing the provided keyword(s), across common annotation fields.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str or list of str</code> <p>Term(s) to search for (e.g., \"keratin\", \"KRT\").</p> required <code>on</code> <code>str</code> <p>Whether to search <code>\"protein\"</code> or <code>\"peptide\"</code> annotations (default: <code>\"protein\"</code>).</p> <code>'protein'</code> <code>search_columns</code> <code>list of str</code> <p>Columns to search in. Defaults to common biological fields.</p> <code>None</code> <code>case</code> <code>bool</code> <p>Case-sensitive search (default: False).</p> <code>False</code> <code>return_all_matches</code> <code>bool</code> <p>If True, return matches from any column. If False, returns only rows that match all terms.</p> <code>True</code> <p>Returns:</p> Type Description <p>pd.DataFrame: Filtered dataframe with a <code>Matched</code> column (True/False) and optionally match columns per term.</p> Example <pre><code>pdata.search_annotations(\"keratin\")\npdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n</code></pre> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def search_annotations(self, query, on='protein', search_columns=None, case=False, return_all_matches=True):\n    \"\"\"\n    Search protein or peptide annotations for matching biological terms.\n\n    This function scans `.prot.var` or `.pep.var` for entries containing the provided keyword(s),\n    across common annotation fields.\n\n    Args:\n        query (str or list of str): Term(s) to search for (e.g., \"keratin\", \"KRT\").\n        on (str): Whether to search `\"protein\"` or `\"peptide\"` annotations (default: `\"protein\"`).\n        search_columns (list of str, optional): Columns to search in. Defaults to common biological fields.\n        case (bool): Case-sensitive search (default: False).\n        return_all_matches (bool): If True, return matches from any column. If False, returns only rows that match all terms.\n\n    Returns:\n        pd.DataFrame: Filtered dataframe with a `Matched` column (True/False) and optionally match columns per term.\n\n    Example:\n        ```python\n        pdata.search_annotations(\"keratin\")\n        pdata.search_annotations([\"keratin\", \"cytoskeleton\"], on=\"peptide\", case=False)\n        ```\n    \"\"\"\n    import pandas as pd\n\n    adata = self.prot if on == \"protein\" else self.pep\n    df = adata.var.copy()\n\n    if search_columns is None:\n        search_columns = [\n            \"Accession\", \"Description\", \"Biological Process\", \"Cellular Component\",\n            \"Molecular Function\", \"Genes\", \"Gene ID\", \"Reactome Pathways\"\n        ]\n\n    # Ensure index is available as a searchable column\n    df = df.copy()\n    df[\"Accession\"] = df.index.astype(str)\n\n    # Convert query to list\n    if isinstance(query, str):\n        query = [query]\n\n    # Search logic\n    def match_func(val, term):\n        if pd.isnull(val):\n            return False\n        return term in val if case else term.lower() in str(val).lower()\n\n    match_results = pd.DataFrame(index=df.index)\n\n    for term in query:\n        per_col_match = pd.DataFrame({\n            col: df[col].apply(match_func, args=(term,)) if col in df.columns else False\n            for col in search_columns\n        })\n        row_match = per_col_match.any(axis=1)\n        match_results[f\"Matched_{term}\"] = row_match\n\n    if return_all_matches:\n        matched_any = match_results.any(axis=1)\n    else:\n        matched_any = match_results.all(axis=1)\n\n    result_df = df.copy()\n    result_df[\"Matched\"] = matched_any\n    for col in match_results.columns:\n        result_df[col] = match_results[col]\n\n    return result_df[result_df[\"Matched\"]]\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.update_identifier_maps","title":"update_identifier_maps","text":"<pre><code>update_identifier_maps(mapping, on='protein', direction='forward', overwrite=False, verbose=True)\n</code></pre> <p>Update cached identifier maps with user-supplied mappings.</p> <p>This function updates the internal forward and reverse identifier maps for either proteins or peptides. Ensures consistency by updating both directions of the mapping.</p> <ul> <li> <p>For <code>'protein'</code>:</p> <ul> <li>forward: gene \u2192 accession  </li> <li>reverse: accession \u2192 gene</li> </ul> </li> <li> <p>For <code>'peptide'</code>:</p> <ul> <li>forward: protein accession \u2192 peptide</li> <li>reverse: peptide \u2192 protein accession</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>dict</code> <p>Dictionary of mappings to add.</p> required <code>on</code> <code>str</code> <p>Which maps to update. Must be <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <code>direction</code> <code>str</code> <p><code>'forward'</code> or <code>'reverse'</code> \u2014 determines how the <code>mapping</code> should be interpreted.</p> <code>'forward'</code> <code>overwrite</code> <code>bool</code> <p>If True, allows overwriting existing entries.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, prints a summary of updated keys.</p> <code>True</code> Note <p>The corresponding reverse map is automatically updated to maintain bidirectional consistency.</p> Example <p>Add new gene-to-accession mappings (protein):     <pre><code>pdata.update_identifier_maps(\n    {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n    on='protein',\n    direction='forward'\n)\n</code></pre></p> <p>Add peptide \u2192 protein mappings:     <pre><code>pdata.update_identifier_maps(\n    {'PEPTIDE_ABC': 'P12345'},\n    on='peptide',\n    direction='reverse'\n)\n</code></pre></p> <p>Overwrite a protein \u2192 gene mapping:     <pre><code>pdata.update_identifier_maps(\n    {'P12345': 'NEWGENE'},\n    on='protein',\n    direction='reverse',\n    overwrite=True\n)\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def update_identifier_maps(self, mapping, on='protein', direction='forward', overwrite=False, verbose=True):\n    \"\"\"\n    Update cached identifier maps with user-supplied mappings.\n\n    This function updates the internal forward and reverse identifier maps\n    for either proteins or peptides. Ensures consistency by updating both\n    directions of the mapping.\n\n    - For `'protein'`:\n        * forward: gene \u2192 accession  \n        * reverse: accession \u2192 gene\n\n    - For `'peptide'`:\n        * forward: protein accession \u2192 peptide\n        * reverse: peptide \u2192 protein accession\n\n    Args:\n        mapping (dict): Dictionary of mappings to add.\n        on (str): Which maps to update. Must be `'protein'` or `'peptide'`.\n        direction (str): `'forward'` or `'reverse'` \u2014 determines how the `mapping` should be interpreted.\n        overwrite (bool): If True, allows overwriting existing entries.\n        verbose (bool): If True, prints a summary of updated keys.\n\n    Note:\n        The corresponding reverse map is automatically updated to maintain bidirectional consistency.\n\n    Example:\n        Add new gene-to-accession mappings (protein):\n            ```python\n            pdata.update_identifier_maps(\n                {'MYGENE1': 'P00001', 'MYGENE2': 'P00002'},\n                on='protein',\n                direction='forward'\n            )\n            ```\n\n        Add peptide \u2192 protein mappings:\n            ```python\n            pdata.update_identifier_maps(\n                {'PEPTIDE_ABC': 'P12345'},\n                on='peptide',\n                direction='reverse'\n            )\n            ```\n\n        Overwrite a protein \u2192 gene mapping:\n            ```python\n            pdata.update_identifier_maps(\n                {'P12345': 'NEWGENE'},\n                on='protein',\n                direction='reverse',\n                overwrite=True\n            )\n            ```\n\n    \"\"\"\n    if on == 'protein':\n        forward, reverse = self._cached_identifier_maps_protein\n    elif on == 'peptide':\n        forward, reverse = self._cached_identifier_maps_peptide\n    else:\n        raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n\n    source_map = forward if direction == 'forward' else reverse\n    target_map = reverse if direction == 'forward' else forward\n\n    added, updated, skipped = 0, 0, 0\n\n    for key, val in mapping.items():\n        if key in source_map:\n            if overwrite:\n                source_map[key] = val\n                target_map[val] = key\n                updated += 1\n            else:\n                skipped += 1\n        else:\n            source_map[key] = val\n            target_map[val] = key\n            added += 1\n\n    message = (\n        f\"[update_identifier_maps] Updated '{on}' ({direction}): \"\n        f\"{added} added, {updated} overwritten, {skipped} skipped.\"\n    )\n\n    if verbose:\n        print(message)\n    self._append_history(message)\n\n    # Update .prot.var[\"Genes\"] if updating protein identifier reverse map (accession \u2192 gene)\n    if on == 'protein' and direction == 'reverse':\n        updated_var_count = 0\n        updated_accessions = []\n\n        for acc, gene in mapping.items():\n            if acc in self.prot.var_names:\n                self.prot.var.at[acc, \"Genes\"] = gene\n                updated_accessions.append(acc)\n                updated_var_count += 1\n\n        if updated_var_count &gt; 0:\n            var_message = (\n                f\"\ud83d\udd01 Updated `.prot.var['Genes']` for {updated_var_count} entries from custom mapping. \"\n                f\"(View details in `pdata.metadata['identifier_map_history']`)\"\n            )\n            if verbose:\n                print(var_message)\n            self._append_history(var_message)\n\n    # Log detailed update history for all cases\n    import datetime\n\n    record = {\n        'on': on,\n        'direction': direction,\n        'input_mapping': dict(mapping),  # shallow copy\n        'overwrite': overwrite,\n        'timestamp': datetime.datetime.now().isoformat(timespec='seconds'),\n        'summary': {\n            'added': added,\n            'updated': updated,\n            'skipped': skipped,\n        }\n    }\n\n    if on == 'protein' and direction == 'reverse':\n        record['updated_var_column'] = {\n            'column': 'Genes',\n            'accessions': updated_accessions,\n            'n_updated': updated_var_count\n        }\n\n    self.metadata.setdefault(\"identifier_map_history\", []).append(record)\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.identifier.IdentifierMixin.update_missing_genes","title":"update_missing_genes","text":"<pre><code>update_missing_genes(gene_col='Genes', verbose=True)\n</code></pre> <p>Fill missing gene names in <code>.prot.var</code> using UniProt API.</p> <p>This function searches for missing values in the specified gene column and attempts to fill them by querying the UniProt API using protein accession IDs. If a gene name cannot be found, a placeholder 'UNKNOWN_' is used instead. <p>Parameters:</p> Name Type Description Default <code>gene_col</code> <code>str</code> <p>Column name in <code>.prot.var</code> to update (default: \"Genes\").</p> <code>'Genes'</code> <code>verbose</code> <code>bool</code> <p>Whether to print summary information (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Note <ul> <li>This function only operates on <code>.prot.var</code>, not <code>.pep.var</code>.</li> <li>If UniProt is unavailable or returns no match, the missing entry is filled as <code>'UNKNOWN_&lt;accession&gt;'</code>.</li> <li>To manually correct unknown entries later, use <code>update_identifier_maps()</code> with <code>direction='reverse'</code>.</li> </ul> Example <p>Automatically fill missing gene names using UniProt:     <pre><code>pdata.update_missing_genes()\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/identifier.py</code> <pre><code>def update_missing_genes(self, gene_col=\"Genes\", verbose=True):\n    \"\"\"\n    Fill missing gene names in `.prot.var` using UniProt API.\n\n    This function searches for missing values in the specified gene column\n    and attempts to fill them by querying the UniProt API using protein\n    accession IDs. If a gene name cannot be found, a placeholder\n    'UNKNOWN_&lt;accession&gt;' is used instead.\n\n    Args:\n        gene_col (str): Column name in `.prot.var` to update (default: \"Genes\").\n        verbose (bool): Whether to print summary information (default: True).\n\n    Returns:\n        None\n\n    Note:\n        - This function only operates on `.prot.var`, not `.pep.var`.\n        - If UniProt is unavailable or returns no match, the missing entry is filled as `'UNKNOWN_&lt;accession&gt;'`.\n        - To manually correct unknown entries later, use `update_identifier_maps()` with `direction='reverse'`.\n\n    Example:\n        Automatically fill missing gene names using UniProt:\n            ```python\n            pdata.update_missing_genes()\n            ```\n    \"\"\"\n    var = self.prot.var\n\n    if gene_col not in var.columns:\n        if verbose:\n            print(f\"{format_log_prefix('warn')} Column '{gene_col}' not found in .prot.var.\")\n        return\n\n    missing_mask = var[gene_col].isna()\n    if not missing_mask.any():\n        if verbose:\n            print(f\"{format_log_prefix('result')} No missing gene names found.\")\n        return\n\n    accessions = var.index[missing_mask].tolist()\n    if verbose:\n        print(f\"{format_log_prefix('info_only')} {len(accessions)} proteins with missing gene names.\")\n\n    try:\n        df = utils.get_uniprot_fields(\n            accessions,\n            search_fields=[\"accession\", \"gene_primary\"],\n            standardize=True\n        )\n    except Exception as e:\n        print(f\"{format_log_prefix('error')} UniProt query failed: {e}\")\n        return\n    df = utils.standardize_uniprot_columns(df)\n\n    if df.empty or \"accession\" not in df.columns or \"gene_primary\" not in df.columns:\n        print(f\"{format_log_prefix('warn')} UniProt returned no usable gene mapping columns.\")\n        return\n\n    gene_map = dict(zip(df[\"accession\"], df[\"gene_primary\"]))\n    filled = self.prot.var.loc[missing_mask].index.map(lambda acc: gene_map.get(acc))\n    final_genes = [\n        gene if pd.notna(gene) else f\"UNKNOWN_{acc}\"\n        for acc, gene in zip(self.prot.var.loc[missing_mask].index, filled)\n    ]\n    self.prot.var.loc[missing_mask, gene_col] = final_genes\n\n    found = sum(pd.notna(filled))\n    unknown = len(final_genes) - found\n    if verbose:\n        if found:\n            print(f\"{format_log_prefix('result')} Recovered {found} gene name(s) from UniProt. Genes found:\")\n            filled_clean = [str(g) for g in filled if pd.notna(g)]\n            preview = \", \".join(filled_clean[:10])\n            if found &gt; 10:\n                preview += \"...\"\n            print(\"        \", preview)\n        if unknown:\n            missing_ids = self.prot.var.loc[missing_mask].index[pd.isna(filled)]\n            print(f\"{format_log_prefix('warn')} {unknown} gene name(s) still missing. Assigned as 'UNKNOWN_&lt;accession&gt;' for:\")\n            print(\"        \", \", \".join(missing_ids[:5]) + (\"...\" if unknown &gt; 10 else \"\"))\n            print(\"     \ud83d\udca1 Tip: You can update these using `pdata.update_identifier_maps({'GENE': 'ACCESSION'}, on='protein', direction='reverse', overwrite=True)`\\n\")\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.metrics","title":"metrics","text":""},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.metrics.MetricsMixin","title":"MetricsMixin","text":"<p>Computes descriptive and RS-derived metrics for proteomic data.</p> <p>This mixin provides utility functions for calculating summary statistics on protein and peptide abundance data, as well as inspecting the structure of the RS (protein \u00d7 peptide) relational matrix.</p> <p>Features:</p> <ul> <li>Computes per-sample quantification and abundance metrics for both proteins and peptides</li> <li>Calculates RS-derived properties such as the number of peptides per protein and the number of unique peptides</li> <li>Updates <code>.obs</code>, <code>.var</code>, and <code>.summary</code> with relevant metrics</li> <li>Provides visualization and tabular summaries of RS matrix connectivity</li> </ul> <p>Methods:</p> Name Description <code>_update_metrics</code> <p>Computes per-sample and RS-derived metrics for <code>.prot</code> and <code>.pep</code>.</p> <code>_update_summary_metrics</code> <p>Adds per-sample high-confidence protein counts to <code>.summary</code>.</p> <code>describe_rs</code> <p>Returns a DataFrame summarizing peptide connectivity per protein.</p> <code>plot_rs</code> <p>Generates histograms of peptide\u2013protein and protein\u2013peptide mapping counts.</p> Source code in <code>src/scpviz/pAnnData/metrics.py</code> <pre><code>class MetricsMixin:\n    \"\"\"\n    Computes descriptive and RS-derived metrics for proteomic data.\n\n    This mixin provides utility functions for calculating summary statistics on\n    protein and peptide abundance data, as well as inspecting the structure of\n    the RS (protein \u00d7 peptide) relational matrix.\n\n    Features:\n\n    - Computes per-sample quantification and abundance metrics for both proteins and peptides\n    - Calculates RS-derived properties such as the number of peptides per protein and the number of unique peptides\n    - Updates `.obs`, `.var`, and `.summary` with relevant metrics\n    - Provides visualization and tabular summaries of RS matrix connectivity\n\n    Functions:\n        _update_metrics: Computes per-sample and RS-derived metrics for `.prot` and `.pep`.\n        _update_summary_metrics: Adds per-sample high-confidence protein counts to `.summary`.\n        describe_rs: Returns a DataFrame summarizing peptide connectivity per protein.\n        plot_rs: Generates histograms of peptide\u2013protein and protein\u2013peptide mapping counts.\n    \"\"\"\n    def _update_metrics(self):\n        \"\"\"\n        Compute and update core QC and RS-based metrics for `.obs` and `.var`.\n\n        This internal method updates:\n\n        - `.prot.obs` and `.pep.obs` with per-sample metrics:\n            \u2022 `*_quant`: Proportion of non-missing values\n            \u2022 `*_count`: Number of non-missing values\n            \u2022 `*_abundance_sum`: Sum of observed abundances\n            \u2022 `mbr_count`, `high_count`: Count of MBR annotations (if present in layer 'X_mbr')\n\n        - `.prot.var` with RS-derived metrics (if available):\n            \u2022 `peptides_per_protein`: Total peptides mapped to each protein\n            \u2022 `unique_peptides`: Number of peptides uniquely mapping to each protein\n\n        Note:\n            This function is typically called automatically after filtering, imputation,\n            or importing new data. It should not be run manually under normal usage.\n        \"\"\"\n        if self.prot is not None:\n            X = self.prot.X.toarray()\n            self.prot.obs['protein_quant'] = np.sum(~np.isnan(X), axis=1) / X.shape[1]\n            self.prot.obs['protein_count'] = np.sum(~np.isnan(X), axis=1)\n            self.prot.obs['protein_abundance_sum'] = np.nansum(X, axis=1)\n\n            if 'X_mbr' in self.prot.layers:\n                self.prot.obs['mbr_count'] = (self.prot.layers['X_mbr'] == 'Peak Found').sum(axis=1)\n                self.prot.obs['high_count'] = (self.prot.layers['X_mbr'] == 'High').sum(axis=1)\n\n        if self.pep is not None:\n            X = self.pep.X.toarray()\n            self.pep.obs['peptide_quant'] = np.sum(~np.isnan(X), axis=1) / X.shape[1]\n            self.pep.obs['peptide_count'] = np.sum(~np.isnan(X), axis=1)\n            self.pep.obs['peptide_abundance_sum'] = np.nansum(X, axis=1)\n\n            if 'X_mbr' in self.pep.layers:\n                self.pep.obs['mbr_count'] = (self.pep.layers['X_mbr'] == 'Peak Found').sum(axis=1)\n                self.pep.obs['high_count'] = (self.pep.layers['X_mbr'] == 'High').sum(axis=1)\n\n        # RS metrics for prot.var\n        if self.rs is not None and self.prot is not None:\n            rs = self.rs  # leave it sparse\n            peptides_per_protein = rs.getnnz(axis=1)\n            unique_mask = rs.getnnz(axis=0) == 1\n            unique_counts = rs[:, unique_mask].getnnz(axis=1)\n            self.prot.var['peptides_per_protein'] = peptides_per_protein\n            self.prot.var['unique_peptides'] = unique_counts\n\n    def _update_summary_metrics(self, unique_peptide_thresh=2):\n        \"\"\"\n        Compute RS-derived per-sample summary metric for protein confidence.\n\n        Adds the following column to `.summary`:\n\n        - `unique_pep2_protein_count`: Number of proteins per sample with at least \n        `unique_peptide_thresh` uniquely mapping peptides (default: 2).\n\n        This is useful for quality control and filtering based on protein-level confidence.\n\n        Args:\n            unique_peptide_thresh (int): Minimum number of uniquely mapping peptides required\n                to consider a protein as confidently quantified.\n        \"\"\"\n        if (\n            self.rs is not None and\n            self.prot is not None and\n            hasattr(self, '_summary') and\n            'unique_peptides' in self.prot.var.columns\n        ):\n            unique_mask = self.prot.var['unique_peptides'] &gt;= unique_peptide_thresh\n            quant_matrix = self.prot.X.toarray()\n            high_conf_matrix = quant_matrix[:, unique_mask]\n            high_conf_count = np.sum(~np.isnan(high_conf_matrix), axis=1)\n            self._summary['unique_pep2_protein_count'] = high_conf_count\n\n    def describe_rs(self):\n        \"\"\"\n        Summarize the protein\u2013peptide RS (relational) matrix.\n\n        Returns a DataFrame with one row per protein, describing its peptide mapping coverage:\n\n        - `peptides_per_protein`: Total number of peptides mapped to each protein.\n        - `unique_peptides`: Number of uniquely mapping peptides (peptides linked to only one protein).\n\n        Returns:\n            pd.DataFrame: Summary statistics for each protein in the RS matrix.\n\n        Note:\n            If `.prot` is available, index labels are taken from `.prot.var_names`.\n        \"\"\"\n        if self.rs is None:\n            print(\"\u26a0\ufe0f No RS matrix set.\")\n            return None\n\n        rs = self.rs\n\n        # peptides per protein\n        peptides_per_protein = rs.getnnz(axis=1)\n        # unique peptides per protein (those mapped only to this protein)\n        unique_mask = rs.getnnz(axis=0) == 1\n        unique_counts = rs[:, unique_mask].getnnz(axis=1)\n\n        summary_df = pd.DataFrame({\n            \"peptides_per_protein\": peptides_per_protein,\n            \"unique_peptides\": unique_counts\n        }, index=self.prot.var_names if self.prot is not None else range(rs.shape[0]))\n\n        return summary_df\n</code></pre>"},{"location":"reference/pAnnData/metadata_mixins/#src.scpviz.pAnnData.metrics.MetricsMixin.describe_rs","title":"describe_rs","text":"<pre><code>describe_rs()\n</code></pre> <p>Summarize the protein\u2013peptide RS (relational) matrix.</p> <p>Returns a DataFrame with one row per protein, describing its peptide mapping coverage:</p> <ul> <li><code>peptides_per_protein</code>: Total number of peptides mapped to each protein.</li> <li><code>unique_peptides</code>: Number of uniquely mapping peptides (peptides linked to only one protein).</li> </ul> <p>Returns:</p> Type Description <p>pd.DataFrame: Summary statistics for each protein in the RS matrix.</p> Note <p>If <code>.prot</code> is available, index labels are taken from <code>.prot.var_names</code>.</p> Source code in <code>src/scpviz/pAnnData/metrics.py</code> <pre><code>def describe_rs(self):\n    \"\"\"\n    Summarize the protein\u2013peptide RS (relational) matrix.\n\n    Returns a DataFrame with one row per protein, describing its peptide mapping coverage:\n\n    - `peptides_per_protein`: Total number of peptides mapped to each protein.\n    - `unique_peptides`: Number of uniquely mapping peptides (peptides linked to only one protein).\n\n    Returns:\n        pd.DataFrame: Summary statistics for each protein in the RS matrix.\n\n    Note:\n        If `.prot` is available, index labels are taken from `.prot.var_names`.\n    \"\"\"\n    if self.rs is None:\n        print(\"\u26a0\ufe0f No RS matrix set.\")\n        return None\n\n    rs = self.rs\n\n    # peptides per protein\n    peptides_per_protein = rs.getnnz(axis=1)\n    # unique peptides per protein (those mapped only to this protein)\n    unique_mask = rs.getnnz(axis=0) == 1\n    unique_counts = rs[:, unique_mask].getnnz(axis=1)\n\n    summary_df = pd.DataFrame({\n        \"peptides_per_protein\": peptides_per_protein,\n        \"unique_peptides\": unique_counts\n    }, index=self.prot.var_names if self.prot is not None else range(rs.shape[0]))\n\n    return summary_df\n</code></pre>"},{"location":"reference/pAnnData/pAnnData/","title":"<code>pAnnData</code> Class and Import","text":""},{"location":"reference/pAnnData/pAnnData/#src.scpviz.pAnnData.pAnnData.pAnnData","title":"src.scpviz.pAnnData.pAnnData.pAnnData","text":"<p>               Bases: <code>BaseMixin</code>, <code>ValidationMixin</code>, <code>SummaryMixin</code>, <code>MetricsMixin</code>, <code>IdentifierMixin</code>, <code>HistoryMixin</code>, <code>EditingMixin</code>, <code>FilterMixin</code>, <code>AnalysisMixin</code>, <code>EnrichmentMixin</code>, <code>IOMixin</code>, <code>PlotMixin</code></p> <p>Unified data container for protein and peptide expression in single-cell and bulk proteomics.</p> <p><code>pAnnData</code> integrates matched protein-level and peptide-level <code>AnnData</code> objects, along with an optional binary relational structure (<code>rs</code>) that maps peptides to their parent proteins. It is designed for single-cell and bulk proteomics data, and supports a range of analysis workflows, including filtering, normalization, imputation, differential expression, enrichment, and visualization.</p> <p>This class is composed of modular mixins to enhance maintainability and organization.</p>"},{"location":"reference/pAnnData/pAnnData/#src.scpviz.pAnnData.pAnnData.pAnnData--mixins","title":"Mixins","text":"<ul> <li>BaseMixin: Core internal utilities, copying, and simple logic.</li> <li>ValidationMixin: Ensures structural and dimensional consistency across <code>.prot</code>, <code>.pep</code>, <code>.summary</code>, and <code>rs</code>.</li> <li>SummaryMixin: Maintains <code>.summary</code>, synchronizes metadata, and caches per-sample metrics.</li> <li>MetricsMixin: Computes descriptive statistics from expression data and relational structure (RS matrix).</li> <li>IdentifierMixin: Manages bidirectional gene/accession mappings and handles missing gene resolution via UniProt.</li> <li>HistoryMixin: Tracks all operations performed on the object for transparency.</li> <li>EditingMixin: Supports in-place editing, direct manipulation of expression matrices, and data export.</li> <li>FilterMixin: Provides flexible filtering of samples and proteins/peptides based on metadata, presence, or quantification.</li> <li>AnalysisMixin: Core statistical operations: differential expression, imputation, PCA, clustering, etc.</li> <li>EnrichmentMixin: Runs STRING-based enrichment analyses (GO, pathways, PPI) using ranked or unranked protein sets.</li> <li>PlotMixin: Used to plot and visualize attributes of the pAnnData object, frequently for QC.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>prot</code> <code>AnnData</code> <p>Protein-level expression matrix, with <code>.obs</code> containing sample metadata and <code>.var</code> describing protein features.</p> <code>None</code> <code>pep</code> <code>AnnData</code> <p>Peptide-level expression matrix, structured analogously to <code>prot</code>.</p> <code>None</code> <code>rs</code> <code>ndarray or spmatrix</code> <p>Binary relational matrix (proteins \u00d7 peptides), where non-zero entries indicate a parent-protein relationship.</p> <code>None</code> <code>summary</code> <code>DataFrame</code> <p>Sample-level metadata table, merged from <code>.prot.obs</code> and <code>.pep.obs</code>, with support for additional metrics.</p> required <code>stats</code> <code>dict</code> <p>Dictionary for storing analysis outputs such as DE results, imputation metadata, and enrichment summaries.</p> required <code>history</code> <code>list of str</code> <p>Chronological list of user-invoked operations, automatically tracked for reproducibility.</p> required Todo <p>Decide whether to standardize internal terminology to <code>classes</code> or <code>class_types</code> for sample-level grouping.</p> Source code in <code>src/scpviz/pAnnData/pAnnData.py</code> <pre><code>class pAnnData(BaseMixin, ValidationMixin, SummaryMixin, MetricsMixin,\n               IdentifierMixin, HistoryMixin, EditingMixin, FilterMixin,\n               AnalysisMixin, EnrichmentMixin, IOMixin, PlotMixin):\n    \"\"\"\n    Unified data container for protein and peptide expression in single-cell and bulk proteomics.\n\n    `pAnnData` integrates matched protein-level and peptide-level `AnnData` objects, along with an optional\n    binary relational structure (`rs`) that maps peptides to their parent proteins. It is designed for single-cell and bulk\n    proteomics data, and supports a range of analysis workflows, including filtering, normalization, imputation, differential\n    expression, enrichment, and visualization.\n\n    This class is composed of modular mixins to enhance maintainability and organization.\n\n    ## Mixins\n\n    - **BaseMixin**: Core internal utilities, copying, and simple logic.\n    - **ValidationMixin**: Ensures structural and dimensional consistency across `.prot`, `.pep`, `.summary`, and `rs`.\n    - **SummaryMixin**: Maintains `.summary`, synchronizes metadata, and caches per-sample metrics.\n    - **MetricsMixin**: Computes descriptive statistics from expression data and relational structure (RS matrix).\n    - **IdentifierMixin**: Manages bidirectional gene/accession mappings and handles missing gene resolution via UniProt.\n    - **HistoryMixin**: Tracks all operations performed on the object for transparency.\n    - **EditingMixin**: Supports in-place editing, direct manipulation of expression matrices, and data export.\n    - **FilterMixin**: Provides flexible filtering of samples and proteins/peptides based on metadata, presence, or quantification.\n    - **AnalysisMixin**: Core statistical operations: differential expression, imputation, PCA, clustering, etc.\n    - **EnrichmentMixin**: Runs STRING-based enrichment analyses (GO, pathways, PPI) using ranked or unranked protein sets.\n    - **PlotMixin**: Used to plot and visualize attributes of the pAnnData object, frequently for QC.\n\n    Args:\n        prot (AnnData): Protein-level expression matrix, with `.obs` containing sample metadata and `.var` describing protein features.\n\n        pep (AnnData): Peptide-level expression matrix, structured analogously to `prot`.\n\n        rs (np.ndarray or sparse.spmatrix, optional):\n            Binary relational matrix (proteins \u00d7 peptides), where non-zero entries indicate a parent-protein relationship.\n\n        summary (pd.DataFrame, optional):\n            Sample-level metadata table, merged from `.prot.obs` and `.pep.obs`, with support for additional metrics.\n\n        stats (dict, optional):\n            Dictionary for storing analysis outputs such as DE results, imputation metadata, and enrichment summaries.\n\n        history (list of str, optional):\n            Chronological list of user-invoked operations, automatically tracked for reproducibility.\n\n    Todo:\n        Decide whether to standardize internal terminology to `classes` or `class_types` for sample-level grouping.\n    \"\"\"\n    def __init__(self, \n                 prot = None, # np.ndarray | sparse.spmatrix \n                 pep = None, # np.ndarray | sparse.spmatrix\n                 rs = None): # np.ndarray | sparse.spmatrix, protein x peptide relational data\n\n        self._prot = ad.AnnData(prot) if prot is not None else None\n        self._pep = ad.AnnData(pep) if pep is not None else None\n        self._rs = None\n        if rs is not None:\n            self._set_RS(rs) # Defined in the EditingMixin\n\n        # Internal attributes\n        self._history = []\n        self._summary = pd.DataFrame()\n        self._stats = {}\n        self._summary_is_stale = False\n\n    def __repr__(self):\n        def format_summary(summary):\n            if summary is None:\n                return \"Summary:\\n  None\"\n\n            default_cols = {\"protein_count\", \"peptide_count\", \"protein_quant\", \"peptide_quant\"}\n            grouping_cols = [col for col in summary.columns\n                            if col not in default_cols and summary[col].nunique() &lt; len(summary)]\n\n            avg_prot = summary[\"protein_count\"].mean() if \"protein_count\" in summary else None\n            avg_quant = summary[\"protein_quant\"].mean() if \"protein_quant\" in summary else None\n            low_quant = (summary[\"protein_quant\"] &lt; 0.5).sum() if \"protein_quant\" in summary else None\n\n            lines = []\n            if grouping_cols:\n                lines.append(f\"Groups: {', '.join(grouping_cols)}\")\n            if avg_prot is not None:\n                lines.append(f\"Avg proteins/sample: {avg_prot:.1f}\")\n            if avg_quant is not None:\n                lines.append(f\"Avg protein quant: {avg_quant:.2f}\")\n            if low_quant is not None:\n                lines.append(f\"Samples &lt; 50% quant: {low_quant}\")\n\n            return \"Summary:\\n\" + \"\\n\".join(f\"  {line}\" for line in lines) if lines else \"Summary:\\n  \u2014\"\n\n        def format_ann(adata, label, shared_obs=False):\n            if adata is None:\n                return f\"{label}: None\"\n\n            shape_str = f\"{adata.shape[0]} files \u00d7 {adata.shape[1]} {label.lower()}s\"\n            obs_cols = ', '.join(adata.obs.columns[:5]) + ('...' if len(adata.obs.columns) &gt; 5 else '')\n            var_cols = ', '.join(adata.var.columns[:5]) + ('...' if len(adata.var.columns) &gt; 5 else '')\n            obsm_keys = ', '.join(adata.obsm.keys())\n            layers_keys = ', '.join(adata.layers.keys())\n\n            obs_line = \"  obs:    (same as protein)\" if shared_obs and label == \"Peptide\" else f\"  obs:    {obs_cols or '\u2014'}\"\n\n            return (f\"{label} (shape: {shape_str})\\n\"\n                    f\"{obs_line}\\n\"\n                    f\"  var:    {var_cols or '\u2014'}\\n\"\n                    f\"  obsm:   {obsm_keys or '\u2014'}\\n\"\n                    f\"  layers: {layers_keys or '\u2014'}\")\n\n        def format_rs_summary(rs):\n            if rs is None:\n                return \"RS:\\n  None\"\n\n            peptides_per_protein = rs.getnnz(axis=1)\n            unique_mask = rs.getnnz(axis=0) == 1\n            unique_counts = rs[:, unique_mask].getnnz(axis=1)\n\n            mean_pep = peptides_per_protein.mean()\n            mean_uniq = unique_counts.mean()\n            pct_highconf = (unique_counts &gt;= 2).mean() * 100\n\n            return (\n                f\"RS (shape: {rs.shape[0]} proteins \u00d7 {rs.shape[1]} peptides)\\n\"\n                f\"  Avg peptides/protein: {mean_pep:.2f}\\n\"\n                f\"  Avg unique peptides : {mean_uniq:.2f}\\n\"\n                f\"  Proteins with \u22652 unique peptides: {pct_highconf:.1f}%\"\n            )\n\n        def format_enrichments(stats):\n            functional = stats.get(\"functional\", {})\n            ppi_keys = stats.get(\"ppi\", {})\n            de_keys = {k for k in stats if \"vs\" in k and not k.endswith((\"_up\", \"_down\"))}\n\n            enriched_de = {\n                meta.get(\"input_key\", k)\n                for k, meta in functional.items()\n                if \"vs\" in k and meta.get(\"input_key\", None) in de_keys\n            }\n\n            n_de = len(de_keys)\n            n_func = len(functional)\n            n_ppi = len(ppi_keys)\n            n_unenriched = n_de - len(enriched_de)\n\n            lines = []\n            if n_de:\n                lines.append(f\"DE comparisons: {n_de}\")\n                if n_func:\n                    examples = sorted(k for k in functional if \"vs\" in k)[:3]\n                    lines.append(f\"Functional enrichment: {n_func} result(s) (e.g. {', '.join(examples)})\")\n                if n_ppi:\n                    lines.append(f\"PPI enrichment: {n_ppi} result(s)\")\n                if n_unenriched &gt; 0:\n                    lines.append(f\"Pending enrichment: {n_unenriched}\")\n            elif n_func or n_ppi:\n                if n_func:\n                    lines.append(f\"Functional enrichment: {n_func} result(s)\")\n                if n_ppi:\n                    lines.append(f\"PPI enrichment: {n_ppi} result(s)\")\n\n            if lines:\n                lines.append(\"\u21aa Use `pdata.list_enrichments()` for details\")\n\n            return \"STRING Enrichment:\\n\" + \"\\n\".join(f\"  {line}\" for line in lines) if lines else \"STRING Enrichment:\\n  \u2014\"\n\n        shared_obs = self.prot is not None and self.pep is not None and self.prot.obs.equals(self.pep.obs)\n\n        lines = [\"pAnnData object\"]\n        lines.append(\"\")  # Spacer\n\n        # Summary\n        lines.append(format_summary(self._summary))\n        lines.append(\"\")  # Spacer\n\n        # Protein and Peptide\n        lines.append(format_ann(self.prot, \"Protein\", shared_obs=shared_obs))\n        lines.append(\"\")  # Spacer\n        lines.append(format_ann(self.pep, \"Peptide\", shared_obs=shared_obs))\n        lines.append(\"\")  # Spacer\n\n        # RS Matrix\n        lines.append(format_rs_summary(self._rs))\n        lines.append(\"\")  # Spacer\n\n        # Enrichment Summary\n        enrichment_info = format_enrichments(self.stats)\n        lines.append(enrichment_info)\n\n        return \"\\n\".join(lines)\n\n    # -----------------------------\n    # Properties (GETTERS)\n    @property\n    def prot(self):\n        return self._prot\n\n    @property\n    def pep(self):\n        return self._pep\n\n    @property\n    def rs(self):\n        return self._rs\n\n    @property\n    def history(self):\n        return self._history\n\n    @property\n    def summary(self):\n        if not hasattr(self, \"_summary\"):\n            raise AttributeError(\"Summary has not been initialized.\")\n        if getattr(self, \"_summary_is_stale\", False):\n            print(\"[summary] \u26a0\ufe0f Warning: .summary has been modified. Run `(pdata).update_summary()` to sync changes back to .obs.\")\n        return self._summary\n\n    @property\n    def stats(self):\n        return self._stats\n\n    @property\n    def metadata(self):\n        if self.prot is not None and 'metadata' in self.prot.uns:\n            return self.prot.uns['metadata']\n        elif self.pep is not None and 'metadata' in self.pep.uns:\n            return self.pep.uns['metadata']\n        return {}\n\n    @property\n    def _cached_identifier_maps_protein(self):\n        if not hasattr(self, \"_gene_maps_protein\"):\n            self._gene_maps_protein = self._build_identifier_maps(self.prot)\n        return self._gene_maps_protein\n\n    @property\n    def _cached_identifier_maps_peptide(self):\n        if not hasattr(self, \"_protein_maps_peptide\"):\n            self._protein_maps_peptide = self._build_identifier_maps(self.pep)\n        return self._protein_maps_peptide\n\n    # -----------------------------\n    # Properties (SETTERS)\n    @prot.setter\n    def prot(self, value: ad.AnnData):\n        self._prot = value\n\n    @pep.setter\n    def pep(self, value: ad.AnnData):\n        self._pep = value\n\n    @rs.setter\n    def rs(self, value):\n        self._set_RS(value)  # From EditingMixin\n\n    @history.setter\n    def history(self, value):\n        self._history = value\n\n    @summary.setter\n    def summary(self, value: pd.DataFrame):\n        self._summary = TrackedDataFrame(\n            value,\n            parent=self,\n            mark_stale_fn=self._mark_summary_stale  # You likely have this defined in SummaryMixin or base\n        )\n        self._summary_is_stale = True\n        suppress = getattr(self, \"_suppress_summary_log\", False)\n        self.update_summary(recompute=True, sync_back=True, verbose=not suppress)\n\n    @stats.setter\n    def stats(self, value):\n        self._stats = value\n</code></pre>"},{"location":"reference/pAnnData/plot_mixin/","title":"Plotting","text":"<p>Mixin for convenient plots/visualization of <code>pAnnData</code> object. Typically a wrapper of functions from the <code>scpviz.plotting</code> module.</p>"},{"location":"reference/pAnnData/plot_mixin/#src.scpviz.pAnnData.plot.PlotMixin","title":"PlotMixin","text":"Source code in <code>src/scpviz/pAnnData/plot.py</code> <pre><code>class PlotMixin:\n    def plot_counts(self, classes=None, y='protein_count', **kwargs):\n        import seaborn as sns\n\n        df = self.summary # type: ignore #, in base\n        if classes is None:\n            df.reset_index()\n            classes = 'index'\n        sns.violinplot(data=df, x=classes, y=y, **kwargs)\n\n    def plot_rs(self, figsize=(10, 4)):\n        \"\"\"\n        Visualize connectivity in the RS (protein \u00d7 peptide) matrix.\n\n        Generates side-by-side histograms:\n\n        - Left: Number of peptides mapped to each protein\n        - Right: Number of proteins associated with each peptide\n\n        Args:\n            figsize (tuple): Size of the matplotlib figure (default: (10, 4)).\n\n        Returns:\n            None\n        \"\"\"\n        import matplotlib\n        import matplotlib.pyplot as plt\n\n        if self.rs is None:\n            print(\"\u26a0\ufe0f No RS matrix to plot.\")\n            return\n\n        rs = self.rs\n        prot_links = rs.getnnz(axis=1)\n        pep_links = rs.getnnz(axis=0)\n\n        fig, axes = plt.subplots(1, 2, figsize=figsize)\n\n        axes[0].hist(prot_links, bins=50, color='gray')\n        axes[0].set_title(\"Peptides per Protein\")\n        axes[0].set_xlabel(\"Peptide Count\")\n        axes[0].set_ylabel(\"Protein Frequency\")\n\n        axes[1].hist(pep_links, bins=50, color='gray')\n        axes[1].set_title(\"Proteins per Peptide\")\n        axes[1].set_xlabel(\"Protein Count\")\n        axes[1].set_ylabel(\"Peptide Frequency\")\n\n        plt.tight_layout()\n        backend = matplotlib.get_backend()\n        if \"agg\" in backend.lower():\n            # Running headless (e.g. pytest, CI)\n            plt.close(fig)\n        else:\n            plt.show(block=False)\n\n    def plot_abundance(self, ax=None, namelist=None, layer=\"X\",\n        on=\"protein\", classes=None, return_df=False, order=None,\n        palette=None, log=True, facet=None, height=4,\n        aspect=0.5, plot_points=True, x_label=\"gene\", kind=\"auto\", **kwargs,):\n        \"\"\"\n        Wrapper for `scpviz.plotting.plot_abundance`.\n\n        Plot abundance of proteins or peptides across samples.\n\n        This function visualizes expression values for selected proteins or peptides\n        using violin + box + strip plots, or bar plots when the number of replicates\n        per group is small. Supports grouping, faceting, and custom ordering.\n\n        Args:\n            ax (matplotlib.axes.Axes): Axis to plot on. Ignored if `facet` is used.\n            pdata (pAnnData): Input pAnnData object.\n            namelist (list of str, optional): List of accessions or gene names to plot.\n                If None, all available features are considered.\n            layer (str): Data layer to use for abundance values. Default is `'X'`.\n            on (str): Data level to plot, either `'protein'` or `'peptide'`.\n            classes (str or list of str, optional): `.obs` column(s) to use for grouping\n                samples. Determines coloring and grouping structure.\n            return_df (bool): If True, returns the DataFrame of replicate and summary values.\n            order (dict or list, optional): Custom order of classes. For dictionary input,\n                keys are class names and values are the ordered categories.  \n                Example: `order = {\"condition\": [\"sc\", \"kd\"]}`.\n            palette (list or dict, optional): Color palette mapping groups to colors.\n            log (bool): If True, apply log2 transformation to abundance values. Default is True.\n            facet (str, optional): `.obs` column to facet by, creating multiple subplots.\n            height (float): Height of each facet plot. Default is 4.\n            aspect (float): Aspect ratio of each facet plot. Default is 0.5.\n            plot_points (bool): Whether to overlay stripplot of individual samples.\n            x_label (str): Label for the x-axis, either `'gene'` or `'accession'`.\n            kind (str): Type of plot. Options:\n\n                - `'auto'`: Default; uses barplot if groups have \u2264 3 samples, otherwise violin.\n                - `'violin'`: Always use violin + box + strip.\n                - `'bar'`: Always use barplot.\n\n            **kwargs: Additional keyword arguments passed to seaborn plotting functions.\n\n        Returns:\n            ax (matplotlib.axes.Axes or seaborn.FacetGrid):\n                The axis or facet grid containing the plot.\n            df (pandas.DataFrame, optional): Returned if `return_df=True`.\n\n        !!! example\n            Plot abundance of two selected proteins:\n                ```python\n                pdata.plot_abundance(ax, namelist=['Slc12a2','Septin6'])\n                ```            \n\n        \"\"\"\n        return plotting.plot_abundance(ax=ax,pdata=self,namelist=namelist,\n            layer=layer,on=on,classes=classes,return_df=return_df,\n            order=order,palette=palette,log=log,facet=facet,\n            height=height,aspect=aspect,plot_points=plot_points,x_label=x_label,\n            kind=kind,**kwargs,\n        )\n</code></pre>"},{"location":"reference/pAnnData/plot_mixin/#src.scpviz.pAnnData.plot.PlotMixin.plot_abundance","title":"plot_abundance","text":"<pre><code>plot_abundance(ax=None, namelist=None, layer='X', on='protein', classes=None, return_df=False, order=None, palette=None, log=True, facet=None, height=4, aspect=0.5, plot_points=True, x_label='gene', kind='auto', **kwargs)\n</code></pre> <p>Wrapper for <code>scpviz.plotting.plot_abundance</code>.</p> <p>Plot abundance of proteins or peptides across samples.</p> <p>This function visualizes expression values for selected proteins or peptides using violin + box + strip plots, or bar plots when the number of replicates per group is small. Supports grouping, faceting, and custom ordering.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>Axis to plot on. Ignored if <code>facet</code> is used.</p> <code>None</code> <code>pdata</code> <code>pAnnData</code> <p>Input pAnnData object.</p> required <code>namelist</code> <code>list of str</code> <p>List of accessions or gene names to plot. If None, all available features are considered.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Data layer to use for abundance values. Default is <code>'X'</code>.</p> <code>'X'</code> <code>on</code> <code>str</code> <p>Data level to plot, either <code>'protein'</code> or <code>'peptide'</code>.</p> <code>'protein'</code> <code>classes</code> <code>str or list of str</code> <p><code>.obs</code> column(s) to use for grouping samples. Determines coloring and grouping structure.</p> <code>None</code> <code>return_df</code> <code>bool</code> <p>If True, returns the DataFrame of replicate and summary values.</p> <code>False</code> <code>order</code> <code>dict or list</code> <p>Custom order of classes. For dictionary input, keys are class names and values are the ordered categories. Example: <code>order = {\"condition\": [\"sc\", \"kd\"]}</code>.</p> <code>None</code> <code>palette</code> <code>list or dict</code> <p>Color palette mapping groups to colors.</p> <code>None</code> <code>log</code> <code>bool</code> <p>If True, apply log2 transformation to abundance values. Default is True.</p> <code>True</code> <code>facet</code> <code>str</code> <p><code>.obs</code> column to facet by, creating multiple subplots.</p> <code>None</code> <code>height</code> <code>float</code> <p>Height of each facet plot. Default is 4.</p> <code>4</code> <code>aspect</code> <code>float</code> <p>Aspect ratio of each facet plot. Default is 0.5.</p> <code>0.5</code> <code>plot_points</code> <code>bool</code> <p>Whether to overlay stripplot of individual samples.</p> <code>True</code> <code>x_label</code> <code>str</code> <p>Label for the x-axis, either <code>'gene'</code> or <code>'accession'</code>.</p> <code>'gene'</code> <code>kind</code> <code>str</code> <p>Type of plot. Options:</p> <ul> <li><code>'auto'</code>: Default; uses barplot if groups have \u2264 3 samples, otherwise violin.</li> <li><code>'violin'</code>: Always use violin + box + strip.</li> <li><code>'bar'</code>: Always use barplot.</li> </ul> <code>'auto'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to seaborn plotting functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ax</code> <code>Axes or FacetGrid</code> <p>The axis or facet grid containing the plot.</p> <code>df</code> <code>(DataFrame, optional)</code> <p>Returned if <code>return_df=True</code>.</p> <p>Example</p> <p>Plot abundance of two selected proteins:     <pre><code>pdata.plot_abundance(ax, namelist=['Slc12a2','Septin6'])\n</code></pre></p> Source code in <code>src/scpviz/pAnnData/plot.py</code> <pre><code>def plot_abundance(self, ax=None, namelist=None, layer=\"X\",\n    on=\"protein\", classes=None, return_df=False, order=None,\n    palette=None, log=True, facet=None, height=4,\n    aspect=0.5, plot_points=True, x_label=\"gene\", kind=\"auto\", **kwargs,):\n    \"\"\"\n    Wrapper for `scpviz.plotting.plot_abundance`.\n\n    Plot abundance of proteins or peptides across samples.\n\n    This function visualizes expression values for selected proteins or peptides\n    using violin + box + strip plots, or bar plots when the number of replicates\n    per group is small. Supports grouping, faceting, and custom ordering.\n\n    Args:\n        ax (matplotlib.axes.Axes): Axis to plot on. Ignored if `facet` is used.\n        pdata (pAnnData): Input pAnnData object.\n        namelist (list of str, optional): List of accessions or gene names to plot.\n            If None, all available features are considered.\n        layer (str): Data layer to use for abundance values. Default is `'X'`.\n        on (str): Data level to plot, either `'protein'` or `'peptide'`.\n        classes (str or list of str, optional): `.obs` column(s) to use for grouping\n            samples. Determines coloring and grouping structure.\n        return_df (bool): If True, returns the DataFrame of replicate and summary values.\n        order (dict or list, optional): Custom order of classes. For dictionary input,\n            keys are class names and values are the ordered categories.  \n            Example: `order = {\"condition\": [\"sc\", \"kd\"]}`.\n        palette (list or dict, optional): Color palette mapping groups to colors.\n        log (bool): If True, apply log2 transformation to abundance values. Default is True.\n        facet (str, optional): `.obs` column to facet by, creating multiple subplots.\n        height (float): Height of each facet plot. Default is 4.\n        aspect (float): Aspect ratio of each facet plot. Default is 0.5.\n        plot_points (bool): Whether to overlay stripplot of individual samples.\n        x_label (str): Label for the x-axis, either `'gene'` or `'accession'`.\n        kind (str): Type of plot. Options:\n\n            - `'auto'`: Default; uses barplot if groups have \u2264 3 samples, otherwise violin.\n            - `'violin'`: Always use violin + box + strip.\n            - `'bar'`: Always use barplot.\n\n        **kwargs: Additional keyword arguments passed to seaborn plotting functions.\n\n    Returns:\n        ax (matplotlib.axes.Axes or seaborn.FacetGrid):\n            The axis or facet grid containing the plot.\n        df (pandas.DataFrame, optional): Returned if `return_df=True`.\n\n    !!! example\n        Plot abundance of two selected proteins:\n            ```python\n            pdata.plot_abundance(ax, namelist=['Slc12a2','Septin6'])\n            ```            \n\n    \"\"\"\n    return plotting.plot_abundance(ax=ax,pdata=self,namelist=namelist,\n        layer=layer,on=on,classes=classes,return_df=return_df,\n        order=order,palette=palette,log=log,facet=facet,\n        height=height,aspect=aspect,plot_points=plot_points,x_label=x_label,\n        kind=kind,**kwargs,\n    )\n</code></pre>"},{"location":"reference/pAnnData/plot_mixin/#src.scpviz.pAnnData.plot.PlotMixin.plot_rs","title":"plot_rs","text":"<pre><code>plot_rs(figsize=(10, 4))\n</code></pre> <p>Visualize connectivity in the RS (protein \u00d7 peptide) matrix.</p> <p>Generates side-by-side histograms:</p> <ul> <li>Left: Number of peptides mapped to each protein</li> <li>Right: Number of proteins associated with each peptide</li> </ul> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>Size of the matplotlib figure (default: (10, 4)).</p> <code>(10, 4)</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/scpviz/pAnnData/plot.py</code> <pre><code>def plot_rs(self, figsize=(10, 4)):\n    \"\"\"\n    Visualize connectivity in the RS (protein \u00d7 peptide) matrix.\n\n    Generates side-by-side histograms:\n\n    - Left: Number of peptides mapped to each protein\n    - Right: Number of proteins associated with each peptide\n\n    Args:\n        figsize (tuple): Size of the matplotlib figure (default: (10, 4)).\n\n    Returns:\n        None\n    \"\"\"\n    import matplotlib\n    import matplotlib.pyplot as plt\n\n    if self.rs is None:\n        print(\"\u26a0\ufe0f No RS matrix to plot.\")\n        return\n\n    rs = self.rs\n    prot_links = rs.getnnz(axis=1)\n    pep_links = rs.getnnz(axis=0)\n\n    fig, axes = plt.subplots(1, 2, figsize=figsize)\n\n    axes[0].hist(prot_links, bins=50, color='gray')\n    axes[0].set_title(\"Peptides per Protein\")\n    axes[0].set_xlabel(\"Peptide Count\")\n    axes[0].set_ylabel(\"Protein Frequency\")\n\n    axes[1].hist(pep_links, bins=50, color='gray')\n    axes[1].set_title(\"Proteins per Peptide\")\n    axes[1].set_xlabel(\"Protein Count\")\n    axes[1].set_ylabel(\"Peptide Frequency\")\n\n    plt.tight_layout()\n    backend = matplotlib.get_backend()\n    if \"agg\" in backend.lower():\n        # Running headless (e.g. pytest, CI)\n        plt.close(fig)\n    else:\n        plt.show(block=False)\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Welcome to the scpviz tutorials. These guides walk you through the main steps of a single-cell or spatial proteomics workflow using <code>pAnnData</code>.  </p>"},{"location":"tutorials/#how-to-use-these-tutorials","title":"How to Use These Tutorials","text":"<p>Each tutorial is designed to be self-contained, with code snippets you can run in a Python environment.</p> <p>Each tutorial includes:</p> <ul> <li>Example code blocks you can copy into a Jupyter notebook.  </li> <li>Visual outputs to illustrate the results.  </li> <li>Tips and notes to explain recommended practices.  </li> </ul> <p>If you\u2019re new to scpviz, start with Importing Data and work through the sequence.  </p>"},{"location":"tutorials/#workflow-at-a-glance","title":"Workflow at a Glance","text":"<pre><code>  graph TB\n  A[\"`Import data  \n  (DIA-NN / PD)`\"] --&gt; B[\"`Parse metadata  \n  (.obs from filenames)`\"]\n  B --&gt; C[\"`Filter proteins/peptides  \n  (\u22652 unique peptides, sample queries)`\"]\n  C --&gt; D[\"`Normalize  \n    (global, reference feature)`\"]\n    D --&gt; E[\"`Impute missing values  \n    (KNN / group-wise)`\"]\n    E --&gt; F[\"`Plotting  \n    (abundance, PCA/UMAP, clustermap)`\"]\n    F --&gt; G[\"`DE analysis  \n    (mean vs. pairwise strategies)`\"]\n    G --&gt; H[\"`Enrichment (STRING)  \n    (GSEA / GO / PPI)`\"]\n    B --&gt; I[\"`Export results`\"]\n\n%% Optional side paths\nB -. \"QC summaries\" .-&gt; F\nC -. \"RS matrix checks\" .-&gt; F\nG -. \"ranked/unranked lists\" .-&gt; H\nD .-&gt; I\nF .-&gt; I\nG .-&gt; I \nH .-&gt; I</code></pre>"},{"location":"tutorials/#1-importing-data","title":"1. Importing Data","text":"<ul> <li>Load DIA-NN or Proteome Discoverer (PD) reports into <code>pAnnData</code>.</li> <li>Automatically parse sample metadata (<code>.obs</code>) from filenames.</li> <li>Understand the <code>prot</code>, <code>pep</code>, and <code>rs</code> matrices.</li> </ul>"},{"location":"tutorials/#2-filtering-and-normalization","title":"2. Filtering and Normalization","text":"<ul> <li>Filter proteins by peptide support (e.g. \u22652 unique peptides).</li> <li>Apply sample-level filters and advanced queries on <code>.obs</code> or <code>.summary</code>.</li> <li>Normalize intensities by global scale, reference features, or other strategies.</li> </ul>"},{"location":"tutorials/#3-imputation","title":"3. Imputation","text":"<ul> <li>Handle missing values using KNN-based or group-wise strategies.</li> <li>Summarize imputation statistics stored in <code>pdata.stats</code>.</li> </ul>"},{"location":"tutorials/#4-plotting","title":"4. Plotting","text":"<ul> <li>Visualize abundance distributions with violin/box/strip plots.</li> <li>Run PCA/UMAP embeddings with flexible coloring options.</li> <li>Generate heatmaps and clustermaps with class annotations.</li> </ul>"},{"location":"tutorials/#5-differential-expression-de","title":"5. Differential Expression (DE)","text":"<ul> <li>Perform DE testing at the protein or peptide level.</li> <li>Compare fold-change strategies (mean-based vs. pairwise median).</li> <li>Export DE results for downstream use.</li> </ul>"},{"location":"tutorials/#6-enrichment-and-networks","title":"6. Enrichment and Networks","text":"<ul> <li>Run GSEA and GO enrichment with STRING.</li> <li>Explore protein\u2013protein interaction networks.</li> <li>Retrieve functional annotations for differentially expressed genes.</li> </ul> <p>For in-depth details, see the API Reference.</p>"},{"location":"tutorials/de/","title":"5. Differential Expression","text":"<p>This tutorial is still under construction</p>"},{"location":"tutorials/de/#tutorial-5-differential-expression-de","title":"Tutorial 5: Differential Expression (DE)","text":"<p>Run DE analysis at the protein or peptide level. \u2014 test across groups (mean, pairwise, peptide-level).  </p>"},{"location":"tutorials/de/#protein-level-de","title":"Protein-Level DE","text":"<pre><code>de_results = pdata.de(group1=\"treated\", group2=\"control\", on=\"protein\")\nde_results.head()\n</code></pre>"},{"location":"tutorials/de/#fold-change-strategies","title":"Fold Change Strategies","text":"<pre><code># Mean-based fold change\npdata.de(group1=\"treated\", group2=\"control\", method=\"mean\")\n\n# Pairwise protein-level median\npdata.de(group1=\"treated\", group2=\"control\", method=\"protein_pairwise\")\n\n# Peptide-level median (via RS matrix)\npdata.de(group1=\"treated\", group2=\"control\", method=\"peptide_pairwise\")\n</code></pre> <p>\ud83d\udca1 Different strategies may be useful depending on noise and sample size.</p>"},{"location":"tutorials/enrichment/","title":"6. Enrichment + Networks","text":"<p>This tutorial is still under construction</p>"},{"location":"tutorials/enrichment/#tutorial-6-enrichment-and-networks","title":"Tutorial 6: Enrichment and Networks","text":"<p>Use STRING integration for enrichment and protein\u2013protein interaction (PPI) analysis.</p>"},{"location":"tutorials/enrichment/#functional-enrichment-gsea-go","title":"Functional Enrichment (GSEA / GO)","text":"<pre><code>pdata.enrichment_functional(on=\"protein\", source=\"de_results\")\n</code></pre>"},{"location":"tutorials/enrichment/#ppi-enrichment","title":"PPI Enrichment","text":"<pre><code>pdata.ppi_enrichment(on=\"protein\", source=\"de_results\")\n</code></pre>"},{"location":"tutorials/enrichment/#viewing-results","title":"Viewing Results","text":"<pre><code>pdata.list_enrichments()\n</code></pre> <p>\ud83d\udd17 Each enrichment run will provide a STRING link for interactive exploration.</p>"},{"location":"tutorials/filtering/","title":"Tutorial 2: Filtering and Normalization","text":"<p>Learn how to filter proteins and peptides in your dataset.</p> <p>All filter functions return a copy of the <code>pAnnData</code> object unless <code>inplace=True</code> is specified.</p> <p>There are three main filtering functions:</p> <ul> <li><code>pdata.filter_prot()</code> \u2013 filters proteins from the dataset  <ul> <li><code>pdata.filter_prot_found()</code> \u2013 sub-function for proteins found within specified samples  </li> <li><code>pdata.filter_prot_significant()</code> \u2013 sub-function for proteins significant within specified samples  </li> </ul> </li> <li><code>pdata.filter_sample()</code> \u2013 filters samples from the dataset  </li> <li><code>pdata.filter_rs()</code> \u2013 RS-based filtering (e.g., filtering by unique peptides)</li> </ul>"},{"location":"tutorials/filtering/#filter_prot","title":"<code>filter_prot()</code>","text":"<p>Filter protein data based on metadata conditions or accession lists (protein or gene names).</p> <p>Tip</p> <p>Multiple filters can be combined in a single call. For example:</p> <pre><code>condition = \"protein_quant &gt; 0.75\"\npdata_filtered = pdata.filter_prot(condition=condition, valid_genes=True, unique_profiles=True)\n</code></pre>"},{"location":"tutorials/filtering/#condition-based-filtering","title":"Condition-based filtering","text":"<p>A condition string to filter protein metadata. Supports: * Standard comparisons, e.g. <code>\"Protein FDR Confidence: Combined == 'High'\"</code> * Substring queries using <code>includes</code>, e.g. <code>\"Description includes 'p97'\"</code> Filter by metadata condition<pre><code>condition = \"Protein FDR Confidence: Combined == 'High'\"\npdata_filtered = pdata.filter_prot(condition=condition)\n</code></pre></p> Substring match on protein description<pre><code>condition = \"Description includes 'VCP'\"\npdata_filtered = pdata.filter_prot(condition=condition)\n</code></pre> <pre><code>pdata_filtered.prot.var\n</code></pre> Accession Description Genes P55072 ... Transitional endoplasmic reticulum ATPase OS=Homo sapiens OX=9606 GN=VCP PE=1 SV=4 ... VCP Q96JH7 ... Deubiquitinating protein VCPIP1 OS=Homo sapiens OX=9606 GN=VCPIP1 PE=1 SV=2 ... VCPIP1 Q8NHG7 ... Small VCP/p97-interacting protein OS=Homo sapiens OX=9606 GN=SVIP PE=1 SV=1 ... SVIP Q9H867 ... Protein N-lysine methyltransferase METTL21D OS=Homo sapiens OX=9606 GN=VCPKMT PE=1 SV=2 ... VCPKMT Numerical condition on metadata<pre><code>pdata_filtered = pdata.filter_prot(condition=\"unique_peptides &gt;= 2\")\n</code></pre> <p>Note: For <code>condition</code>, the first variable must match a column name in <code>prot.var</code>. Otherwise, an error will be raised.</p>"},{"location":"tutorials/filtering/#accession-based-filtering-accession-list-or-gene-names","title":"Accession-based filtering (accession list or gene names)","text":"<p>Accession-based filtering accepts both UniProt accessions as well as gene names. <code>pAnnData</code> objects automatically search the UniProt API for primary gene names upon import, and stores these in the object.</p> Filter by specific accessions or genes<pre><code>accessions = ['GAPDH', 'P53']\npdata_filtered = pdata.filter_prot(accessions=accessions)\n</code></pre>"},{"location":"tutorials/filtering/#valid-genes","title":"Valid genes","text":"<p>This removes rows with missing gene names and resolves duplicate gene names by appending numeric suffixes.</p> Filter proteins with valid genes only<pre><code>pdata_filtered = pdata.filter_prot(valid_genes=True)\n</code></pre> Accession Genes (before) Genes (after) P12345 GAPDH GAPDH P23456 ACTB ACTB P34567 NaN \u274c (removed) P45678 HSP90AA1 HSP90AA1 P45679 HSP90AA1 HSP90AA1_2 P56789 TUBB TUBB \u2026 \u2026 \u2026"},{"location":"tutorials/filtering/#unique-profiles","title":"Unique profiles","text":"<p>Removes rows with duplicate abundance profiles across samples (typically for isoforms with no distinguishing peptides). </p> <p>Tip</p> <p>Recommended to use for single-cell data, which has higher data sparsity and missing values in peptides, which frequently leads to duplicated profiles.</p> Filter proteins with unique abundance profiles<pre><code>pdata_filtered = pdata.filter_prot(unique_profiles=True)\n</code></pre> <p>For more information, see the API documentation for filter_prot()</p>"},{"location":"tutorials/filtering/#filter_prot_found","title":"<code>filter_prot_found()</code>","text":"<p>Filter proteins or peptides based on \"Found In\" detection across samples or groups.</p> <p>This method filters features by checking whether they are found in a minimum number or proportion of samples, either at the group level (e.g., biological condition) or based on individual files.</p> <p>Note: A true <code>match_any</code> flag retain proteins true in any group/file (OR logic). If <code>False</code>, requires all groups/files to be true (AND logic). The flag defaults to <code>True</code>.</p>"},{"location":"tutorials/filtering/#groups-and-match_any","title":"<code>groups</code> and <code>match_any</code>","text":"Filter proteins found in both cell lines (match_any=False, AND logic)<pre><code>pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_count=2, match_any=False)\n</code></pre> <p>In this example, the class column <code>\"cellline\"</code> contains two groups: A and B. Proteins must be detected in at least two samples within each cell line to be retained.</p> Cell line A Cell line B Result F1 F2 F3 F4 \ud83d\udfe9 \ud83d\udfe9 \ud83d\udfe9 \u26aa \u2705 Kept (found \u22652 per cell line) \ud83d\udfe9 \ud83d\udfe9 \u26aa \u26aa \u274c Filtered (not enough in cell line B), kept if <code>match_any=True</code> <p>The <code>group</code> parameter refers to a sample class column (e.g., <code>\"cellline\"</code>, <code>\"treatment\"</code>, <code>\"condition\"</code>). Each unique value in that column (e.g., <code>A</code>, <code>B</code>) is treated as a separate subgroup.</p> Filter proteins found in any cell line (match_any=True, OR logic, ratio \u2265 0.4)<pre><code>pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_ratio=0.4, match_any=True)\n</code></pre> <p>This example uses the class column <code>\"cellline\"</code> containing A and B. Proteins are retained if they are found in at least 40% of samples within any one cell line.</p> Cell line A Cell line B Ratio (A, B) Result F1 F2 F3 F4 F5 \ud83d\udfe9 \ud83d\udfe9 \u26aa \u26aa \u26aa (0.67, 0.00) \u2705 Kept (\u22650.4 in A) \ud83d\udfe9 \u26aa \u26aa \ud83d\udfe9 \u26aa (0.33, 0.50) \u2705 Kept (\u22650.4 in B) \u26aa \u26aa \ud83d\udfe9 \u26aa \u26aa (0.33, 0.00) \u274c Filtered (&lt;0.4 in both) <p>With <code>match_any=True</code>, OR logic is applied across groups \u2014 a protein passes if it meets the minimum ratio threshold in any one subgroup.</p>"},{"location":"tutorials/filtering/#filter-by-found-in-file-list","title":"Filter by found in file-list","text":"Filter proteins found in all three input files<pre><code>pdata_filtered = pdata.filter_prot_found(group=[\"F1\", \"F2\", \"F3\"])\n</code></pre> F1 F2 F3 Result \ud83d\udfe9 \ud83d\udfe9 \ud83d\udfe9 \u2705 Kept (found in all 3 files) \ud83d\udfe9 \u26aa \ud83d\udfe9 \u274c Filtered (not found in File 2) Filter proteins found in files of a specific sub-group with AND logic<pre><code>pdata.annotate_found(classes=['group', 'condition'])\npdata_filtered = pdata.filter_prot_found(group=['groupA_control', 'groupB_treated'],min_ratio=0.5,match_any=False)\n</code></pre> groupA_control groupA_treated groupB_control groupB_treated Result \ud83d\udfe9 \u26aa \u26aa \ud83d\udfe9 \u2705 Kept (Found in both specified groups) \ud83d\udfe9 \ud83d\udfe9 \ud83d\udfe9 \u26aa \u274c Filtered (Not found in <code>groupB_treated</code> group) Filter by class column, based on a minimum ratio (e.g., at least 50% in each cell line)<pre><code>pdata_filtered = pdata.filter_prot_found(group=\"cellline\", min_ratio=0.5, match_any=False)\n</code></pre> Cell line A Cell line B Ratio Result F1 F2 F3 F4 F5 \u2265 0.5 \ud83d\udfe9 \u26aa \ud83d\udfe9 \ud83d\udfe9 \u26aa (0.5, 0.66) \u2705 \ud83d\udfe9 \u26aa \ud83d\udfe9 \u26aa \u26aa (0.5, 0.33) \u274c (\u2705 if <code>match_any=True</code>) \ud83d\udfe9 \u26aa \u26aa \u26aa \u26aa (0.25, 0) \u274c <p>For more information, see the API documentation for filter_prot_found()</p>"},{"location":"tutorials/filtering/#filter_prot_significant","title":"<code>filter_prot_significant()</code>","text":"<p>Filter proteins based on significance across samples or groups using FDR thresholds. </p> <p>This method filters proteins by checking whether they are significant (e.g., <code>PG.Q.Value &lt; 0.01</code>) in a minimum number or proportion of samples, either per file or grouped.</p> <p>The grouping logic is akin to that of <code>filter_prot_found()</code>.</p> <p>Warn</p> <p>Only DIA-NN files contain per-sample specific q-values. For PD files, use <code>pdata.filter_prot_significant()</code> to filter based on global q-values.</p> <p>Note: A true <code>match_any</code> flag retain proteins significant in any group/file (OR logic). If <code>False</code>, requires all groups/files to be significant (AND logic). The flag defaults to <code>True</code>.</p> Filter proteins significant in all 'cellline' groups ('celline_A' and 'celline_B')<pre><code>pdata_filtered = pdata.filter_prot_significant(group=[\"cellline\"], min_count=2)\n</code></pre> Filter proteins significant in all three input files<pre><code>pdata_filtered = pdata.filter_prot_significant(group=[\"F1\", \"F2\", \"F3\"])\n</code></pre> Filter proteins significant in files of a specific sub-group<pre><code>pdata.annotate_significant(classes=['group', 'treatment'])\npdata_filtered = pdata.filter_prot_significant(group=[\"groupA_control\", \"groupB_treated\"])\n</code></pre> <p>For more information, see the API documentation for filter_prot_significant()</p>"},{"location":"tutorials/filtering/#filter_sample","title":"<code>filter_sample()</code>","text":"<p>Filter samples in a <code>pAnnData</code> object based on categorical, numeric, or identifier-based criteria. Accepts exactly one of the following arguments:</p> <ul> <li><code>values</code>: A dictionary or list of dictionaries specifying class-based filters (e.g., treatment, cellline).  </li> <li><code>condition</code>: A logical condition string evaluated against summary-level numeric metadata (e.g., protein count).  </li> <li><code>file_list</code>: A list of sample or file names to retain.</li> </ul>"},{"location":"tutorials/filtering/#filter-by-value","title":"Filter by value","text":"<p>Categorical metadata filtering allows selection of samples based on <code>.obs</code> or <code>.summary</code> fields such as treatment, cell line, or condition. This supports:</p> <ul> <li>A single dictionary, e.g. <code>{'cellline': 'A'}</code> </li> <li>A list of dictionaries for multiple matching cases, e.g. <code>[{...}, {...}]</code> </li> <li>Exact matching: <ul> <li><code>exact_cases=True</code> for strict combination matching across all key\u2013value pairs</li> <li><code>exact_cases=False</code> applies an OR logic within fields and AND logic across fields.</li> </ul> </li> </ul>"},{"location":"tutorials/filtering/#exact_casesfalse","title":"<code>exact_cases=False</code>","text":"<p>Filter by metadata values (exact_cases=False)<pre><code>pdata_filtered = pdata.filter_sample(values={'condition': ['kd','sc'], 'cellline': 'A'})\n</code></pre> When <code>exact_cases=False</code>, the logic is (OR within fields, AND across fields). This means any sample that matches any treatment in <code>['kd','sc']</code> and has <code>cellline='A'</code> is kept.</p> Sample Treatment Cell line Match logic Result 1 sc A \u2705 treatment in [kd, sc] and \u2705 cellline=A \u2705 Kept 2 kd A \u2705 treatment in [kd, sc] and \u2705 cellline=A \u2705 Kept 3 sc B \u2705 treatment in [kd, sc] and \u274c cellline=A \u274c 4 kd B \u2705 treatment in [kd, sc] and \u274c cellline=A \u274c"},{"location":"tutorials/filtering/#exact_casestrue","title":"<code>exact_cases=True</code>","text":"<p>Filter with multiple exact matching cases (exact_cases=True)<pre><code>pdata_filtered = pdata.filter_sample(\n    values=[\n        {'condition': 'kd', 'cellline': 'A'},\n        {'condition': 'sc', 'cellline': 'B'}\n    ],\n    exact_cases=True\n)\n</code></pre> When <code>exact_cases=True</code>, the logic requires an exact match to one of the full dictionaries. Here, only samples matching either <code>{treatment: 'kd', cellline: 'A'}</code> or <code>{treatment: 'sc', cellline: 'B'}</code> are kept.</p> Sample Treatment Cell line Match dictionary Result 1 sc A \u274c No exact match \u274c 2 kd A \u2705 Matches \u2705 Kept 3 sc B \u2705 Matches \u2705 Kept 4 kd B \u274c No exact match \u274c"},{"location":"tutorials/filtering/#filter-by-condition","title":"Filter by condition","text":"<p>Use a logical condition string referencing columns in <code>pdata.summary</code>. This enables numeric and boolean filtering based on sample-level summary statistics.</p> Filter samples with more than 1000 proteins<pre><code>pdata_filtered = pdata.filter_sample(condition=\"protein_count &gt; 1000\")\n</code></pre>"},{"location":"tutorials/filtering/#using-min_prot","title":"Using <code>min_prot</code>","text":"<p>A convenience shortcut for filtering based on a minimum protein count.</p> Filter samples with fewer than 1000 proteins<pre><code>pdata_filtered = pdata.filter_sample(min_prot=1000)\n</code></pre>"},{"location":"tutorials/filtering/#filter-by-file-list","title":"Filter by file list","text":"<p>Filter samples directly by their file or sample identifiers.</p> Keep specific samples by name<pre><code>pdata_filtered = pdata.filter_sample(file_list=['Sample_001', 'Sample_007'])\n</code></pre> Exclude specific samples by name<pre><code>pdata_filtered = pdata.filter_sample(exclude_file_list=['Sample_001', 'Sample_007'])\n</code></pre>"},{"location":"tutorials/filtering/#advanced-query-mode","title":"Advanced query mode","text":"<p>Enable advanced filtering with <code>query_mode=True</code> to execute raw pandas-style queries. This interprets <code>values</code> or <code>condition</code> as a raw <code>.query()</code> string evaluated directly on <code>.obs</code> or <code>.summary</code>.</p> Query .obs metadata using values<pre><code>pdata_filtered = pdata.filter_sample(values=\"cellline == 'AS' and condition == 'kd'\", query_mode=True)\n</code></pre> Query .summary metadata using condition<pre><code>pdata_filtered = pdata.filter_sample(condition=\"protein_count &gt; 1000 and protein_quant &gt; 0.9\", query_mode=True)\n</code></pre> <p>Complex logical expressions such as <code>(A and B) or C</code> are supported.</p>"},{"location":"tutorials/filtering/#additional-flags","title":"Additional flags","text":"<ul> <li><code>cleanup</code>: If <code>True</code> (default), remove proteins that become all-NaN or all-zero after sample filtering and synchronize RS/peptide matrices.   Set to <code>False</code> to retain all proteins (useful for downstream DE analyses requiring consistent feature alignment).</li> </ul> <p>For more information, see the API documentation for filter_sample()</p>"},{"location":"tutorials/filtering/#filter_rs","title":"<code>filter_rs()</code>","text":"<p>Filter the RS matrix and associated <code>.prot</code> and <code>.pep</code> data based on peptide\u2013protein relationships. This method applies rules for retaining proteins with sufficient peptide evidence and/or removing ambiguous peptides.</p>"},{"location":"tutorials/filtering/#key-parameters","title":"Key Parameters","text":"<ul> <li><code>min_peptides_per_protein</code> (int, optional) \u2013 Minimum total number of peptides required per protein.  </li> <li><code>min_unique_peptides_per_protein</code> (int, optional) \u2013 Minimum number of unique peptides required per protein.  </li> <li><code>max_proteins_per_peptide</code> (int, optional) \u2013 Maximum number of proteins a peptide can map to (peptides exceeding this are removed).  </li> <li><code>preset</code> (str or dict, optional) \u2013 Predefined filter presets:  </li> <li><code>\"default\"</code> \u2192 unique peptides \u2265 2  </li> <li><code>\"lenient\"</code> \u2192 total peptides \u2265 2  </li> <li>A dictionary specifying thresholds manually.   The default preset is <code>\"default\"</code>.</li> </ul>"},{"location":"tutorials/filtering/#filter-by-unique-peptides","title":"Filter by unique peptides","text":"Filter proteins with \u22652 unique peptides<pre><code>pdata_filtered = pdata.filter_rs(min_unique_peptides_per_protein=2)\n</code></pre> Protein Peptide 1 Peptide 2 Peptide 3 Unique peptides Result P001 \ud83d\udfe9 \ud83d\udfe9 \u26aa 2 \u2705 Kept P002 \ud83d\udfe9 \u26aa \u26aa 1 \u274c Filtered P003 \ud83d\udfe9 \ud83d\udfe9 \ud83d\udfe9 3 \u2705 Kept <p>Proteins with fewer than two unique peptides are removed by default. The filtering operation updates both <code>.prot</code> and <code>.pep</code> tables and synchronizes their mappings in the RS matrix.</p> <p>For more information, see the API documentation for filter_rs()</p>"},{"location":"tutorials/importing/","title":"1. Importing Data","text":"<p>This tutorial is still under construction</p>"},{"location":"tutorials/importing/#tutorial-1-importing-data","title":"Tutorial 1: Importing Data","text":"<p>This tutorial shows how to import DIA-NN or Proteome Discoverer (PD) outputs into a <code>pAnnData</code> object.  \u2014 load DIA-NN or PD outputs into <code>pAnnData</code>.  </p>"},{"location":"tutorials/importing/#loading-dia-nn-reports","title":"Loading DIA-NN Reports","text":"<pre><code>import scpviz as scv\n\n# Load DIA-NN report\npdata = scv.pAnnData.from_file(\"example_diann_report.txt\", source=\"diann\")\n\npdata.describe()\n</code></pre>"},{"location":"tutorials/importing/#loading-proteome-discoverer-pd-reports","title":"Loading Proteome Discoverer (PD) Reports","text":"<pre><code>pdata = scv.pAnnData.from_file(\"example_pd_report.txt\", source=\"pd\")\npdata.describe()\n</code></pre> <p>note that for PD, there is only global FDR data unlike for DIA-NN</p>"},{"location":"tutorials/importing/#before-version-32-ie-pd-25","title":"before version 3.2 i.e. PD 2.5","text":""},{"location":"tutorials/importing/#can-also-import-using-the-same-format","title":"can also import, using the same format","text":""},{"location":"tutorials/importing/#metadata-parsing","title":"Metadata Parsing","text":"<p>\u2014 extract <code>.obs</code> columns from filenames or reports. Sample metadata (columns in <code>.obs</code>) can be inferred directly from filenames:</p> <pre><code>pdata.obs.head()\n</code></pre> <ul> <li>any updates to .summary will be automatically pushed to <code>.prot.obs</code> and <code>.pep.obs</code> (if available). User will be prompted when necessary to run <code>pdata.update_summary()</code>, typically if I (author of package) can't tell if its intentional/</li> </ul> <p>If filenames follow different formats, scpviz will suggest possible <code>.obs</code> columns or default to generic labels.</p> <p>In this case, I recommend making a parsing function - e.g.  <code>parse_filenames</code> and reassigning to <code>.summary</code>. </p> <pre><code>def parse_filename_index(df):\n    \"\"\"\n    Parses the index of a DataFrame assumed to be filenames into structured metadata columns.\n\n    Expected filename format (delimited by \"_\"):\n        [0] date\n        [1] gradient\n        [2] sample_id\n        [3] size\n        [4] confirmation\n        [5] thickness\n        [6] sample\n        [7] organism\n        [8] region\n        [9] well_position\n\n    Args:\n        df (pd.DataFrame): DataFrame with index containing delimited filenames.\n\n    Returns:\n        pd.DataFrame: Original DataFrame with added metadata columns.\n    \"\"\"\n    colnames = [\n        'date',\n        'gradient',\n        'sample_id',\n        'size',\n        'confirmation',\n        'thickness',\n        'sample',\n        'organism',\n        'region',\n        'well_position'\n    ]\n\n    parts = df.index.to_series().str.split('_', expand=True)\n    if parts.shape[1] != len(colnames):\n        raise ValueError(f\"Expected {len(colnames)} parts after splitting index, got {parts.shape[1]}\")\n\n    df_parsed = df.copy()\n    for i, col in enumerate(colnames):\n        df_parsed[col] = parts.iloc[:, i]\n    return df_parsed\n</code></pre>"},{"location":"tutorials/importing/#export-results","title":"Export results","text":"<p>\u2014 save processed datasets, DE tables, or plots.</p>"},{"location":"tutorials/importing/#_1","title":"...","text":"<p>\u27a1\ufe0f Next: Filtering and Normalization</p>"},{"location":"tutorials/imputation/","title":"3. Imputation + Normalization","text":"<p>This tutorial is still under construction</p>"},{"location":"tutorials/imputation/#tutorial-3-imputation-and-normalization","title":"Tutorial 3: Imputation and Normalization","text":""},{"location":"tutorials/imputation/#imputation","title":"Imputation","text":"<p>Missing values are common in proteomics. scpviz provides several imputation methods.</p> <p>Note</p> <p>Pre-processing functions like normalize() and impute() act directly on the pAnnData object rather than returning a copy. By default, the processed data are written to the active .X layer unless a new layer name is specified.</p>"},{"location":"tutorials/imputation/#knn-imputation","title":"KNN Imputation","text":"<pre><code>pdata.impute(method=\"knn\", n_neighbors=5)\n</code></pre>"},{"location":"tutorials/imputation/#group-wise-imputation","title":"Group-wise Imputation","text":"<pre><code>pdata.impute(method=\"group_mean\", groupby=\"condition\")\n</code></pre>"},{"location":"tutorials/imputation/#_1","title":"3. Imputation + Normalization","text":""},{"location":"tutorials/imputation/#checking-imputation-stats","title":"Checking Imputation Stats","text":"<pre><code>pdata.stats[\"imputation\"]\n</code></pre> <p>\ud83d\udcca This dictionary stores how many values were imputed per group or overall.</p>"},{"location":"tutorials/imputation/#normalization","title":"Normalization","text":"<p>and apply normalization strategies \u2014 adjust for sample effects (global, reference feature, or directLFQ).  </p>"},{"location":"tutorials/imputation/#normalization_1","title":"Normalization","text":"<pre><code># Normalize intensities by global median\npdata.normalize(method=\"global\")\n\n# Normalize to reference proteins\npdata.normalize(method=\"reference_feature\", reference_columns=[\"ACTB\", \"GAPDH\"])\n</code></pre> <p>\ud83d\udca1 Note: Normalization choices can affect downstream DE and enrichment.</p>"},{"location":"tutorials/installation/","title":"Installation","text":"<p><code>scpviz</code> is distributed as a Python package and can be installed with <code>pip</code>.</p> <pre><code>pip install scpviz\n</code></pre> <p>This will install all required dependencies, including <code>scanpy</code>, <code>anndata</code>, <code>pandas</code>, and common plotting libraries.</p>"},{"location":"tutorials/installation/#development-installation","title":"Development installation","text":"<p>To install the latest development version directly from GitHub:</p> <pre><code>git clone https://github.com/gnaprs/scpviz.git\ncd scpviz\npip install -e .\n</code></pre> <p>This installs <code>scpviz</code> in editable mode, so any changes to the code will be reflected immediately. </p> <p>If you would like to contribute to <code>scpviz</code>, please see the Contributing Guide</p>"},{"location":"tutorials/installation/#dependencies","title":"Dependencies","text":"<ul> <li>Python \u2265 3.8</li> <li>Core scientific stack: <code>numpy</code>, <code>scipy</code>, <code>pandas</code> </li> <li>Data structures: <code>anndata</code>, <code>scanpy</code> </li> <li>Plotting: <code>matplotlib</code>, <code>seaborn</code>, <code>upsetplot</code> </li> <li>Network and enrichment: <code>requests</code> (for UniProt/STRING API access)  </li> </ul> <p>Optional dependencies</p> <p>Some external functions (e.g. Leiden clustering or directLFQ normalization) may require internet access or additional packages. These are noted in the corresponding tutorial pages.</p>"},{"location":"tutorials/plotting/","title":"4. Plotting","text":"<p>This tutorial is still under construction</p>"},{"location":"tutorials/plotting/#tutorial-4-plotting","title":"Tutorial 4: Plotting","text":"<p>Generate publication-ready plots with scpviz. \u2014 generate abundance plots, PCA/UMAP, clustermaps, raincloud, volcano plots.  </p> <p>Most plotting functions accept a <code>matplotlib.axes.Axes</code> object as the first argument, allowing seamless integration into multi-panel figures.</p>"},{"location":"tutorials/plotting/#abundance-plots","title":"Abundance Plots","text":"<p><pre><code>pdata.plot_abundance(\n    namelist=[\"ACTB\", \"GAPDH\"],\n    classes=\"condition\",\n    order=[\"control\", \"treated\"]\n)\n</code></pre> The <code>plot_abundance()</code> function automatically selects between barplots and violin plots (with inner points) depending on the number of samples per group.</p>"},{"location":"tutorials/plotting/#pca-and-umap","title":"PCA and UMAP","text":"<pre><code>pdata.plot_pca(classes=\"celltype\")\npdata.plot_umap(classes=\"condition\")\n</code></pre>"},{"location":"tutorials/plotting/#clustermap","title":"Clustermap","text":"<pre><code>pdata.plot_clustermap(namelist=[\"TP53\", \"VIM\", \"MAPT\"], classes=\"condition\")\n</code></pre> <p>\ud83c\udfa8 Colors automatically follow sample classes, but you can customize palettes.</p>"},{"location":"tutorials/quickstart/","title":"Quickstart","text":"<p>This quickstart tutorial demonstrates a minimal end-to-end workflow using scpviz.</p> <p>First, install <code>scpviz</code> and import the modules:</p> <pre><code>!pip install scpviz\n</code></pre> <pre><code>from scpviz import pAnnData as pAnnData\nfrom scpviz import plotting as scplt\nfrom scpviz import utils as scutils\n</code></pre>"},{"location":"tutorials/quickstart/#import","title":"Import","text":"<p>Proteomics data is stored in a <code>pAnnData</code> object. <code>scpviz</code> currently supports two data formats: Proteome Discoverer (Thermo Fisher) and DIA-NN reports. We\u2019ve provided sample test files below so you can follow along:</p> <p> </p> Proteome DiscovererDIA-NN Import Proteome Discoverer data<pre><code>obs_columns = ['Sample','cellline','treatment','condition','duration']\npdata = pAnnData.import_data(\n    source_type='pd', \n    prot_file = 'pd_prot.txt', \n    pep_file='pd32_PeptideSequenceGroups.txt',\n    obs_columns=obs_columns)\n</code></pre> output<pre><code>\ud83e\udded [USER] Importing data of type [pd]\n--------------------------\nStarting import [Proteome Discoverer]\n\nSource file: ../assets/pd32_Proteins.txt / ../assets/pd32_PeptideSequenceGroups.txt\nNumber of files: 12\nProteins: 10393\nPeptides: 167114\n    ...\n    \u2705 [OK] pAnnData object is valid.\n    \u2705 [OK] Import complete. Use `print(pdata)` to view the object.\n--------------------------\n</code></pre> Import DIA-NN data<pre><code>obs_columns = ['user', 'date', 'ms', 'acquisition', 'faims', 'column', 'gradient', 'amount', 'region', 'rep']\npdata = pAnnData.import_data(\n    source_type='diann', \n    report_file = 'diann_report.parquet', \n    obs_columns=obs_columns)\n</code></pre> output<pre><code>\ud83e\udded [USER] Importing data of type [diann]\n--------------------------\nStarting import [DIA-NN]\n\nSource file: ../assets/diann_report.parquet\nNumber of files: 12\nProteins: 12652\nPeptides: 251047\n    ...\n    \u2705 [OK] pAnnData object is valid.\n    \u2705 [OK] Import complete. Use `print(pdata)` to view the object.\n--------------------------\n</code></pre> <p>Note</p> <p>Refer to the Importing Data tutorial for more options, including defining metadata columns.</p> <p>We can have a quick look at our <code>pdata</code> object by checking <code>pdata.summary</code>:</p> Checking pdata.summary<pre><code>pdata.summary\n</code></pre> Sample cellline treatment condition duration ... protein_count ... peptide_count ... F4 AS RA kd d7 \u2026 9660 \u2026 139793 \u2026 F23 BE RA sc d7 \u2026 1724 \u2026 4534 \u2026 F24 BE RA sc d7 \u2026 9748 \u2026 133675 \u2026"},{"location":"tutorials/quickstart/#pre-processing","title":"Pre-processing","text":""},{"location":"tutorials/quickstart/#filtering","title":"Filtering","text":"<p>File F23 shows a markedly low protein count (~1700) compared to ~9000 in all other samples. This file is an outlier and should be removed prior to downstream analysis.</p> <p>Two filtering approaches can be used to achieve the same result:</p> Filter by protein countExclude by filename Filter by minimum protein count<pre><code>pdata = pdata.filter_sample(min_prot=8000)\n</code></pre> output<pre><code>\ud83e\udded [USER] Filtering samples [condition]:\n    Returning a copy of sample data based on condition:\n    \ud83d\udd38 Condition: protein_count &gt;= 8000\n    \u2139\ufe0f Auto-cleanup: No empty proteins found (all-NaN or all-zero).\n    \u2192 Samples kept: 11, Samples dropped: 1\n    \u2192 Proteins kept: 10393\n</code></pre> Filter by excluding specific file<pre><code>pdata = pdata.filter_sample(exclude_file_list=['F23'])\n</code></pre> output<pre><code>\ud83e\udded [USER] Filtering samples [file list]:\n    Returning a copy of sample data based on file list:\n    \ud83d\udd38 Files requested (excluding): 1\n    \u2139\ufe0f Auto-cleanup: No empty proteins found (all-NaN or all-zero).\n    \u2192 Samples kept: 11, Samples dropped: 1\n    \u2192 Proteins kept: 10393\n</code></pre> <p>After filtering, the dataset now contains 11 samples.  </p> <p>Note</p> <p>All filter functions return a copy of the <code>pAnnData</code> object unless <code>inplace=True</code> is specified. See the Filtering tutorial for more options.</p>"},{"location":"tutorials/quickstart/#quick-visualization","title":"Quick visualization","text":"<p>Visualize proteins of interest across samples:</p> Plot protein abundance<pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(4,4))\npdata.plot_abundance(ax, namelist=[\"GAPDH\", \"VCP\", \"AHNAK\"], classes=[\"cellline\",\"condition\"])\nplt.show()\n</code></pre> Protein abundance visualization for selected proteins. <p>Alternatively, we can explore the PCA embeddings to get an overview of sample clustering. Other dimensionality reduction methods such as UMAP and t-SNE can also be used.</p> Plot PCA embeddings<pre><code>fig, ax = plt.subplots(figsize = (4,4))\nax = scplt.plot_pca(ax, pdata, classes=[\"cellline\",\"condition\"], add_ellipses=True)\n</code></pre> <p>The samples appear well-clustered by both cell line and condition, suggesting good reproducibility and biological separation.</p> <p>Finally, we can examine the coefficient of variation (CV) of each sample. Here, the samples show overall low variability (median ~0.1), with slightly higher CVs observed in the AS_sc group (~0.35).</p> Plot sample CVs<pre><code>fig, ax = plt.subplots(figsize = (4,4))\nax = scplt.plot_cv(ax, pdata, classes=[\"cellline\",\"condition\"])\n</code></pre> <p>Note</p> <p>Refer to the plotting tutorial for more advanced plotting options.</p>"},{"location":"tutorials/quickstart/#normalization-and-imputation","title":"Normalization and Imputation","text":"<p>Biological and technical variation across samples (e.g., in the AS_sc group) can arise from sample processing or data acquisition. Normalize your data to reduce variation between samples \u2014 for example, using median scaling.  </p> Normalization and imputation<pre><code>pdata.normalize(method=\"median\")\npdata.impute(method=\"min\")\n</code></pre> output<pre><code>\ud83e\udded [USER] Global normalization using 'median'. Layer will be saved as 'X_norm_median'.\n     \u2705 Normalized all 11 samples.\n     \u2139\ufe0f Set protein data to layer X_norm_median.\n\ud83e\udded [USER] Global imputation using 'min'. Layer saved as 'X_impute_min'. Minimum scaled by 1.\n     \u2705 8234 values imputed.\n     \u2139\ufe0f 11 samples fully imputed, 0 samples partially imputed, 0 skipped feature(s) with all missing values.\n     \u2139\ufe0f Set protein data to layer X_impute_min.\n</code></pre> <p>After normalization, CVs for the AS_sc group improve compared to pre-normalized data.</p> Plot sample CVs after normalization<pre><code>fig, ax = plt.subplots(figsize = (4,4))\nax = scplt.plot_cv(ax, pdata, classes=[\"cellline\",\"condition\"])\n</code></pre> Sample-wise CVs after normalization and imputation. <p>Other imputation methods are also available, including KNN, median, and minimum with a scaling factor. </p> <p>Note</p> <p>Refer to the Normalization &amp; Imputation tutorial for additional examples and parameter options, such as Harmony or DirectLFQ.</p>"},{"location":"tutorials/quickstart/#differential-expression","title":"Differential expression","text":""},{"location":"tutorials/quickstart/#volcano-plots","title":"Volcano Plots","text":"<p>Run a differential expression (DE) analysis, commonly visualized with volcano plots. To start, we define a comparison ratio: for instance, comparing cell line BE under the kd condition against cell line BE under sc.</p> Differentiatial expression with volcano plots<pre><code>fig, ax = plt.subplots(figsize=(4,4))\ncomparison_values=[{'cellline':'BE', 'condition':'kd'},{'cellline':'BE', 'condition':'sc'}]\nax = scplt.plot_volcano(ax, pdata, values=comparison_values)\n</code></pre> output<pre><code>\ud83e\udded [USER] Running differential expression [protein]\n   \ud83d\udd38 Comparing groups: [{'cellline': 'BE', 'condition': 'kd'}] vs [{'cellline': 'BE', 'condition': 'sc'}]\n   \ud83d\udd38 Group sizes: 3 vs 2 samples\n   \ud83d\udd38 Method: ttest | Fold Change: mean | Layer: X\n   \ud83d\udd38 P-value threshold: 0.05 | Log2FC threshold: 1\n     \u2705 DE complete. Results stored in:\n       \u2022 .stats[\"[{'cellline': 'BE', 'condition': 'kd'}] vs [{'cellline': 'BE', 'condition': 'sc'}]\"]\n       \u2022 Columns: log2fc, p_value, significance, etc.\n       \u2022 Upregulated: 279 | Downregulated: 258 | Not significant: 9856\n</code></pre> Volcano plot highlighting upregulated and downregulated proteins. <p>Access the DE results stored in <code>.stats</code> under the key shown in the output.</p> Access DE results<pre><code>pdata.stats[\"[{'cellline': 'BE', 'condition': 'kd'}] vs [{'cellline': 'BE', 'condition': 'sc'}]\"].head(8)\n</code></pre> Genes [{'cellline': 'BE', 'condition': 'kd'}] [{'cellline': 'BE', 'condition': 'sc'}] log2fc p_value test_statistic significance_score significance PPP1R37 601891.3155 103551.4786 2.54 0.0118 5.51 4.90 upregulated IGSF9B 438967.4093 193638.7087 1.18 0.0159 4.94 2.12 upregulated GPR161 126213.0252 54380.4809 1.21 0.0102 5.81 2.42 upregulated TIGD5 43568.9795 9048.0415 2.27 0.0230 4.31 3.71 upregulated TTC9B 222577.1287 26482.7005 3.07 1.81e-05 49.58 14.57 upregulated NMNAT2 269130.1753 82171.6244 1.71 0.0046 7.69 4.01 upregulated ATXN7L1 254803.2800 66922.2053 1.93 0.0123 5.42 3.68 upregulated SASS6 1765918.661 779926.980 1.18 0.0365 3.61 1.69 upregulated \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 <p>The table above shows the top DE results (<code>df.head(8)</code>) including log\u2082 fold change, p-value, and significance score.</p>"},{"location":"tutorials/quickstart/#string-enrichment","title":"STRING enrichment","text":"<p>We can perform STRING enrichment on the sets of up- and downregulated proteins from our DE analysis. First, list the available enrichment keys:</p> List enrichment keys<pre><code>pdata.list_enrichments()\n</code></pre> output<pre><code>\ud83e\udded [USER] Listing STRING enrichment status\n\n     \u2139\ufe0f Available DE comparisons (not yet enriched):\n        - BE_kd vs BE_sc\n\n  \ud83d\udd39 To run enrichment:\n      pdata.enrichment_functional(from_de=True, de_key=\"...\")\n\n\u2705 Completed STRING enrichment results:\n    (none)\n\n\u2705 Completed STRING PPI results:\n    (none)\n</code></pre> <p>Since we just ran a DE analysis, the key <code>BE_kd vs BE_sc</code> is available. We can run STRING functional enrichment on both up- and downregulated proteins.</p> STRING functional enrichment<pre><code>pdata.enrichment_functional(from_de=True, de_key=\"BE_kd vs BE_sc\")\n</code></pre> output<pre><code>\ud83e\udded [USER] Running STRING enrichment [DE-based: [{'cellline': 'BE', 'condition': 'kd'}] vs [{'cellline': 'BE', 'condition': 'sc'}]]\n\n\ud83d\udd39 Up-regulated proteins\n     \u2139\ufe0f Found 0 cached STRING IDs. 150 need lookup.\n          \u2139\ufe0f Cached 149 STRING IDs from UniProt API xref_string.\n          \u26a0\ufe0f No STRING mappings returned from STRING API.\n   \ud83d\udd38 Proteins: 150 \u2192 STRING IDs: 149\n   \ud83d\udd38 Species: 9606 | Background: None\n     \u2705 [OK] Enrichment complete (3.92s)\n   \u2022 Access result: pdata.stats['functional'][\"BE_kd vs BE_sc_up\"][\"result\"]\n   \u2022 Plot command : pdata.plot_enrichment_svg(\"BE_kd vs BE_sc\", direction=\"up\")\n   \u2022 View online  : https://string-db.org/cgi/network?identifiers=9606.ENSP00000351310%0d9606....ENSP00000265018&amp;caller_identity=scpviz&amp;species=9606&amp;show_query_node_labels=1\n\n\n\ud83d\udd39 Down-regulated proteins\n     \u2139\ufe0f Found 0 cached STRING IDs. 150 need lookup.\n          \u2139\ufe0f Cached 149 STRING IDs from UniProt API xref_string.\n          \u26a0\ufe0f No STRING mappings returned from STRING API.\n   \ud83d\udd38 Proteins: 150 \u2192 STRING IDs: 149\n   \ud83d\udd38 Species: 9606 | Background: None\n     \u2705 [OK] Enrichment complete (2.35s)\n   \u2022 Access result: pdata.stats['functional'][\"BE_kd vs BE_sc_down\"][\"result\"]\n   \u2022 Plot command : pdata.plot_enrichment_svg(\"BE_kd vs BE_sc\", direction=\"down\")\n   \u2022 View online  : https://string-db.org/cgi/network?identifiers=9606.ENSP00000368678%0d9606.....ENSP00000263512%0d9606.ENSP00000382767%0d9606&amp;caller_identity=scpviz&amp;species=9606&amp;show_query_node_labels=1\n</code></pre> <p>Once enrichment is complete, you can visualize the Gene Ontology (Biological Process) results:</p> Plot sample CVs<pre><code>pdata.plot_enrichment_svg(\"BE_kd vs BE_sc\", direction=\"down\")\n</code></pre> <p>Note</p> <p>Refer to the Enrichment tutorial for details on additional STRING features such as PPI networks, GSEA, and combined enrichment-PPI analyses.</p>"},{"location":"tutorials/quickstart/#next-steps","title":"Next steps","text":"<p>For a complete workflow \u2014 from importing data to enrichment and network analysis \u2014 see the Tutorial Index.</p> <ul> <li>Learn more about data import in Importing Data.  </li> <li>Explore filtering options in Filtering.  </li> <li>Explore normalization and imputation options in Normalization &amp; Imputation.  </li> <li>Learn about differential expression and volcano plots in Differential Expression.  </li> <li>Perform functional and PPI enrichment in STRING Enrichment.  </li> <li>See advanced visualization techniques in the Plotting Tutorial.</li> </ul>"},{"location":"tutorials/single_cell/","title":"Single-cell Example","text":"<p>This tutorial is still under construction</p>"},{"location":"tutorials/single_cell/#single-cell","title":"single cell","text":"<p>Handling single cell data is slightly different from bulk data. More sparse, missing values, so need to handle normalization and imputation more carefully. Here, we go through some typical workflows for single cell proteomics (field is still being established!)</p>"},{"location":"tutorials/single_cell/#preprocessing","title":"preprocessing","text":""},{"location":"tutorials/single_cell/#filter-proteins","title":"filter proteins","text":"<p>Filter by significance, valid genes and for unique profiles (due to single cell data having high missing values, a lot of peptides are often missing - this means often isoforms will have duplicated abundance profiles since there are no unique peptides distinguishing the isoforms.)</p> <pre><code>pdata_filtered = pdata.filter_prot(valid_genes=True, unique_profiles=True)\npdata_filtered = pdata_filtered.filter_prot_significance()\n</code></pre> <p>I also suggest doing a 40% filter for interested groups followed by minimum value imputation <pre><code>pdata_filtered = pdata.filter_prot_found(groups='sample', min_ratio = 0.4)\npdata_filtered.impute(method='min', min_value = 0.2)\n</code></pre></p>"},{"location":"tutorials/single_cell/#normalization","title":"normalization","text":"<p>typically bulk proteomics is median normalization</p> <p>there's also directlfq algorithm that we can use, is implemented by doing <pre><code>pdata.normalize(method='directlfq')\n</code></pre></p> <p>Note</p> <p>this algorithm will create files in the workspace, and also might take awhile</p>"},{"location":"tutorials/single_cell/#visualize-data","title":"visualize data","text":"<p>typically done with umap</p> <p>can also use tsne plot</p>"},{"location":"tutorials/single_cell/#using-scanpy","title":"using scanpy","text":"<p>need to first use cleanup function to make data clean for scanpy (expects 0, not NaNs, will throw error otherwise) <pre><code>pdata.clean_X()\n</code></pre></p> <p>scanpy expects AnnData objects, so we send in the .prot objects (after we have done filtering with eg pep)</p> <pre><code>pdata.filter_rs(unique_pep=2)\nprot = pdata.prot\nsc.tsne(prot)\n</code></pre>"}]}