{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "accession_list = [\"P49411\",\n",
    "\"P11586\",\n",
    "\"Q9P2E9-1\",\n",
    "\"Q9H307\",\n",
    "\"Q92616\",\n",
    "\"Q9NX58\",\n",
    "\"Q99873\",\n",
    "\"O43493-1\",\n",
    "\"Q10567\",\n",
    "\"O43169\",\n",
    "\"O00170\",\n",
    "\"O75477\",\n",
    "\"Q15165-1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import zlib\n",
    "from xml.etree import ElementTree\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "\n",
    "POLLING_INTERVAL = 3\n",
    "API_URL = \"https://rest.uniprot.org\"\n",
    "\n",
    "\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "\n",
    "def check_response(response):\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.HTTPError:\n",
    "        print(response.json())\n",
    "        raise\n",
    "\n",
    "\n",
    "def submit_id_mapping(from_db, to_db, ids):\n",
    "    request = requests.post(\n",
    "        f\"{API_URL}/idmapping/run\",\n",
    "        data={\"from\": from_db, \"to\": to_db, \"ids\": \",\".join(ids)},\n",
    "    )\n",
    "    check_response(request)\n",
    "    return request.json()[\"jobId\"]\n",
    "\n",
    "\n",
    "def get_next_link(headers):\n",
    "    re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "\n",
    "def check_id_mapping_results_ready(job_id):\n",
    "    while True:\n",
    "        request = session.get(f\"{API_URL}/idmapping/status/{job_id}\")\n",
    "        check_response(request)\n",
    "        j = request.json()\n",
    "        if \"jobStatus\" in j:\n",
    "            if j[\"jobStatus\"] == \"RUNNING\":\n",
    "                print(f\"Retrying in {POLLING_INTERVAL}s\")\n",
    "                time.sleep(POLLING_INTERVAL)\n",
    "            else:\n",
    "                raise Exception(j[\"jobStatus\"])\n",
    "        else:\n",
    "            return bool(j[\"results\"] or j[\"failedIds\"])\n",
    "\n",
    "\n",
    "def get_batch(batch_response, file_format, compressed):\n",
    "    batch_url = get_next_link(batch_response.headers)\n",
    "    while batch_url:\n",
    "        batch_response = session.get(batch_url)\n",
    "        batch_response.raise_for_status()\n",
    "        yield decode_results(batch_response, file_format, compressed)\n",
    "        batch_url = get_next_link(batch_response.headers)\n",
    "\n",
    "\n",
    "def combine_batches(all_results, batch_results, file_format):\n",
    "    if file_format == \"json\":\n",
    "        for key in (\"results\", \"failedIds\"):\n",
    "            if key in batch_results and batch_results[key]:\n",
    "                all_results[key] += batch_results[key]\n",
    "    elif file_format == \"tsv\":\n",
    "        return all_results + batch_results[1:]\n",
    "    else:\n",
    "        return all_results + batch_results\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def get_id_mapping_results_link(job_id):\n",
    "    url = f\"{API_URL}/idmapping/details/{job_id}\"\n",
    "    request = session.get(url)\n",
    "    check_response(request)\n",
    "    return request.json()[\"redirectURL\"]\n",
    "\n",
    "\n",
    "def decode_results(response, file_format, compressed):\n",
    "    if compressed:\n",
    "        decompressed = zlib.decompress(response.content, 16 + zlib.MAX_WBITS)\n",
    "        if file_format == \"json\":\n",
    "            j = json.loads(decompressed.decode(\"utf-8\"))\n",
    "            return j\n",
    "        elif file_format == \"tsv\":\n",
    "            return [line for line in decompressed.decode(\"utf-8\").split(\"\\n\") if line]\n",
    "        elif file_format == \"xlsx\":\n",
    "            return [decompressed]\n",
    "        elif file_format == \"xml\":\n",
    "            return [decompressed.decode(\"utf-8\")]\n",
    "        else:\n",
    "            return decompressed.decode(\"utf-8\")\n",
    "    elif file_format == \"json\":\n",
    "        return response.json()\n",
    "    elif file_format == \"tsv\":\n",
    "        return [line for line in response.text.split(\"\\n\") if line]\n",
    "    elif file_format == \"xlsx\":\n",
    "        return [response.content]\n",
    "    elif file_format == \"xml\":\n",
    "        return [response.text]\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def get_xml_namespace(element):\n",
    "    m = re.match(r\"\\{(.*)\\}\", element.tag)\n",
    "    return m.groups()[0] if m else \"\"\n",
    "\n",
    "\n",
    "def merge_xml_results(xml_results):\n",
    "    merged_root = ElementTree.fromstring(xml_results[0])\n",
    "    for result in xml_results[1:]:\n",
    "        root = ElementTree.fromstring(result)\n",
    "        for child in root.findall(\"{http://uniprot.org/uniprot}entry\"):\n",
    "            merged_root.insert(-1, child)\n",
    "    ElementTree.register_namespace(\"\", get_xml_namespace(merged_root[0]))\n",
    "    return ElementTree.tostring(merged_root, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "\n",
    "def print_progress_batches(batch_index, size, total):\n",
    "    n_fetched = min((batch_index + 1) * size, total)\n",
    "    print(f\"Fetched: {n_fetched} / {total}\")\n",
    "\n",
    "\n",
    "def get_id_mapping_results_search(url):\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    file_format = query[\"format\"][0] if \"format\" in query else \"json\"\n",
    "    if \"size\" in query:\n",
    "        size = int(query[\"size\"][0])\n",
    "    else:\n",
    "        size = 500\n",
    "        query[\"size\"] = size\n",
    "    compressed = (\n",
    "        query[\"compressed\"][0].lower() == \"true\" if \"compressed\" in query else False\n",
    "    )\n",
    "    parsed = parsed._replace(query=urlencode(query, doseq=True))\n",
    "    url = parsed.geturl()\n",
    "    request = session.get(url)\n",
    "    check_response(request)\n",
    "    results = decode_results(request, file_format, compressed)\n",
    "    total = int(request.headers[\"x-total-results\"])\n",
    "    print_progress_batches(0, size, total)\n",
    "    for i, batch in enumerate(get_batch(request, file_format, compressed), 1):\n",
    "        results = combine_batches(results, batch, file_format)\n",
    "        print_progress_batches(i, size, total)\n",
    "    if file_format == \"xml\":\n",
    "        return merge_xml_results(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_id_mapping_results_stream(url):\n",
    "    if \"/stream/\" not in url:\n",
    "        url = url.replace(\"/results/\", \"/results/stream/\")\n",
    "    request = session.get(url)\n",
    "    check_response(request)\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    file_format = query[\"format\"][0] if \"format\" in query else \"json\"\n",
    "    compressed = (\n",
    "        query[\"compressed\"][0].lower() == \"true\" if \"compressed\" in query else False\n",
    "    )\n",
    "    return decode_results(request, file_format, compressed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: 10 / 10\n",
      "{'results': [{'from': 'P49411', 'to': 'TUFM'}, {'from': 'P11586', 'to': 'MTHFD1'}, {'from': 'Q9H307', 'to': 'PNN'}, {'from': 'Q92616', 'to': 'GCN1'}, {'from': 'Q9NX58', 'to': 'LYAR'}, {'from': 'Q99873', 'to': 'PRMT1'}, {'from': 'Q10567', 'to': 'AP1B1'}, {'from': 'O43169', 'to': 'CYB5B'}, {'from': 'O00170', 'to': 'AIP'}, {'from': 'O75477', 'to': 'ERLIN1'}], 'failedIds': ['Q9P2E9-1', 'Q15165-1', 'O43493-1']}\n"
     ]
    }
   ],
   "source": [
    "job_id = submit_id_mapping(\n",
    "    from_db=\"UniProtKB_AC-ID\", to_db=\"Gene_Name\", ids=accession_list\n",
    ")\n",
    "if check_id_mapping_results_ready(job_id):\n",
    "    link = get_id_mapping_results_link(job_id)\n",
    "    results = get_id_mapping_results_search(link)\n",
    "    # Equivalently using the stream endpoint which is more demanding\n",
    "    # on the API and so is less stable:\n",
    "    # results = get_id_mapping_results_stream(link)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'P49411', 'to': 'TUFM'},\n",
       " {'from': 'P11586', 'to': 'MTHFD1'},\n",
       " {'from': 'Q9H307', 'to': 'PNN'},\n",
       " {'from': 'Q92616', 'to': 'GCN1'},\n",
       " {'from': 'Q9NX58', 'to': 'LYAR'},\n",
       " {'from': 'Q99873', 'to': 'PRMT1'},\n",
       " {'from': 'Q10567', 'to': 'AP1B1'},\n",
       " {'from': 'O43169', 'to': 'CYB5B'},\n",
       " {'from': 'O00170', 'to': 'AIP'},\n",
       " {'from': 'O75477', 'to': 'ERLIN1'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q9P2E9-1', 'Q15165-1', 'O43493-1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['failedIds']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
