{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scviz import utils\n",
    "from scviz import setup\n",
    "\n",
    "# A = [[7,2,3],[4,np.nan,6],[10,5,np.nan],[np.nan,np.nan,2]]\n",
    "# print(A)\n",
    "# print(\"\\n\\n\")\n",
    "# column_trans = ColumnTransformer(\n",
    "# [('imp_col1', SimpleImputer(strategy='mean'), [1]),\n",
    "#  ('imp_col2', SimpleImputer(strategy='constant', fill_value=29), [2,3])],\n",
    "# remainder='passthrough')\n",
    "\n",
    "# print(column_trans.fit_transform(A)[:, [2,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(self, classes=None, layer=\"X\", method='mean', on='protein', set_X=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Impute missing values across samples (globally or within classes) using SimpleImputer.\n",
    "\n",
    "    Parameters:\n",
    "        classes (str or list): Class columns in .obs to group by.\n",
    "        layer (str): Data layer to impute from.\n",
    "        method (str): 'mean', 'median', or 'min'.\n",
    "        on (str): 'protein' or 'peptide'.\n",
    "        set_X (bool): Whether to set .X to the imputed result.\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer, KNNImputer\n",
    "    from scipy import sparse\n",
    "    from scviz import utils\n",
    "\n",
    "\n",
    "    if not self._check_data(on):\n",
    "        return\n",
    "\n",
    "    adata = self.prot if on == 'protein' else self.pep\n",
    "    if layer != \"X\" and layer not in adata.layers:\n",
    "        raise ValueError(f\"Layer '{layer}' not found in .{on}.\")\n",
    "\n",
    "    impute_data = adata.layers[layer] if layer != \"X\" else adata.X\n",
    "    was_sparse = sparse.issparse(impute_data)\n",
    "    impute_data = impute_data.toarray() if was_sparse else impute_data.copy()\n",
    "    original_data = impute_data.copy()\n",
    "\n",
    "    layer_name = f\"X_impute_{method}\"\n",
    "\n",
    "    if method not in {\"mean\", \"median\", \"min\",\"knn\"}:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "    if classes is None:\n",
    "        # Global imputation\n",
    "        if method == 'min':\n",
    "            min_vals = np.nanmin(impute_data, axis=0)\n",
    "            min_vals = np.where(np.isnan(min_vals), 0, min_vals)\n",
    "            mask = np.isnan(impute_data)\n",
    "            impute_data[mask] = np.take(min_vals, np.where(mask)[1])\n",
    "        elif method == 'knn':\n",
    "            n_neighbors = kwargs.get('n_neighbors', 3)\n",
    "            imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "            impute_data = imputer.fit_transform(impute_data)\n",
    "        else:\n",
    "            imputer = SimpleImputer(strategy=method)\n",
    "            impute_data = imputer.fit_transform(impute_data)\n",
    "\n",
    "        print(f\"ℹ️ Global imputation using '{method}'. Layer saved as '{layer_name}'.\")\n",
    "\n",
    "    else:\n",
    "        # Group-wise imputation\n",
    "        if method == 'knn':\n",
    "            raise ValueError(\"KNN imputation is not supported for group-wise imputation.\")\n",
    "\n",
    "        sample_names = utils.get_samplenames(adata, classes)\n",
    "        sample_names = np.array(sample_names)\n",
    "        unique_groups = np.unique(sample_names)\n",
    "\n",
    "        for group in unique_groups:\n",
    "            idx = np.where(sample_names == group)[0]\n",
    "            group_data = impute_data[idx, :]\n",
    "\n",
    "            if method == 'min':\n",
    "                min_vals = np.nanmin(group_data, axis=0)\n",
    "                min_vals = np.where(np.isnan(min_vals), 0, min_vals)\n",
    "                mask = np.isnan(group_data)\n",
    "                group_data[mask] = np.take(min_vals, np.where(mask)[1])\n",
    "                imputed_group = group_data\n",
    "            else:\n",
    "                imputer = SimpleImputer(strategy=method)\n",
    "                imputed_group = imputer.fit_transform(group_data)\n",
    "\n",
    "            impute_data[idx, :] = imputed_group\n",
    "\n",
    "        print(f\"ℹ️ Group-wise imputation using '{method}' on class(es): {classes}. Layer saved as '{layer_name}'.\")\n",
    "\n",
    "    summary_lines = []\n",
    "    if classes is None:\n",
    "        num_imputed = np.sum(np.isnan(original_data) & ~np.isnan(impute_data))\n",
    "        summary_lines.append(f\"✅ {num_imputed} values imputed.\")\n",
    "    else:\n",
    "        sample_names = utils.get_samplenames(adata, classes)\n",
    "        sample_names = np.array(sample_names)\n",
    "        unique_groups = np.unique(sample_names)\n",
    "\n",
    "        counts_by_group = {}\n",
    "        for group in unique_groups:\n",
    "            idx = np.where(sample_names == group)[0]\n",
    "            before = original_data[idx, :]\n",
    "            after = impute_data[idx, :]\n",
    "            mask = np.isnan(before) & ~np.isnan(after)\n",
    "            counts_by_group[group] = np.sum(mask)\n",
    "\n",
    "        total = sum(counts_by_group.values())\n",
    "        summary_lines.append(f\"✅ {total} values imputed total.\")\n",
    "        for group, count in counts_by_group.items():\n",
    "            summary_lines.append(f\"   - {group}: {count} values\")\n",
    "\n",
    "    print(\"\\n\".join(summary_lines))\n",
    "\n",
    "    adata.layers[layer_name] = sparse.csr_matrix(impute_data) if was_sparse else impute_data\n",
    "\n",
    "    if set_X:\n",
    "        self.set_X(layer=layer_name, on=on)\n",
    "\n",
    "    self._history.append(\n",
    "        f\"{on}: Imputed layer '{layer}' using '{method}' (grouped by {classes if classes else 'ALL'}). Stored in '{layer_name}'.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import f\n",
    "\n",
    "\n",
    "def normalize(self, classes = None, layer = \"X\", method = 'sum', on = 'protein', set_X = True, force = False, use_nonmissing = False, **kwargs):  \n",
    "    \"\"\" \n",
    "    Normalize the data across samples (globally or within groups).\n",
    "\n",
    "    Parameters:\n",
    "    - classes (str or list): Sample-level class/grouping column(s) in .obs.\n",
    "    - layer (str): Data layer to normalize from (default='X').\n",
    "    - method (str): Normalization method. Options: 'sum', 'median', 'mean', 'max', 'reference_feature', 'robust_scale', 'quantile_transform'.\n",
    "    - on (str): 'protein' or 'peptide'.\n",
    "    - set_X (bool): Whether to set .X to the normalized result.\n",
    "    - force (bool): Whether to force normalization even with bad rows.\n",
    "    - use_nonmissing (bool): Whether to use only fully observed columns for normalization.\n",
    "    - **kwargs: Additional arguments for normalization methods.\n",
    "        (e.g., reference_columns for 'reference_feature', n_neighbors for 'knn').\n",
    "        max_missing_fraction: Maximum fraction of missing values allowed in a row. Default is 0.5.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if not self._check_data(on):\n",
    "        return\n",
    "\n",
    "    adata = self.prot if on == 'protein' else self.pep\n",
    "    if layer != \"X\" and layer not in adata.layers:\n",
    "        raise ValueError(f\"Layer {layer} not found in .{on}.\")\n",
    "    \n",
    "    normalize_data = adata.layers[layer] if layer != \"X\" else adata.X\n",
    "    was_sparse = sparse.issparse(normalize_data)\n",
    "    normalize_data = normalize_data.toarray() if was_sparse else normalize_data.copy()\n",
    "    original_data = normalize_data.copy()\n",
    "\n",
    "    # Check for bad rows (too many missing values)\n",
    "    missing_fraction = np.isnan(normalize_data).sum(axis=1) / normalize_data.shape[1]\n",
    "    max_missing_fraction = kwargs.pop(\"max_missing_fraction\", 0.5)\n",
    "    bad_rows_mask = missing_fraction > max_missing_fraction\n",
    "\n",
    "    if np.any(bad_rows_mask):\n",
    "        n_bad = np.sum(bad_rows_mask)\n",
    "        print(f\"⚠️ {n_bad} sample(s) have >{int(max_missing_fraction*100)}% missing values.\")\n",
    "        print(\"   Suggest running `.impute()` before normalization for more stable results.\")\n",
    "        print(\"   Alternatively, try `use_nonmissing=True` to normalize using only consistently observed proteins.\")\n",
    "        if not force:\n",
    "            print(\"   ➡️ Use `force=True` to proceed anyway.\")\n",
    "            return\n",
    "\n",
    "    layer_name = 'X_norm_' + method\n",
    "    normalize_funcs = ['sum', 'median', 'mean', 'max', 'reference_feature', 'robust_scale', 'quantile_transform']\n",
    "\n",
    "    if method not in normalize_funcs:\n",
    "        raise ValueError(f\"Unsupported normalization method: {method}\")\n",
    "\n",
    "    if classes is None:\n",
    "        normalize_data = self._normalize_helper(normalize_data, method, use_nonmissing=use_nonmissing, **kwargs)\n",
    "        msg=f\"ℹ️ Global normalization using '{method}'\"\n",
    "    else:\n",
    "        # Group-wise normalization\n",
    "        sample_names = utils.get_samplenames(adata, classes)\n",
    "        sample_names = np.array(sample_names)\n",
    "        unique_groups = np.unique(sample_names)\n",
    "\n",
    "        for group in unique_groups:\n",
    "            idx = np.where(sample_names == group)[0]\n",
    "            group_data = normalize_data[idx, :]\n",
    "\n",
    "            normalized_group = self._normalize_helper(group_data, method=method, use_nonmissing=use_nonmissing, **kwargs)\n",
    "            normalize_data[idx, :] = normalized_group\n",
    "\n",
    "        msg=f\"ℹ️ Group-wise normalization using '{method}' on class(es): {classes}\"\n",
    "\n",
    "    if use_nonmissing and method in {'sum', 'mean', 'median', 'max'}:\n",
    "        msg += f\" (using only fully observed columns)\"\n",
    "\n",
    "    msg += f\". Layer saved as '{layer_name}'.\"\n",
    "    print(msg)\n",
    "\n",
    "    # summary printout\n",
    "    summary_lines = []\n",
    "    if classes is None:\n",
    "        summary_lines.append(f\"✅ Normalized all {normalize_data.shape[0]} samples.\")\n",
    "    else:\n",
    "        for group in unique_groups:\n",
    "            count = np.sum(sample_names == group)\n",
    "            summary_lines.append(f\"   - {group}: {count} samples normalized\")\n",
    "        summary_lines.insert(0, f\"✅ Normalized {normalize_data.shape[0]} samples total.\")\n",
    "    print(\"\\n\".join(summary_lines))            \n",
    "\n",
    "    adata.layers[layer_name] = sparse.csr_matrix(normalize_data) if was_sparse else normalize_data\n",
    "\n",
    "    if set_X:\n",
    "        self.set_X(layer = layer_name, on = on)\n",
    "\n",
    "    # Determine if use_nonmissing note should be added\n",
    "    note = \"\"\n",
    "    if kwargs.get(\"use_nonmissing\", False) and method in {'sum', 'mean', 'median', 'max'}:\n",
    "        note = \" (using only fully observed columns)\"\n",
    "\n",
    "    self._history.append(\n",
    "        f\"{on}: Normalized layer {layer} using {method}{note} (grouped by {classes}). Stored in `{layer_name}`.\"\n",
    "    )\n",
    "\n",
    "def _normalize_helper(self, data, method, use_nonmissing, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function for normalization methods.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): Data to normalize.\n",
    "    - method (str): Normalization method.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Normalized data.\n",
    "    \"\"\"\n",
    "\n",
    "    if method in {'sum', 'mean', 'median', 'max'}:\n",
    "        reducer = {\n",
    "                'sum': np.nansum,\n",
    "                'mean': np.nanmean,\n",
    "                'median': np.nanmedian,\n",
    "                'max': np.nanmax\n",
    "            }[method]\n",
    "\n",
    "        if use_nonmissing:\n",
    "            fully_observed_cols = ~np.isnan(data).any(axis=0)\n",
    "            if not np.any(fully_observed_cols):\n",
    "                raise ValueError(\"No fully observed columns available for normalization with `use_nonmissing=True`.\")\n",
    "            used_cols = np.where(fully_observed_cols)[0]\n",
    "            print(f\"ℹ️ Normalizing using only fully observed columns: {used_cols}\")\n",
    "            row_vals = reducer(data[:, fully_observed_cols], axis=1)\n",
    "        else:\n",
    "            row_vals = reducer(data, axis=1)\n",
    "\n",
    "        scale = np.nanmax(row_vals) / row_vals\n",
    "        scale = np.where(np.isnan(scale), 1.0, scale)\n",
    "        data_norm = data * scale[:, None]\n",
    "\n",
    "    elif method == 'reference_feature':\n",
    "        # norm by reference feature: scale each row s.t. the reference column is the same across all rows (scale to max value of reference column)\n",
    "        reference_columns = kwargs.get('reference_columns', [2])\n",
    "        reference_method = kwargs.get('reference_method', 'median')  # default to median\n",
    "\n",
    "        reducer_map = {\n",
    "            'mean': np.nanmean,\n",
    "            'median': np.nanmedian,\n",
    "            'sum': np.nansum\n",
    "        }\n",
    "\n",
    "        if reference_method not in reducer_map:\n",
    "            raise ValueError(f\"Unsupported reference method: {reference_method}. Supported methods are: {list(reducer_map.keys())}\")\n",
    "        reducer = reducer_map[reference_method]\n",
    "\n",
    "        # resolve reference column names if needed\n",
    "        if isinstance(reference_columns[0], str):\n",
    "            gene_to_acc, _ = self.get_gene_maps(on='protein')\n",
    "            resolved = utils.resolve_accessions(self.prot, reference_columns, gene_map=gene_to_acc)\n",
    "            reference_acc = [ref for ref in resolved if ref in self.prot.var.index]\n",
    "            reference_columns = [self.prot.var.index.get_loc(ref) for ref in reference_acc]\n",
    "            print(f\"ℹ️ Normalizing using found reference columns: {reference_acc}\")\n",
    "            self._history.append(f\"Used reference_feature normalization with resolved accessions: {resolved}\")\n",
    "        else:\n",
    "            reference_columns = [int(ref) for ref in reference_columns]\n",
    "            reference_acc = [self.prot.var.index[ref] for ref in reference_columns if ref < self.prot.shape[1]]\n",
    "            print(f\"ℹ️ Normalizing using reference columns: {reference_acc}\")\n",
    "            self._history.append(f\"Used reference_feature normalization with resolved accessions: {reference_acc}\")\n",
    "\n",
    "        scaling_factors = np.nanmean(np.nanmax(data[:, reference_columns], axis=0) / (data[:, reference_columns]), axis=1)\n",
    "\n",
    "        nan_rows = np.where(np.isnan(scaling_factors))[0]\n",
    "        if nan_rows.size > 0:\n",
    "            print(f\"⚠️ Rows {list(nan_rows)} have all missing reference values.\")\n",
    "            print(\"   ➡️ Falling back to row median normalization for these rows.\")\n",
    "            \n",
    "            fallback = np.nanmedian(data[nan_rows, :], axis=1)\n",
    "            fallback[fallback == 0] = np.nan  # avoid division by 0\n",
    "            fallback_scale = np.nanmax(fallback) / fallback\n",
    "            fallback_scale = np.where(np.isnan(fallback_scale), 1.0, fallback_scale)  # default to 1.0 if all else fails\n",
    "\n",
    "            scaling_factors[nan_rows] = fallback_scale\n",
    "\n",
    "        scaling_factors = np.where(np.isnan(scaling_factors), np.nanmean(scaling_factors), scaling_factors)\n",
    "        data_norm = data * scaling_factors[:, None]\n",
    "\n",
    "    elif method == 'robust_scale':\n",
    "        # norm by robust_scale: Center to the median and component wise scale according to the interquartile range. See sklearn.preprocessing.robust_scale for more information.\n",
    "        from sklearn.preprocessing import robust_scale\n",
    "        data_norm = robust_scale(data, axis=1)\n",
    "\n",
    "    elif method == 'quantile_transform':\n",
    "        # norm by quantile_transform: Transform features using quantiles information. See sklearn.preprocessing.quantile_transform for more information.\n",
    "        from sklearn.preprocessing import quantile_transform\n",
    "        data_norm = quantile_transform(data, axis=1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from anndata import AnnData\n",
    "from scipy import sparse\n",
    "\n",
    "# ---- STEP 1: Create toy data ----\n",
    "# 6 samples (obs), 4 proteins (var)\n",
    "X = np.array([\n",
    "    [1,    np.nan, 10,   100,  500,  2.0],     # BE_kd1\n",
    "    [2,    20,     np.nan, 200,  500,  2.5],   # BE_kd2\n",
    "    [np.nan, 30,   30,   np.nan, 500,  3.0],   # BE_kd3\n",
    "    [100,  np.nan, 1000, 500,  500,  2.8],     # AS_sc1\n",
    "    [200, 400,     np.nan, np.nan, 500,  2.2], # AS_sc2\n",
    "    [np.nan, 600,  3000, 1500, 500,  2.1],     # AS_sc3\n",
    "])\n",
    "\n",
    "\n",
    "obs = pd.DataFrame({\n",
    "    \"cellline\": [\"BE\", \"BE\", \"BE\", \"AS\", \"AS\", \"AS\"],\n",
    "    \"treatment\": [\"kd\", \"kd\", \"kd\", \"sc\", \"sc\", \"sc\"]\n",
    "}, index=[f\"sample{i+1}\" for i in range(6)])\n",
    "\n",
    "var = pd.DataFrame({\"Genes\": [\"GAPDH\", \"ACTB\", \"TUBB\", \"MYH9\", \"HSP90\", \"RPLP0\"]}, index=[f\"P{i+1}\" for i in range(6)])\n",
    "\n",
    "adata = AnnData(X=X, obs=obs, var=var)\n",
    "\n",
    "# ---- STEP 2: Wrap in dummy pAnnData ----\n",
    "class DummyPAnnData:\n",
    "    def __init__(self, adata):\n",
    "        self.prot = adata\n",
    "        self.pep = None\n",
    "        self._history = []\n",
    "        \n",
    "    def _check_data(self, on):\n",
    "        return on == 'protein' and self.prot is not None\n",
    "\n",
    "    @property\n",
    "    def _cached_identifier_maps_protein(self):\n",
    "        if not hasattr(self, \"_gene_maps_protein\"):\n",
    "            self._gene_maps_protein = self._build_identifier_maps(self.prot)\n",
    "        return self._gene_maps_protein\n",
    "\n",
    "    def set_X(self, layer=\"X\", on=\"protein\"):\n",
    "        adata = self.prot if on == \"protein\" else self.pep\n",
    "        adata.X = adata.layers[layer].copy()\n",
    "        print(f\"ℹ️ Set {on} data to layer {layer}.\")\n",
    "\n",
    "    def get_gene_maps(self, on='protein'):\n",
    "        \"\"\"\n",
    "        Returns identifier mapping dictionaries:\n",
    "        - on='protein': (gene → accession, accession → gene)\n",
    "        - on='peptide': (protein accession → peptide, peptide → protein accession)\n",
    "\n",
    "        Alias: get_gene_maps() for compatibility.\n",
    "        \"\"\"\n",
    "        if on == 'protein':\n",
    "            return self._cached_identifier_maps_protein\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid value for 'on': {on}. Must be 'protein' or 'peptide'.\")\n",
    "\n",
    "    def _build_identifier_maps(self, adata, gene_col=\"Genes\"):\n",
    "        \"\"\"\n",
    "        Builds bidirectional mapping for:\n",
    "        - protein: gene ↔ accession\n",
    "        - peptide: peptide ↔ protein accession\n",
    "\n",
    "        Returns: (forward, reverse)\n",
    "        \"\"\"\n",
    "        from pandas import notna\n",
    "\n",
    "        forward = {}\n",
    "        reverse = {}\n",
    "\n",
    "        if adata is self.prot:\n",
    "            if gene_col in adata.var.columns:\n",
    "                for acc, gene in zip(adata.var_names, adata.var[gene_col]):\n",
    "                    if notna(gene):\n",
    "                        gene = str(gene)\n",
    "                        forward[gene] = acc\n",
    "                        reverse[acc] = gene\n",
    "\n",
    "        elif adata is self.pep:\n",
    "            prot_acc_col = utils.get_pep_prot_mapping(self)\n",
    "            pep_to_prot = adata.var[prot_acc_col]\n",
    "            for pep, prot in zip(adata.var_names, pep_to_prot):\n",
    "                if notna(prot):\n",
    "                    forward[prot] = pep\n",
    "                    reverse[pep] = prot\n",
    "\n",
    "        return forward, reverse\n",
    "\n",
    "    impute = impute\n",
    "    normalize = normalize\n",
    "    _normalize_helper = _normalize_helper\n",
    "\n",
    "pdata = DummyPAnnData(adata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# impute checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Global imputation using 'mean'. Layer saved as 'X_impute_mean'.\n",
      "✅ 8 values imputed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 2.625e+02, 1.000e+01, 1.000e+02],\n",
       "       [2.000e+00, 2.000e+01, 1.010e+03, 2.000e+02],\n",
       "       [7.575e+01, 3.000e+01, 3.000e+01, 5.750e+02],\n",
       "       [1.000e+02, 2.625e+02, 1.000e+03, 5.000e+02],\n",
       "       [2.000e+02, 4.000e+02, 1.010e+03, 5.750e+02],\n",
       "       [7.575e+01, 6.000e+02, 3.000e+03, 1.500e+03]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata.impute(method='mean', set_X = False)\n",
    "pdata.prot.layers[\"X_impute_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Group-wise imputation using 'mean' on class(es): ['cellline', 'treatment']. Layer saved as 'X_impute_mean'.\n",
      "✅ 8 values imputed total.\n",
      "   - AS, sc: 4 values\n",
      "   - BE, kd: 4 values\n",
      "\n",
      "✅ Imputed matrix:\n",
      "[[1.0e+00 2.5e+01 1.0e+01 1.0e+02]\n",
      " [2.0e+00 2.0e+01 2.0e+01 2.0e+02]\n",
      " [1.5e+00 3.0e+01 3.0e+01 1.5e+02]\n",
      " [1.0e+02 5.0e+02 1.0e+03 5.0e+02]\n",
      " [2.0e+02 4.0e+02 2.0e+03 1.0e+03]\n",
      " [1.5e+02 6.0e+02 3.0e+03 1.5e+03]]\n"
     ]
    }
   ],
   "source": [
    "pdata.impute(classes=[\"cellline\", \"treatment\"], method=\"mean\", set_X = False)\n",
    "print(\"\\n✅ Imputed matrix:\")\n",
    "print(np.round(pdata.prot.layers['X_impute_mean'], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Group-wise imputation using 'min' on class(es): ['cellline', 'treatment']. Layer saved as 'X_impute_min'.\n",
      "✅ 8 values imputed total.\n",
      "   - AS, sc: 4 values\n",
      "   - BE, kd: 4 values\n",
      "\n",
      "✅ Imputed matrix:\n",
      "[[1.0e+00 2.0e+01 1.0e+01 1.0e+02]\n",
      " [2.0e+00 2.0e+01 1.0e+01 2.0e+02]\n",
      " [1.0e+00 3.0e+01 3.0e+01 1.0e+02]\n",
      " [1.0e+02 4.0e+02 1.0e+03 5.0e+02]\n",
      " [2.0e+02 4.0e+02 1.0e+03 5.0e+02]\n",
      " [1.0e+02 6.0e+02 3.0e+03 1.5e+03]]\n"
     ]
    }
   ],
   "source": [
    "pdata.impute(classes=[\"cellline\", \"treatment\"], method=\"min\", set_X = False)\n",
    "print(\"\\n✅ Imputed matrix:\")\n",
    "print(np.round(pdata.prot.layers['X_impute_min'], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Global imputation using 'min'. Layer saved as 'X_impute_min'.\n",
      "✅ 8 values imputed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0e+00, 2.0e+01, 1.0e+01, 1.0e+02],\n",
       "       [2.0e+00, 2.0e+01, 1.0e+01, 2.0e+02],\n",
       "       [1.0e+00, 3.0e+01, 3.0e+01, 1.0e+02],\n",
       "       [1.0e+02, 2.0e+01, 1.0e+03, 5.0e+02],\n",
       "       [2.0e+02, 4.0e+02, 1.0e+01, 1.0e+02],\n",
       "       [1.0e+00, 6.0e+02, 3.0e+03, 1.5e+03]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata.impute(method='min', set_X = False)\n",
    "pdata.prot.layers[\"X_impute_min\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Global imputation using 'knn'. Layer saved as 'X_impute_knn'.\n",
      "✅ 8 values imputed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.00e+00, 2.50e+01, 1.00e+01, 1.00e+02],\n",
       "       [2.00e+00, 2.00e+01, 2.00e+01, 2.00e+02],\n",
       "       [1.50e+00, 3.00e+01, 3.00e+01, 1.50e+02],\n",
       "       [1.00e+02, 2.10e+02, 1.00e+03, 5.00e+02],\n",
       "       [2.00e+02, 4.00e+02, 5.05e+02, 3.00e+02],\n",
       "       [1.01e+02, 6.00e+02, 3.00e+03, 1.50e+03]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata.impute(method='knn', set_X = False, n_neighbors=2)\n",
    "pdata.prot.layers[\"X_impute_knn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Genes",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9e5c32db-20a2-40b7-870d-2f8d86124e4f",
       "rows": [
        [
         "P1",
         "GAPDH"
        ],
        [
         "P2",
         "ACTB"
        ],
        [
         "P3",
         "TUBB"
        ],
        [
         "P4",
         "MYH9"
        ],
        [
         "P5",
         "HSP90"
        ],
        [
         "P6",
         "RPLP0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P1</th>\n",
       "      <td>GAPDH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>ACTB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "      <td>TUBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P4</th>\n",
       "      <td>MYH9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P5</th>\n",
       "      <td>HSP90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P6</th>\n",
       "      <td>RPLP0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Genes\n",
       "P1  GAPDH\n",
       "P2   ACTB\n",
       "P3   TUBB\n",
       "P4   MYH9\n",
       "P5  HSP90\n",
       "P6  RPLP0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata.prot.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Normalizing using only fully observed columns: [4 5]\n",
      "ℹ️ Global normalization using 'sum' (using only fully observed columns). Layer saved as 'X_norm_sum'.\n",
      "✅ Normalized all 6 samples.\n",
      "\n",
      "Normalized matrix:\n",
      "[[   1.       nan   10.02  100.2   501.      2.  ]\n",
      " [   2.     20.02     nan  200.2   500.5     2.5 ]\n",
      " [    nan   30.     30.       nan  500.      3.  ]\n",
      " [ 100.04     nan 1000.4   500.2   500.2     2.8 ]\n",
      " [ 200.32  400.64     nan     nan  500.8     2.2 ]\n",
      " [    nan  601.08 3005.38 1502.69  500.9     2.1 ]]\n",
      "\n",
      "Row sums after normalization: [ 614.22  725.22  563.   2103.64 1103.96 5612.14]\n",
      "Are all row sums equal? False\n"
     ]
    }
   ],
   "source": [
    "# Test global normalization by 'sum'\n",
    "pdata.normalize(method='sum', classes=None, on='protein', set_X=False, use_nonmissing=True)\n",
    "\n",
    "# Inspect result\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "layer_name = \"X_norm_sum\"\n",
    "print(\"\\nNormalized matrix:\")\n",
    "print(pdata.prot.layers[layer_name])\n",
    "\n",
    "# Check that all row sums are now equal\n",
    "row_sums = np.nansum(pdata.prot.layers[layer_name], axis=1)\n",
    "print(\"\\nRow sums after normalization:\", row_sums)\n",
    "print(\"Are all row sums equal?\", np.allclose(row_sums, row_sums[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Global normalization using 'sum'. Layer saved as 'X_norm_sum'.\n",
      "✅ Normalized all 6 samples.\n",
      "\n",
      "Normalized matrix:\n",
      "[[   9.13883        nan   91.38825  913.88254 4569.41272   18.27765]\n",
      " [  15.46473  154.64734        nan 1546.47343 3866.18357   19.33092]\n",
      " [       nan  298.51332  298.51332        nan 4975.22202   29.85133]\n",
      " [ 266.41145        nan 2664.11451 1332.05726 1332.05726    7.45952]\n",
      " [1016.53058 2033.06115        nan        nan 2541.32644   11.18184]\n",
      " [       nan  600.      3000.      1500.       500.         2.1    ]]\n",
      "\n",
      "Row sums after normalization: [5602.1 5602.1 5602.1 5602.1 5602.1 5602.1]\n",
      "Are all row sums equal? True\n"
     ]
    }
   ],
   "source": [
    "# Test global normalization by 'sum'\n",
    "pdata.normalize(method='sum', classes=None, on='protein', set_X=False, use_nonmissing=False)\n",
    "\n",
    "# Inspect result\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "layer_name = \"X_norm_sum\"\n",
    "print(\"\\nNormalized matrix:\")\n",
    "print(pdata.prot.layers[layer_name])\n",
    "\n",
    "# Check that all row sums are now equal\n",
    "row_sums = np.nansum(pdata.prot.layers[layer_name], axis=1)\n",
    "print(\"\\nRow sums after normalization:\", row_sums)\n",
    "print(\"Are all row sums equal?\", np.allclose(row_sums, row_sums[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Normalizing using only fully observed columns: [4 5]\n",
      "ℹ️ Normalizing using only fully observed columns: [4 5]\n",
      "ℹ️ Group-wise normalization using 'mean' on class(es): cellline (using only fully observed columns). Layer saved as 'X_norm_mean'.\n",
      "✅ Normalized 6 samples total.\n",
      "   - AS: 3 samples normalized\n",
      "   - BE: 3 samples normalized\n",
      "\n",
      "Normalized (group-wise mean) matrix:\n",
      "[[   1.00199        nan   10.01992  100.1992   500.99602    2.00398]\n",
      " [   2.00199   20.0199         nan  200.199    500.49751    2.50249]\n",
      " [       nan   30.        30.             nan  500.         3.     ]\n",
      " [ 100.             nan 1000.       500.       500.         2.8    ]\n",
      " [ 200.23895  400.4779         nan        nan  500.59737    2.20263]\n",
      " [       nan  600.83649 3004.18243 1502.09122  500.69707    2.10293]]\n",
      "\n",
      "Row means by group:\n",
      "group\n",
      "AS    [420.56000000000006, 275.8792114695341, 1121.9...\n",
      "BE      [122.84422310756972, 145.0441791044776, 140.75]\n",
      "dtype: object\n",
      "\n",
      "Are all row means within each group equal?\n"
     ]
    }
   ],
   "source": [
    "# Reset to original data first\n",
    "pdata.prot.X = X.copy()\n",
    "\n",
    "# Test group-wise normalization by 'mean'\n",
    "pdata.normalize(method='mean', classes='cellline', on='protein',set_X=False, use_nonmissing=True)\n",
    "\n",
    "# Inspect result\n",
    "print(\"\\nNormalized (group-wise mean) matrix:\")\n",
    "print(pdata.prot.layers[\"X_norm_mean\"])\n",
    "\n",
    "# Check that row means within each group are equal\n",
    "df = pd.DataFrame(pdata.prot.layers[\"X_norm_mean\"])\n",
    "df['group'] = pdata.prot.obs['cellline'].values\n",
    "means_by_group = df.groupby('group').apply(lambda g: np.nanmean(g.drop(columns='group').values, axis=1))\n",
    "print(\"\\nRow means by group:\")\n",
    "print(means_by_group)\n",
    "\n",
    "# Check that all row means within each group are equal\n",
    "print(\"\\nAre all row means within each group equal?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500. 300. 600.]\n",
      "[1.2 2.  1. ]\n",
      "[10. 20. 30.]\n",
      "[3.  1.5 1. ]\n",
      "ℹ️ Group-wise normalization using 'median' on class(es): cellline. Layer saved as 'X_norm_median'.\n",
      "✅ Normalized 6 samples total.\n",
      "   - AS: 3 samples normalized\n",
      "   - BE: 3 samples normalized\n",
      "\n",
      "Normalized (group-wise median) matrix:\n",
      "[[   3.       nan   30.    300.   1500.      6.  ]\n",
      " [   3.     30.       nan  300.    750.      3.75]\n",
      " [    nan   30.     30.       nan  500.      3.  ]\n",
      " [ 120.       nan 1200.    600.    600.      3.36]\n",
      " [ 400.    800.       nan     nan 1000.      4.4 ]\n",
      " [    nan  600.   3000.   1500.    500.      2.1 ]]\n",
      "\n",
      "Row median by group:\n",
      "group\n",
      "AS    [600.0, 600.0, 600.0]\n",
      "BE       [30.0, 30.0, 30.0]\n",
      "dtype: object\n",
      "\n",
      "Are all row median within each group equal?\n"
     ]
    }
   ],
   "source": [
    "# Reset to original data first\n",
    "pdata.prot.X = X.copy()\n",
    "\n",
    "# Test group-wise normalization by 'median'\n",
    "pdata.normalize(method='median', classes='cellline', on='protein',set_X=False)\n",
    "\n",
    "# Inspect result\n",
    "print(\"\\nNormalized (group-wise median) matrix:\")\n",
    "print(pdata.prot.layers[\"X_norm_median\"])\n",
    "\n",
    "# Check that row means within each group are equal\n",
    "df = pd.DataFrame(pdata.prot.layers[\"X_norm_median\"])\n",
    "df['group'] = pdata.prot.obs['cellline'].values\n",
    "median_by_group = df.groupby('group').apply(lambda g: np.nanmedian(g.drop(columns='group').values, axis=1))\n",
    "print(\"\\nRow median by group:\")\n",
    "print(median_by_group)\n",
    "\n",
    "# Check that all row median within each group are equal\n",
    "print(\"\\nAre all row median within each group equal?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Normalizing using found reference columns: ['P1', 'P2']\n",
      "ℹ️ Global normalization using 'reference_feature'. Layer saved as 'X_norm_reference_feature'.\n",
      "✅ Normalized all 6 samples.\n",
      "\n",
      "Normalized matrix using reference feature:\n",
      "[[   200.         nan   2000.    20000.   100000.      400.  ]\n",
      " [   130.     1300.         nan  13000.    32500.      162.5 ]\n",
      " [      nan    600.      600.         nan  10000.       60.  ]\n",
      " [   200.         nan   2000.     1000.     1000.        5.6 ]\n",
      " [   250.      500.         nan       nan    625.        2.75]\n",
      " [      nan    600.     3000.     1500.      500.        2.1 ]]\n",
      "\n",
      "Row median by group:\n",
      "group\n",
      "AS     [1000.0, 375.0, 600.0]\n",
      "BE    [2000.0, 1300.0, 600.0]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "pdata = DummyPAnnData(adata)\n",
    "\n",
    "# Test reference feature normalization using gene names\n",
    "pdata.normalize(\n",
    "    method='reference_feature',\n",
    "    reference_columns=['GAPDH', 'ACTB'],\n",
    "    reference_method='mean',\n",
    "    set_X=False\n",
    ")\n",
    "\n",
    "# Inspect result\n",
    "print(\"\\nNormalized matrix using reference feature:\")\n",
    "print(pdata.prot.layers[\"X_norm_reference_feature\"])\n",
    "# Check that reference columns median across all rows\n",
    "df = pd.DataFrame(pdata.prot.layers[\"X_norm_reference_feature\"])\n",
    "df['group'] = pdata.prot.obs['cellline'].values\n",
    "median_by_group = df.groupby('group').apply(lambda g: np.nanmedian(g.drop(columns='group').values, axis=1))\n",
    "print(\"\\nRow median by group:\")\n",
    "print(median_by_group)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
